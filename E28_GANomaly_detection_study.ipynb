{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이미지 이상탐지모델(GAN)\n",
    "\n",
    "- 공항 출국장에서 수하물 X-선 검사를 받으면서 보안검사를 받아보신 경험들이 한번쯤 있으시겠죠? 실은 비단 여러분들의 수하물 정도가 아니라 비행기에 실리고 내리는 모든 화물들에 대해 보이지 않는 어딘가에서 X-선으로 꼼꼼이 검사를 하고 있답니다. 그러기 위해 보안 검사원들이 불철주야 수고를 하고 계신데요, 블랙프라이데이 같은 시즌이 도래하면 하루에도 똑같은 가방 수천개가 컨베이어벨트를 지나가는 것을 눈을 떼지 못하고 X-선으로 지켜보고 있어야만 하는 세관 검사원들의 웃지못할 이야기도 있다고 합니다.\n",
    "\n",
    "- 딥러닝 기술이 유용하게 사용될 수 있는 대표적인 사례로 바로 이상감지(Anomaly Detection)를 꼽을 수 있습니다. 위 그림에서처럼 혹시라도 위험한 물품이 포함되어 있지 않을까 24시간 사람이 뚫어져라 모니터를 쳐다보고 있어야만 하는 고된 일을 인공지능이 대신 정확하게 수행해 줄 수 있다면 좋겠죠? 이런 과제는 실제로 공항, 항만, 주요 군사시설 등에서 매우 관심을 가지고 진행하고 있답니다. 그런데 이런 이상감지 모델로 GAN 기술이 유용하게 활용되고 있다고 합니다.\n",
    "\n",
    "- 왜 그럴까요? 그냥 쉽게 생각할 수 있는 지도학습(supervised learning)으로는 어려울까요? 네 그렇습니다. 이상감지 태스크의 경우, 정상(Normal) 상황에 대한 데이터는 아주 풍부하게 얻을 수 있지만, 이상(Anomaly) 상황에 대한 데이터는 매우 부족할 뿐 아니라 사실상 모든 케이스의 이상 상황을 정의할 수조차 없기 때문입니다. 이 때 GAN이 사용된다면 이상(Anomaly) 데이터가 충분하지 않은 경우에도 꽤 쓸만한 이상감지 성능을 발휘하는 모델을 만들 수 있다고 합니다.\n",
    "\n",
    "- 그럼 어떻게 이런 모델을 만들어 가는 것인지, 함께 알아보도록 할까요?\n",
    "\n",
    "## 학습목표\n",
    "- 이상(Anomaly) 데이터가 부족한 상황에서 GAN을 이용해 이미지 이상감지 모델을 구축하는 논리를 파악한다.\n",
    "- Skip-GANomaly 모델 및 Loss 함수를 구현해 본다.\n",
    "- 간단한 데이터셋을 이용해 Skip-GANomaly 의 이상감지 효과를 파악해 본다.\n",
    "\n",
    "## 학습 목차\n",
    "- Anomaly Detection with GAN\n",
    "- GANomaly\n",
    "- Skip-GANomaly\n",
    "- 데이터셋 구성\n",
    "- 모델과 Loss함수 구성\n",
    "- 모델 학습과 평가\n",
    "\n",
    "## 평가 루브릭\n",
    "\n",
    "- 아래의 기준을 바탕으로 프로젝트를 평가합니다.\n",
    "\n",
    "###  평가문항\t상세기준\n",
    "1. Anomaly Detection 태스크 특성에 맞도록 데이터셋 가공이 진행되었다.: 개구리 클래스가 배제되어 테스트셋에만 반영되는 로직이 구현되었다.\n",
    "2. Skip-GANomaly 모델이 정상적으로 구현되었다.: 모델 학습 및 테스트가 원활하게 진행되었다.\n",
    "3. 이상감지 수행 결과가 체계적으로 시각화되었다. : Anomaly score 분호, 테스트셋 이상감지 정확도(%), 이상감지 성공실패사례 제시가 진행되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection with GAN\n",
    "\n",
    "- 2017년에 발표된 Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery라는 논문에서 AnoGAN이라는 모델을 제안합니다. 논문의 내용은 제목에 그대로 담겨 있는데요, GAN을 이용하면 Anomaly 데이터가 부족한 상황에서도 Unsupervised 방식으로 Anomaly Detection 모델 구현이 가능하다는 아이디어입니다.\n",
    "\n",
    "- 이게 어떻게 가능한 것일까요?\n",
    "\n",
    "![title](E-28-01.max-800x600.png)\n",
    "\n",
    "- 위 그림을 봅시다. 얼핏 보면 흔히 보는 GAN의 모델 구조를 가지고 있습니다. 여기서 중요한 것은 학습데이터로 Anomal하지 않은 정상(normal) 데이터만 사용한다는 점입니다. 그렇다면 GAN의 원리에 의해 Discriminator(빨간색)은 정상적인 이미지와 Generator(푸른색)만들어 낸 가짜 이미지를 잘 구분하게 될 것이고, Generator는 그런 Discriminator를 속이기 위해 정상 데이터같은 이미지를 만들어 낼 것입니다.\n",
    "\n",
    "- 여기서 AnoGAN의 저자들은 이런 생각을 했습니다. 잘 훈련된 GAN의 Generator x=G(z)란 바로 Latent variable z와 이미지 x 사이의 어떤 맵핑을 나타내는 함수가 아닐까? 그렇다면 Normal 데이터를 잘 만들어낼 수 있는 z의 영역을 (z를 수없이 샘플링해 가면서) 더듬더듬 찾아내면 위 그림의 오른쪽과 같은 윤곽이 얻어지게 될 수 있지 않을까?\n",
    "\n",
    "- 그래서 추가적인 Loss 2개를 아래와 같이 정의합니다.\n",
    "\n",
    "![title](GANomaly.png)\n",
    "\n",
    "- 그다음 아이디어는 어찌보면 당연한 것입니다. 지금까지 학습한 GAN의 Generator와 Discriminator는 모두 Normal 데이터에 대해서만 최적화된 파라미터를 가지고 있으며, Generator가 정상데이터처럼 보이도록 만드는 $ z_γ\\\\ $ 까지 찾아두었다면, Normal한 x에 대해서는 위 Loss값이 아주 작을 것입니다. 그러나 Anomaly 데이터 $ x $에 대해서는 이 Loss값이 일정 threshold 이상으로 치솟게 될 것이라는 논리입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EGBAD (feat. by BiGAN)\n",
    "\n",
    "![title](BiGAN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANomaly\n",
    "\n",
    "![title](GANomaly1.png)\n",
    "\n",
    "![title](GANomaly2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-GANomaly\n",
    "\n",
    "![title](Skip-GANomaly1.png)\n",
    "\n",
    "![title](Skip-GANomaly2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 구성(Fashion-MNIST)\n",
    "\n",
    "- 지금부터는 Skip-GANomaly 모델을 실제로 구현하는 과정을 진행하려고 합니다. 이런 이상감지 모델을 학습할 때 쉽게 사용하는 데이터 구성은 MNIST 같이 잘 알려진 데이터셋에서 특정 클래스를 이상 데이터로 활용하는 방식입니다. 예를 들어 0으로 라벨링된 모든 이미지를 훈련 데이터에서 제거하고 1~9로만 학습을 시킨 후 테스트 데이터를 0을 포함한(이상 데이터를 포함한) 형태로 구성하여 이상감지 성능을 평가하는 방식입니다.\n",
    "\n",
    "- 오늘 우리는 Fashion MNIST를 활용해 볼 것입니다. Fashion MNIST에서 8번 라벨이 가방(Bag)입니다. 그래서 이번에는 가방을 제거하고 의복과 신발류로만 구성된 Fashion MNIST를 정상 데이터로 삼아볼 생각입니다. 말하자면, 우리의 모델은 가방이라는 패션 아이템을 보도듣도 못하였으며, 가방이라는 패션이라는 것은 매우 특이한 상황이라고 가정하는 것입니다^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix, precision_recall_curve, average_precision_score\n",
    "from scipy.interpolate import interp1d\n",
    "from inspect import signature\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from scipy.stats import norm\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이상감지 데이터셋 구축\n",
    "\n",
    "- 위에서 언급한 대로 학습/테스트 데이터셋을 구축해 보겠습니다.\n",
    "\n",
    "- (주의) Fashion-MNIST는 1채널 grayscale 데이터셋입니다. 그냥은 Convolution 연산이 되지 않으므로 채널방향 차원이 하나 늘어나도록 reshape하는 과정이 필요합니다. \n",
    "- (주의) UNet 구조의 활용을 위해서 기존의 28 X 28 사이즈의 Fashion-MNIST 데이터 이미지를 32 X 32 로 패딩처리해 줄 필요가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 32, 32, 1)\n",
      "(10000, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "train_data = (train_data - 127.5) / 127.5\n",
    "test_data = (test_data - 127.5) / 127.5\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Fashion MNIST padding to 32 X 32\n",
    "train_data_32 = np.zeros((train_data.shape[0], 32, 32)).astype('float32')\n",
    "test_data_32 = np.zeros((test_data.shape[0], 32, 32)).astype('float32')     \n",
    "train_data_32[:, 2:30, 2:30] = train_data\n",
    "test_data_32[:, 2:30, 2:30] = test_data\n",
    "\n",
    "# 1channel data reshape\n",
    "train_data = train_data_32.reshape(train_data_32.shape[0], 32, 32, 1).astype('float32')\n",
    "test_data = test_data_32.reshape(test_data_32.shape[0], 32, 32, 1).astype('float32')\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 잠깐 Fashion-MNIST 데이터가 어떻게 생겼는지 확인해 볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(train_data[i].reshape(32, 32), cmap='gray')\n",
    "    plt.title(f'index: {i}')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위에서 설명한 대로 이상 데이터로 선정된 8번 라벨(Bag) 데이터를 제외하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_labels(labels):\n",
    "    new_t_labels = []\n",
    "    for old_label in labels:\n",
    "        if old_label == 8:   # Bag:8\n",
    "            new_t_labels.append([0])  # Bag을 이상치로 처리\n",
    "        else:\n",
    "            new_t_labels.append([1])  # 그 외의 경우는 정상치\n",
    "             \n",
    "    return np.array(new_t_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bol_train_labels = set_labels(train_labels)\n",
    "bol_test_labels = set_labels(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = []\n",
    "normal_labels = []\n",
    "anomaly_data = []\n",
    "anomaly_labels = []\n",
    "for data, label in zip(train_data, bol_train_labels):\n",
    "    if label == 0:\n",
    "        anomaly_data.append(data)\n",
    "        anomaly_labels.append(label)\n",
    "    else:\n",
    "        normal_data.append(data)\n",
    "        normal_labels.append(label)\n",
    "        \n",
    "normal_data = np.array(normal_data)\n",
    "normal_labels = np.array(normal_labels)\n",
    "anomaly_data = np.array(anomaly_data)\n",
    "anomaly_labels = np.array(anomaly_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000, 32, 32, 1) (54000, 1)\n",
      "(6000, 32, 32, 1) (6000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(normal_data.shape, normal_labels.shape)\n",
    "print(anomaly_data.shape, anomaly_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = normal_data\n",
    "bol_train_labels = normal_labels\n",
    "test_data = tf.concat([test_data, anomaly_data], 0)\n",
    "bol_test_labels = tf.concat([bol_test_labels, anomaly_labels], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000, 32, 32, 1)\n",
      "(16000, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000, 1)\n",
      "(16000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(bol_train_labels.shape)\n",
    "print(bol_test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 어떻습니까? 60000건의 훈련데이터 중 6000건이 제외되어 10000건의 테스트 데이터에 추가되었습니다. 계산이 맞아떨어지나요?\n",
    "\n",
    "- 데이터셋이 정확하게 구성되었는지 좀더 검증해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "for label in bol_train_labels:\n",
    "    if label == 0:\n",
    "        print(label)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제 데이터셋을 구성하고 label을 검증해 보겠습니다.\n",
    "\n",
    "- 훈련 데이터셋에는 라벨이 1인 데이터만 존재하고, 테스트 데이터에는 0과 1이 섞여 있어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, bol_train_labels))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, bol_test_labels))\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]], shape=(8, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for data, label in train_dataset.take(1):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]], shape=(8, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for data, label in test_dataset.take(1):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델과 Loss함수 구성\n",
    "## Generator\n",
    "- 이제 본격적으로 모델을 구성해 보겠습니다. Generator는 그동안 자주 다루었을 UNet 구조를 따릅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_block, self).__init__()\n",
    "        self.conv_layer = tf.keras.Sequential([\n",
    "            layers.Conv2D(num_filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                          kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        outputs = self.conv_layer(inputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_T_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_T_block, self).__init__()\n",
    "        self.conv_T_layer = tf.keras.Sequential([\n",
    "            layers.Conv2DTranspose(num_filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                                   kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, concat, training=False):\n",
    "        upsample = self.conv_T_layer(inputs)\n",
    "        outputs = tf.concat([upsample, concat], -1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, num_output_channel=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder_1 = Conv_block(64) # 16\n",
    "        self.encoder_2 = Conv_block(128) # 8\n",
    "        self.encoder_3 = Conv_block(256) # 4\n",
    "        self.encoder_4 = Conv_block(512) # 2\n",
    "        \n",
    "        self.center = Conv_block(512) # 1\n",
    "        \n",
    "        self.decoder_4 = Conv_T_block(512) # 2\n",
    "        self.decoder_3 = Conv_T_block(256) # 4\n",
    "        self.decoder_2 = Conv_T_block(128) # 8\n",
    "        self.decoder_1 = Conv_T_block(64) # 16\n",
    "        \n",
    "        self.output_layer = layers.Conv2DTranspose(num_output_channel, 1, strides=2, padding='same', use_bias=False, # 32\n",
    "                                                   kernel_initializer=tf.random_normal_initializer(0., 0.02))\n",
    "                \n",
    "    def call(self, inputs, training=False):\n",
    "        en_1 = self.encoder_1(inputs) # gen\n",
    "        en_2 = self.encoder_2(en_1)\n",
    "        en_3 = self.encoder_3(en_2)\n",
    "        en_4 = self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        de_4 = self.decoder_4(center, en_4)\n",
    "        de_3 = self.decoder_3(de_4, en_3)\n",
    "        de_2 = self.decoder_2(de_3, en_2)\n",
    "        de_1 = self.decoder_1(de_2, en_1)\n",
    "        \n",
    "        outputs = self.output_layer(de_1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator\n",
    "- Discriminator도 Generator처럼 Conv_block을 활용하며, 최종적으로 sigmoid를 거쳐 0~1 사이의 숫자를 리턴합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.encoder_1 = Conv_block(64) # 16\n",
    "        self.encoder_2 = Conv_block(128) # 8\n",
    "        self.encoder_3 = Conv_block(256) # 4\n",
    "        self.encoder_4 = Conv_block(512) # 2\n",
    "        \n",
    "        self.center = Conv_block(100) # 1\n",
    "        \n",
    "        self.outputs = layers.Conv2D(1, 3, strides=1, padding='same',\n",
    "                                          use_bias=False, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        en_1 = self.encoder_1(inputs) # dis\n",
    "        en_2 = self.encoder_2(en_1)\n",
    "        en_3 = self.encoder_3(en_2)\n",
    "        en_4 = self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        outputs = self.outputs(center)\n",
    "        \n",
    "        return outputs, center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전체 모델 구성\n",
    "- Generator와 Discriminator을 합쳐 전체 모델을 구성해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(num_output_channel=1)  # Generator가 32X32X1 짜리 이미지를 생성해야 합니다. \n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss 함수\n",
    "- GAN 모델의 핵심은 Loss 함수의 구성방법에 달려 있다고 해도 과언이 아닙니다. Skip-GANomaly는 이전 모델들과 달리 일반적인 GAN의 학습 절차와 같은 형태의 Loss 구성이 진행됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_loss = tf.keras.losses.MeanSquaredError()\n",
    "l1_loss = tf.keras.losses.MeanAbsoluteError()\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(pred_real, pred_fake):\n",
    "    real_loss = cross_entropy(tf.ones_like(pred_real), pred_real)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(pred_fake), pred_fake)\n",
    "    \n",
    "    total_dis_loss = (real_loss + fake_loss) * 0.5\n",
    "    \n",
    "    return total_dis_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아래 Generator Loss에는 이전 스텝에서 설명했던 Skip-GANomaly의 주요 loss 함수들이 포함되어 있음을 주목해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(real_output, fake_output, input_data, gen_data, latent_first, latent_sec):\n",
    "    w_adv = 1.\n",
    "    w_context = 40.\n",
    "    w_encoder = 1.\n",
    "    \n",
    "    adv_loss = cross_entropy(real_output, fake_output)\n",
    "    context_loss = l1_loss(input_data, gen_data)\n",
    "    encoder_loss = l2_loss(latent_first, latent_sec)\n",
    "    \n",
    "    total_gen_loss = w_adv * adv_loss + \\\n",
    "                     w_context * context_loss + \\\n",
    "                     w_encoder * encoder_loss\n",
    "    \n",
    "    return total_gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer 설정\n",
    "generator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습과 평가\n",
    "## Model Train\n",
    "- 이제 본격적으로 모델을 학습해 보겠습니다. 총 25Epoch 대략 1시간 정도 소요될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(images, training=True)\n",
    "        \n",
    "        pred_real, feat_real = discriminator(images, training=True)\n",
    "        pred_fake, feat_fake = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(pred_real, pred_fake,\n",
    "                                  images, generated_images,\n",
    "                                  feat_real, feat_fake)\n",
    "\n",
    "        disc_loss = discriminator_loss(pred_real, pred_fake)        \n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(os.getenv('HOME'),'aiffel/GANomaly/ganomaly_skip_no_norm/ckpt')\n",
    "\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 총 25Epoch 를 수행하는데 1시간 이상 소요될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 100, \t Total Gen Loss : 18.950666427612305, \t Total Dis Loss : 0.36304792761802673\n",
      "Steps : 200, \t Total Gen Loss : 24.331050872802734, \t Total Dis Loss : 0.084014892578125\n",
      "Steps : 300, \t Total Gen Loss : 21.02449607849121, \t Total Dis Loss : 0.26930153369903564\n",
      "Steps : 400, \t Total Gen Loss : 21.966201782226562, \t Total Dis Loss : 0.06967641413211823\n",
      "Steps : 500, \t Total Gen Loss : 23.933237075805664, \t Total Dis Loss : 0.05286383628845215\n",
      "Steps : 600, \t Total Gen Loss : 21.265411376953125, \t Total Dis Loss : 0.048699796199798584\n",
      "Steps : 700, \t Total Gen Loss : 23.14034652709961, \t Total Dis Loss : 1.2490978240966797\n",
      "Steps : 800, \t Total Gen Loss : 24.46974754333496, \t Total Dis Loss : 0.8310852646827698\n",
      "Steps : 900, \t Total Gen Loss : 24.702604293823242, \t Total Dis Loss : 0.08954264968633652\n",
      "Steps : 1000, \t Total Gen Loss : 23.079544067382812, \t Total Dis Loss : 0.06265253573656082\n",
      "Steps : 1100, \t Total Gen Loss : 22.46427345275879, \t Total Dis Loss : 0.07759592682123184\n",
      "Steps : 1200, \t Total Gen Loss : 24.65382194519043, \t Total Dis Loss : 0.0205821692943573\n",
      "Steps : 1300, \t Total Gen Loss : 24.89409065246582, \t Total Dis Loss : 0.08877786993980408\n",
      "Steps : 1400, \t Total Gen Loss : 23.584091186523438, \t Total Dis Loss : 0.01604536920785904\n",
      "Steps : 1500, \t Total Gen Loss : 24.56049919128418, \t Total Dis Loss : 0.02059660106897354\n",
      "Steps : 1600, \t Total Gen Loss : 25.0732364654541, \t Total Dis Loss : 0.06846402585506439\n",
      "Steps : 1700, \t Total Gen Loss : 23.09374237060547, \t Total Dis Loss : 0.031030263751745224\n",
      "Steps : 1800, \t Total Gen Loss : 23.811634063720703, \t Total Dis Loss : 0.049683354794979095\n",
      "Steps : 1900, \t Total Gen Loss : 24.815879821777344, \t Total Dis Loss : 0.06580714881420135\n",
      "Steps : 2000, \t Total Gen Loss : 22.28557586669922, \t Total Dis Loss : 0.03177697956562042\n",
      "Steps : 2100, \t Total Gen Loss : 23.73630142211914, \t Total Dis Loss : 0.02130071632564068\n",
      "Steps : 2200, \t Total Gen Loss : 24.748035430908203, \t Total Dis Loss : 0.04614990949630737\n",
      "Steps : 2300, \t Total Gen Loss : 25.214092254638672, \t Total Dis Loss : 0.21440085768699646\n",
      "Steps : 2400, \t Total Gen Loss : 25.341432571411133, \t Total Dis Loss : 0.016450729221105576\n",
      "Steps : 2500, \t Total Gen Loss : 23.792200088500977, \t Total Dis Loss : 0.008385522291064262\n",
      "Steps : 2600, \t Total Gen Loss : 24.344472885131836, \t Total Dis Loss : 0.02901705726981163\n",
      "Steps : 2700, \t Total Gen Loss : 25.534849166870117, \t Total Dis Loss : 0.021857943385839462\n",
      "Steps : 2800, \t Total Gen Loss : 25.170684814453125, \t Total Dis Loss : 0.02014091983437538\n",
      "Steps : 2900, \t Total Gen Loss : 24.849037170410156, \t Total Dis Loss : 0.026782436296343803\n",
      "Steps : 3000, \t Total Gen Loss : 23.61293601989746, \t Total Dis Loss : 0.07349176704883575\n",
      "Steps : 3100, \t Total Gen Loss : 24.70842742919922, \t Total Dis Loss : 0.0044362288899719715\n",
      "Steps : 3200, \t Total Gen Loss : 26.0474796295166, \t Total Dis Loss : 0.008038978092372417\n",
      "Steps : 3300, \t Total Gen Loss : 25.253501892089844, \t Total Dis Loss : 0.03245531767606735\n",
      "Steps : 3400, \t Total Gen Loss : 27.382152557373047, \t Total Dis Loss : 0.010383057408034801\n",
      "Steps : 3500, \t Total Gen Loss : 24.495080947875977, \t Total Dis Loss : 0.00859462283551693\n",
      "Steps : 3600, \t Total Gen Loss : 25.989242553710938, \t Total Dis Loss : 0.003513069823384285\n",
      "Steps : 3700, \t Total Gen Loss : 24.246580123901367, \t Total Dis Loss : 0.012571780011057854\n",
      "Steps : 3800, \t Total Gen Loss : 22.802997589111328, \t Total Dis Loss : 0.08580375462770462\n",
      "Steps : 3900, \t Total Gen Loss : 27.761741638183594, \t Total Dis Loss : 0.013352871872484684\n",
      "Steps : 4000, \t Total Gen Loss : 24.4405460357666, \t Total Dis Loss : 0.017270855605602264\n",
      "Steps : 4100, \t Total Gen Loss : 26.012744903564453, \t Total Dis Loss : 0.00718042254447937\n",
      "Steps : 4200, \t Total Gen Loss : 25.75621795654297, \t Total Dis Loss : 0.017025142908096313\n",
      "Steps : 4300, \t Total Gen Loss : 27.6635684967041, \t Total Dis Loss : 0.007585530169308186\n",
      "Steps : 4400, \t Total Gen Loss : 26.097013473510742, \t Total Dis Loss : 0.007676554843783379\n",
      "Steps : 4500, \t Total Gen Loss : 27.053390502929688, \t Total Dis Loss : 0.006840817630290985\n",
      "Steps : 4600, \t Total Gen Loss : 23.130184173583984, \t Total Dis Loss : 0.007902167737483978\n",
      "Steps : 4700, \t Total Gen Loss : 26.953310012817383, \t Total Dis Loss : 0.0056398408487439156\n",
      "Steps : 4800, \t Total Gen Loss : 27.282312393188477, \t Total Dis Loss : 0.016141047701239586\n",
      "Steps : 4900, \t Total Gen Loss : 24.513233184814453, \t Total Dis Loss : 0.3894449770450592\n",
      "Steps : 5000, \t Total Gen Loss : 25.54000473022461, \t Total Dis Loss : 0.0073956153355538845\n",
      "Steps : 5100, \t Total Gen Loss : 26.419212341308594, \t Total Dis Loss : 0.006666277535259724\n",
      "Steps : 5200, \t Total Gen Loss : 26.67188835144043, \t Total Dis Loss : 0.01221967488527298\n",
      "Steps : 5300, \t Total Gen Loss : 25.725290298461914, \t Total Dis Loss : 0.0058810412883758545\n",
      "Steps : 5400, \t Total Gen Loss : 25.824419021606445, \t Total Dis Loss : 0.005363216623663902\n",
      "Steps : 5500, \t Total Gen Loss : 26.216638565063477, \t Total Dis Loss : 0.03190930560231209\n",
      "Steps : 5600, \t Total Gen Loss : 25.32459831237793, \t Total Dis Loss : 0.017511237412691116\n",
      "Steps : 5700, \t Total Gen Loss : 24.65340232849121, \t Total Dis Loss : 0.004490986466407776\n",
      "Steps : 5800, \t Total Gen Loss : 25.221370697021484, \t Total Dis Loss : 0.00755220977589488\n",
      "Steps : 5900, \t Total Gen Loss : 25.302762985229492, \t Total Dis Loss : 0.0056996652856469154\n",
      "Steps : 6000, \t Total Gen Loss : 25.388954162597656, \t Total Dis Loss : 0.003938066307455301\n",
      "Steps : 6100, \t Total Gen Loss : 24.38412857055664, \t Total Dis Loss : 0.006371962372213602\n",
      "Steps : 6200, \t Total Gen Loss : 25.523738861083984, \t Total Dis Loss : 0.004090184811502695\n",
      "Steps : 6300, \t Total Gen Loss : 25.331586837768555, \t Total Dis Loss : 0.008536542765796185\n",
      "Steps : 6400, \t Total Gen Loss : 24.872129440307617, \t Total Dis Loss : 0.019948512315750122\n",
      "Steps : 6500, \t Total Gen Loss : 25.62432098388672, \t Total Dis Loss : 0.0020038720685988665\n",
      "Steps : 6600, \t Total Gen Loss : 24.384122848510742, \t Total Dis Loss : 0.006113867741078138\n",
      "Steps : 6700, \t Total Gen Loss : 26.774972915649414, \t Total Dis Loss : 0.002526306314393878\n",
      "Time for epoch 1 is 324.3019914627075 sec\n",
      "Steps : 6800, \t Total Gen Loss : 28.620840072631836, \t Total Dis Loss : 0.0023189268540591\n",
      "Steps : 6900, \t Total Gen Loss : 27.344728469848633, \t Total Dis Loss : 0.0031497892923653126\n",
      "Steps : 7000, \t Total Gen Loss : 26.7363338470459, \t Total Dis Loss : 0.005715928040444851\n",
      "Steps : 7100, \t Total Gen Loss : 26.31499481201172, \t Total Dis Loss : 0.00475654099136591\n",
      "Steps : 7200, \t Total Gen Loss : 27.973461151123047, \t Total Dis Loss : 0.006014704704284668\n",
      "Steps : 7300, \t Total Gen Loss : 27.488798141479492, \t Total Dis Loss : 0.002016410930082202\n",
      "Steps : 7400, \t Total Gen Loss : 28.477025985717773, \t Total Dis Loss : 0.00569583335891366\n",
      "Steps : 7500, \t Total Gen Loss : 26.74057388305664, \t Total Dis Loss : 0.0028888669330626726\n",
      "Steps : 7600, \t Total Gen Loss : 28.566776275634766, \t Total Dis Loss : 0.001729482552036643\n",
      "Steps : 7700, \t Total Gen Loss : 27.776138305664062, \t Total Dis Loss : 0.0034926440566778183\n",
      "Steps : 7800, \t Total Gen Loss : 30.35663604736328, \t Total Dis Loss : 0.0012372888159006834\n",
      "Steps : 7900, \t Total Gen Loss : 29.3544921875, \t Total Dis Loss : 0.0015764269046485424\n",
      "Steps : 8000, \t Total Gen Loss : 27.296024322509766, \t Total Dis Loss : 0.0033561072777956724\n",
      "Steps : 8100, \t Total Gen Loss : 25.159048080444336, \t Total Dis Loss : 0.03984099626541138\n",
      "Steps : 8200, \t Total Gen Loss : 29.917236328125, \t Total Dis Loss : 0.003500506281852722\n",
      "Steps : 8300, \t Total Gen Loss : 26.92332649230957, \t Total Dis Loss : 0.0024210354313254356\n",
      "Steps : 8400, \t Total Gen Loss : 27.794958114624023, \t Total Dis Loss : 0.0026586856693029404\n",
      "Steps : 8500, \t Total Gen Loss : 26.356382369995117, \t Total Dis Loss : 0.0072218794375658035\n",
      "Steps : 8600, \t Total Gen Loss : 26.57534408569336, \t Total Dis Loss : 0.00300408573821187\n",
      "Steps : 8700, \t Total Gen Loss : 26.473377227783203, \t Total Dis Loss : 0.004166629631072283\n",
      "Steps : 8800, \t Total Gen Loss : 26.20082664489746, \t Total Dis Loss : 0.002967276144772768\n",
      "Steps : 8900, \t Total Gen Loss : 28.276288986206055, \t Total Dis Loss : 0.015483996830880642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 9000, \t Total Gen Loss : 24.231895446777344, \t Total Dis Loss : 0.011123724281787872\n",
      "Steps : 9100, \t Total Gen Loss : 25.648147583007812, \t Total Dis Loss : 0.005706910043954849\n",
      "Steps : 9200, \t Total Gen Loss : 27.00678062438965, \t Total Dis Loss : 0.003051439765840769\n",
      "Steps : 9300, \t Total Gen Loss : 24.471410751342773, \t Total Dis Loss : 0.2545592784881592\n",
      "Steps : 9400, \t Total Gen Loss : 25.36526107788086, \t Total Dis Loss : 0.0045244996435940266\n",
      "Steps : 9500, \t Total Gen Loss : 26.33062744140625, \t Total Dis Loss : 0.0018835117807611823\n",
      "Steps : 9600, \t Total Gen Loss : 27.64450454711914, \t Total Dis Loss : 0.028279514983296394\n",
      "Steps : 9700, \t Total Gen Loss : 26.167694091796875, \t Total Dis Loss : 0.04369707405567169\n",
      "Steps : 9800, \t Total Gen Loss : 25.798442840576172, \t Total Dis Loss : 0.002382066799327731\n",
      "Steps : 9900, \t Total Gen Loss : 26.846050262451172, \t Total Dis Loss : 0.0010459464974701405\n",
      "Steps : 10000, \t Total Gen Loss : 28.771909713745117, \t Total Dis Loss : 0.004249574616551399\n",
      "Steps : 10100, \t Total Gen Loss : 25.76610565185547, \t Total Dis Loss : 0.02021070383489132\n",
      "Steps : 10200, \t Total Gen Loss : 26.520362854003906, \t Total Dis Loss : 0.006568641401827335\n",
      "Steps : 10300, \t Total Gen Loss : 28.33048439025879, \t Total Dis Loss : 0.0025394996628165245\n",
      "Steps : 10400, \t Total Gen Loss : 23.965105056762695, \t Total Dis Loss : 0.006462959107011557\n",
      "Steps : 10500, \t Total Gen Loss : 27.03451919555664, \t Total Dis Loss : 0.013250419870018959\n",
      "Steps : 10600, \t Total Gen Loss : 27.789955139160156, \t Total Dis Loss : 0.006781676784157753\n",
      "Steps : 10700, \t Total Gen Loss : 26.979358673095703, \t Total Dis Loss : 0.0023204092867672443\n",
      "Steps : 10800, \t Total Gen Loss : 25.916156768798828, \t Total Dis Loss : 0.009975826367735863\n",
      "Steps : 10900, \t Total Gen Loss : 24.087860107421875, \t Total Dis Loss : 0.04829101264476776\n",
      "Steps : 11000, \t Total Gen Loss : 28.813457489013672, \t Total Dis Loss : 0.0027612384874373674\n",
      "Steps : 11100, \t Total Gen Loss : 26.506011962890625, \t Total Dis Loss : 0.0052979267202317715\n",
      "Steps : 11200, \t Total Gen Loss : 25.737417221069336, \t Total Dis Loss : 0.00569493742659688\n",
      "Steps : 11300, \t Total Gen Loss : 27.35357093811035, \t Total Dis Loss : 0.003125517163425684\n",
      "Steps : 11400, \t Total Gen Loss : 26.46086883544922, \t Total Dis Loss : 0.0028946283273398876\n",
      "Steps : 11500, \t Total Gen Loss : 26.633159637451172, \t Total Dis Loss : 0.005966363474726677\n",
      "Steps : 11600, \t Total Gen Loss : 23.996461868286133, \t Total Dis Loss : 0.007167549803853035\n",
      "Steps : 11700, \t Total Gen Loss : 25.85281753540039, \t Total Dis Loss : 0.0033594751730561256\n",
      "Steps : 11800, \t Total Gen Loss : 26.050485610961914, \t Total Dis Loss : 0.0017713290872052312\n",
      "Steps : 11900, \t Total Gen Loss : 26.632410049438477, \t Total Dis Loss : 0.048604726791381836\n",
      "Steps : 12000, \t Total Gen Loss : 28.721275329589844, \t Total Dis Loss : 0.0021390593610703945\n",
      "Steps : 12100, \t Total Gen Loss : 27.608673095703125, \t Total Dis Loss : 0.0011086590820923448\n",
      "Steps : 12200, \t Total Gen Loss : 27.903976440429688, \t Total Dis Loss : 0.0018044927855953574\n",
      "Steps : 12300, \t Total Gen Loss : 29.202579498291016, \t Total Dis Loss : 0.0018542543984949589\n",
      "Steps : 12400, \t Total Gen Loss : 26.573209762573242, \t Total Dis Loss : 0.0026706992648541927\n",
      "Steps : 12500, \t Total Gen Loss : 26.395198822021484, \t Total Dis Loss : 0.005769972689449787\n",
      "Steps : 12600, \t Total Gen Loss : 26.13884735107422, \t Total Dis Loss : 0.002430659020319581\n",
      "Steps : 12700, \t Total Gen Loss : 25.88590431213379, \t Total Dis Loss : 0.00457829050719738\n",
      "Steps : 12800, \t Total Gen Loss : 27.6251277923584, \t Total Dis Loss : 0.0013182060793042183\n",
      "Steps : 12900, \t Total Gen Loss : 27.085878372192383, \t Total Dis Loss : 0.0011635764967650175\n",
      "Steps : 13000, \t Total Gen Loss : 24.08481788635254, \t Total Dis Loss : 0.030383175238966942\n",
      "Steps : 13100, \t Total Gen Loss : 28.56393051147461, \t Total Dis Loss : 0.0010708176996558905\n",
      "Steps : 13200, \t Total Gen Loss : 29.631046295166016, \t Total Dis Loss : 0.01991693675518036\n",
      "Steps : 13300, \t Total Gen Loss : 29.18147087097168, \t Total Dis Loss : 0.0319090262055397\n",
      "Steps : 13400, \t Total Gen Loss : 28.34709358215332, \t Total Dis Loss : 0.001065573189407587\n",
      "Steps : 13500, \t Total Gen Loss : 28.846921920776367, \t Total Dis Loss : 0.001020008698105812\n",
      "Time for epoch 2 is 318.91353726387024 sec\n",
      "Steps : 13600, \t Total Gen Loss : 29.66362762451172, \t Total Dis Loss : 0.0007999194203875959\n",
      "Steps : 13700, \t Total Gen Loss : 27.949514389038086, \t Total Dis Loss : 0.0009491386008448899\n",
      "Steps : 13800, \t Total Gen Loss : 29.016416549682617, \t Total Dis Loss : 0.0007352098473347723\n",
      "Steps : 13900, \t Total Gen Loss : 27.535232543945312, \t Total Dis Loss : 0.0029112843330949545\n",
      "Steps : 14000, \t Total Gen Loss : 28.78400993347168, \t Total Dis Loss : 0.0017325632506981492\n",
      "Steps : 14100, \t Total Gen Loss : 28.342252731323242, \t Total Dis Loss : 0.0037622388917952776\n",
      "Steps : 14200, \t Total Gen Loss : 27.844797134399414, \t Total Dis Loss : 0.0012195573654025793\n",
      "Steps : 14300, \t Total Gen Loss : 27.132152557373047, \t Total Dis Loss : 0.005104418843984604\n",
      "Steps : 14400, \t Total Gen Loss : 27.191274642944336, \t Total Dis Loss : 0.004084684420377016\n",
      "Steps : 14500, \t Total Gen Loss : 24.98815155029297, \t Total Dis Loss : 0.003122527152299881\n",
      "Steps : 14600, \t Total Gen Loss : 27.21432876586914, \t Total Dis Loss : 0.004150701686739922\n",
      "Steps : 14700, \t Total Gen Loss : 25.50592803955078, \t Total Dis Loss : 0.006987500004470348\n",
      "Steps : 14800, \t Total Gen Loss : 27.192384719848633, \t Total Dis Loss : 0.0011812876909971237\n",
      "Steps : 14900, \t Total Gen Loss : 26.078166961669922, \t Total Dis Loss : 0.001685073133558035\n",
      "Steps : 15000, \t Total Gen Loss : 25.115507125854492, \t Total Dis Loss : 0.006199479568749666\n",
      "Steps : 15100, \t Total Gen Loss : 29.297086715698242, \t Total Dis Loss : 0.001081200665794313\n",
      "Steps : 15200, \t Total Gen Loss : 27.922264099121094, \t Total Dis Loss : 0.0012808521278202534\n",
      "Steps : 15300, \t Total Gen Loss : 27.20823860168457, \t Total Dis Loss : 0.001942798146046698\n",
      "Steps : 15400, \t Total Gen Loss : 28.357358932495117, \t Total Dis Loss : 0.0016546715050935745\n",
      "Steps : 15500, \t Total Gen Loss : 28.81523895263672, \t Total Dis Loss : 0.0020640993025153875\n",
      "Steps : 15600, \t Total Gen Loss : 31.24800682067871, \t Total Dis Loss : 0.0024089824873954058\n",
      "Steps : 15700, \t Total Gen Loss : 30.17538833618164, \t Total Dis Loss : 0.0012609822442755103\n",
      "Steps : 15800, \t Total Gen Loss : 28.052839279174805, \t Total Dis Loss : 0.001671724021434784\n",
      "Steps : 15900, \t Total Gen Loss : 29.901905059814453, \t Total Dis Loss : 0.0012918149586766958\n",
      "Steps : 16000, \t Total Gen Loss : 28.191356658935547, \t Total Dis Loss : 0.0026623117737472057\n",
      "Steps : 16100, \t Total Gen Loss : 26.171676635742188, \t Total Dis Loss : 0.0009017715929076076\n",
      "Steps : 16200, \t Total Gen Loss : 30.269351959228516, \t Total Dis Loss : 0.0010514961322769523\n",
      "Steps : 16300, \t Total Gen Loss : 26.711959838867188, \t Total Dis Loss : 0.0014042863622307777\n",
      "Steps : 16400, \t Total Gen Loss : 27.21686363220215, \t Total Dis Loss : 0.0020289195235818624\n",
      "Steps : 16500, \t Total Gen Loss : 26.559040069580078, \t Total Dis Loss : 0.001325385645031929\n",
      "Steps : 16600, \t Total Gen Loss : 29.149335861206055, \t Total Dis Loss : 0.0014494298957288265\n",
      "Steps : 16700, \t Total Gen Loss : 28.90856170654297, \t Total Dis Loss : 0.0007953335298225284\n",
      "Steps : 16800, \t Total Gen Loss : 27.147886276245117, \t Total Dis Loss : 0.0018419743282720447\n",
      "Steps : 16900, \t Total Gen Loss : 27.891286849975586, \t Total Dis Loss : 0.007415328174829483\n",
      "Steps : 17000, \t Total Gen Loss : 27.52263069152832, \t Total Dis Loss : 0.0017053093761205673\n",
      "Steps : 17100, \t Total Gen Loss : 27.546892166137695, \t Total Dis Loss : 0.0006103913765400648\n",
      "Steps : 17200, \t Total Gen Loss : 26.394676208496094, \t Total Dis Loss : 0.0029096028301864862\n",
      "Steps : 17300, \t Total Gen Loss : 27.49527931213379, \t Total Dis Loss : 0.003387077245861292\n",
      "Steps : 17400, \t Total Gen Loss : 28.16606330871582, \t Total Dis Loss : 0.0016441617626696825\n",
      "Steps : 17500, \t Total Gen Loss : 26.475866317749023, \t Total Dis Loss : 0.012132675386965275\n",
      "Steps : 17600, \t Total Gen Loss : 25.55286979675293, \t Total Dis Loss : 0.006624673027545214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 17700, \t Total Gen Loss : 29.960460662841797, \t Total Dis Loss : 0.001664613839238882\n",
      "Steps : 17800, \t Total Gen Loss : 31.235210418701172, \t Total Dis Loss : 0.0030623781494796276\n",
      "Steps : 17900, \t Total Gen Loss : 30.3709716796875, \t Total Dis Loss : 0.001307120663113892\n",
      "Steps : 18000, \t Total Gen Loss : 29.35557746887207, \t Total Dis Loss : 0.0010738512501120567\n",
      "Steps : 18100, \t Total Gen Loss : 31.580867767333984, \t Total Dis Loss : 0.0012100555468350649\n",
      "Steps : 18200, \t Total Gen Loss : 28.677488327026367, \t Total Dis Loss : 0.0019476208835840225\n",
      "Steps : 18300, \t Total Gen Loss : 27.767831802368164, \t Total Dis Loss : 0.004501285031437874\n",
      "Steps : 18400, \t Total Gen Loss : 27.405183792114258, \t Total Dis Loss : 0.003814148250967264\n",
      "Steps : 18500, \t Total Gen Loss : 27.492448806762695, \t Total Dis Loss : 0.0021384425926953554\n",
      "Steps : 18600, \t Total Gen Loss : 27.337059020996094, \t Total Dis Loss : 0.001989545300602913\n",
      "Steps : 18700, \t Total Gen Loss : 27.496389389038086, \t Total Dis Loss : 0.0026052251923829317\n",
      "Steps : 18800, \t Total Gen Loss : 27.327735900878906, \t Total Dis Loss : 0.0013300010468810797\n",
      "Steps : 18900, \t Total Gen Loss : 27.946285247802734, \t Total Dis Loss : 0.0021393606439232826\n",
      "Steps : 19000, \t Total Gen Loss : 28.874771118164062, \t Total Dis Loss : 0.0008330626878887415\n",
      "Steps : 19100, \t Total Gen Loss : 26.92392349243164, \t Total Dis Loss : 0.0009542270563542843\n",
      "Steps : 19200, \t Total Gen Loss : 27.99344825744629, \t Total Dis Loss : 0.0008886826108209789\n",
      "Steps : 19300, \t Total Gen Loss : 28.724674224853516, \t Total Dis Loss : 0.0015952029498293996\n",
      "Steps : 19400, \t Total Gen Loss : 28.713102340698242, \t Total Dis Loss : 0.002066028770059347\n",
      "Steps : 19500, \t Total Gen Loss : 26.25408172607422, \t Total Dis Loss : 0.0015560389729216695\n",
      "Steps : 19600, \t Total Gen Loss : 27.000566482543945, \t Total Dis Loss : 0.0013896867167204618\n",
      "Steps : 19700, \t Total Gen Loss : 27.72755241394043, \t Total Dis Loss : 0.0009640274802222848\n",
      "Steps : 19800, \t Total Gen Loss : 28.490100860595703, \t Total Dis Loss : 0.0011500658001750708\n",
      "Steps : 19900, \t Total Gen Loss : 30.667442321777344, \t Total Dis Loss : 0.0006099002785049379\n",
      "Steps : 20000, \t Total Gen Loss : 27.322341918945312, \t Total Dis Loss : 0.0017221610760316253\n",
      "Steps : 20100, \t Total Gen Loss : 27.926733016967773, \t Total Dis Loss : 0.0004144211998209357\n",
      "Steps : 20200, \t Total Gen Loss : 26.921186447143555, \t Total Dis Loss : 0.0007404005154967308\n",
      "Time for epoch 3 is 306.1642818450928 sec\n",
      "Steps : 20300, \t Total Gen Loss : 26.76364517211914, \t Total Dis Loss : 0.0008153874077834189\n",
      "Steps : 20400, \t Total Gen Loss : 26.033477783203125, \t Total Dis Loss : 0.0007919110357761383\n",
      "Steps : 20500, \t Total Gen Loss : 28.744609832763672, \t Total Dis Loss : 0.0008265894139185548\n",
      "Steps : 20600, \t Total Gen Loss : 29.635684967041016, \t Total Dis Loss : 0.0005099411937408149\n",
      "Steps : 20700, \t Total Gen Loss : 27.9648494720459, \t Total Dis Loss : 0.014329257421195507\n",
      "Steps : 20800, \t Total Gen Loss : 29.063579559326172, \t Total Dis Loss : 0.0002982124569825828\n",
      "Steps : 20900, \t Total Gen Loss : 30.205041885375977, \t Total Dis Loss : 0.0002669627428986132\n",
      "Steps : 21000, \t Total Gen Loss : 28.0218448638916, \t Total Dis Loss : 0.0005474870558828115\n",
      "Steps : 21100, \t Total Gen Loss : 26.519187927246094, \t Total Dis Loss : 0.0004920693463645875\n",
      "Steps : 21200, \t Total Gen Loss : 28.627269744873047, \t Total Dis Loss : 0.0005192215321585536\n",
      "Steps : 21300, \t Total Gen Loss : 29.744699478149414, \t Total Dis Loss : 0.0008000809466466308\n",
      "Steps : 21400, \t Total Gen Loss : 26.08153533935547, \t Total Dis Loss : 0.0005168721545487642\n",
      "Steps : 21500, \t Total Gen Loss : 28.891956329345703, \t Total Dis Loss : 0.00047819281462579966\n",
      "Steps : 21600, \t Total Gen Loss : 27.1082820892334, \t Total Dis Loss : 0.00031784968450665474\n",
      "Steps : 21700, \t Total Gen Loss : 30.84442901611328, \t Total Dis Loss : 0.00029596081003546715\n",
      "Steps : 21800, \t Total Gen Loss : 24.636943817138672, \t Total Dis Loss : 0.013880601152777672\n",
      "Steps : 21900, \t Total Gen Loss : 30.83104133605957, \t Total Dis Loss : 0.0004702354490291327\n",
      "Steps : 22000, \t Total Gen Loss : 31.411245346069336, \t Total Dis Loss : 0.01939215511083603\n",
      "Steps : 22100, \t Total Gen Loss : 32.3260498046875, \t Total Dis Loss : 0.0040062423795461655\n",
      "Steps : 22200, \t Total Gen Loss : 30.89752960205078, \t Total Dis Loss : 0.0010923445224761963\n",
      "Steps : 22300, \t Total Gen Loss : 27.354984283447266, \t Total Dis Loss : 0.0014261935139074922\n",
      "Steps : 22400, \t Total Gen Loss : 26.772294998168945, \t Total Dis Loss : 0.0022896702867001295\n",
      "Steps : 22500, \t Total Gen Loss : 27.389677047729492, \t Total Dis Loss : 0.01768389716744423\n",
      "Steps : 22600, \t Total Gen Loss : 25.75737953186035, \t Total Dis Loss : 0.004720413126051426\n",
      "Steps : 22700, \t Total Gen Loss : 27.84758758544922, \t Total Dis Loss : 0.0007185983704403043\n",
      "Steps : 22800, \t Total Gen Loss : 27.044567108154297, \t Total Dis Loss : 0.0016270685009658337\n",
      "Steps : 22900, \t Total Gen Loss : 27.662769317626953, \t Total Dis Loss : 0.0006254777545109391\n",
      "Steps : 23000, \t Total Gen Loss : 23.926963806152344, \t Total Dis Loss : 0.07773815095424652\n",
      "Steps : 23100, \t Total Gen Loss : 28.297719955444336, \t Total Dis Loss : 0.0005616393173113465\n",
      "Steps : 23200, \t Total Gen Loss : 27.481260299682617, \t Total Dis Loss : 0.001758181257173419\n",
      "Steps : 23300, \t Total Gen Loss : 26.845195770263672, \t Total Dis Loss : 0.0013640762772411108\n",
      "Steps : 23400, \t Total Gen Loss : 28.004606246948242, \t Total Dis Loss : 0.00040046474896371365\n",
      "Steps : 23500, \t Total Gen Loss : 26.0118350982666, \t Total Dis Loss : 0.004224760923534632\n",
      "Steps : 23600, \t Total Gen Loss : 26.96441078186035, \t Total Dis Loss : 0.0011564403539523482\n",
      "Steps : 23700, \t Total Gen Loss : 26.482337951660156, \t Total Dis Loss : 0.0015700384974479675\n",
      "Steps : 23800, \t Total Gen Loss : 25.71661376953125, \t Total Dis Loss : 0.005487455055117607\n",
      "Steps : 23900, \t Total Gen Loss : 27.03778076171875, \t Total Dis Loss : 0.0009748625452630222\n",
      "Steps : 24000, \t Total Gen Loss : 27.993183135986328, \t Total Dis Loss : 0.002211274579167366\n",
      "Steps : 24100, \t Total Gen Loss : 28.113691329956055, \t Total Dis Loss : 0.00044306452036835253\n",
      "Steps : 24200, \t Total Gen Loss : 27.26881980895996, \t Total Dis Loss : 0.0005729073309339583\n",
      "Steps : 24300, \t Total Gen Loss : 26.846487045288086, \t Total Dis Loss : 0.0005474050994962454\n",
      "Steps : 24400, \t Total Gen Loss : 30.083890914916992, \t Total Dis Loss : 0.00026223232271149755\n",
      "Steps : 24500, \t Total Gen Loss : 28.06201171875, \t Total Dis Loss : 0.0009116686997003853\n",
      "Steps : 24600, \t Total Gen Loss : 27.616086959838867, \t Total Dis Loss : 0.000413018831750378\n",
      "Steps : 24700, \t Total Gen Loss : 28.779136657714844, \t Total Dis Loss : 0.0002884008572436869\n",
      "Steps : 24800, \t Total Gen Loss : 29.44210433959961, \t Total Dis Loss : 0.0011420255759730935\n",
      "Steps : 24900, \t Total Gen Loss : 29.23733901977539, \t Total Dis Loss : 0.0002824889379553497\n",
      "Steps : 25000, \t Total Gen Loss : 28.546884536743164, \t Total Dis Loss : 0.0009306552237831056\n",
      "Steps : 25100, \t Total Gen Loss : 26.652587890625, \t Total Dis Loss : 0.0010197126539424062\n",
      "Steps : 25200, \t Total Gen Loss : 26.756866455078125, \t Total Dis Loss : 0.0008709977264516056\n",
      "Steps : 25300, \t Total Gen Loss : 28.21446418762207, \t Total Dis Loss : 0.001056730980053544\n",
      "Steps : 25400, \t Total Gen Loss : 28.810243606567383, \t Total Dis Loss : 0.0005553390947170556\n",
      "Steps : 25500, \t Total Gen Loss : 27.50948143005371, \t Total Dis Loss : 0.0016960434149950743\n",
      "Steps : 25600, \t Total Gen Loss : 29.803192138671875, \t Total Dis Loss : 0.00021661599748767912\n",
      "Steps : 25700, \t Total Gen Loss : 27.75325584411621, \t Total Dis Loss : 0.0010327540803700686\n",
      "Steps : 25800, \t Total Gen Loss : 30.4735107421875, \t Total Dis Loss : 0.002089977962896228\n",
      "Steps : 25900, \t Total Gen Loss : 26.02648162841797, \t Total Dis Loss : 0.42539161443710327\n",
      "Steps : 26000, \t Total Gen Loss : 27.596773147583008, \t Total Dis Loss : 0.007879874669015408\n",
      "Steps : 26100, \t Total Gen Loss : 31.57723617553711, \t Total Dis Loss : 0.0017384820384904742\n",
      "Steps : 26200, \t Total Gen Loss : 30.21570587158203, \t Total Dis Loss : 0.001797282136976719\n",
      "Steps : 26300, \t Total Gen Loss : 27.753938674926758, \t Total Dis Loss : 0.0012046280317008495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 26400, \t Total Gen Loss : 26.025821685791016, \t Total Dis Loss : 0.003434157697483897\n",
      "Steps : 26500, \t Total Gen Loss : 25.946882247924805, \t Total Dis Loss : 0.09313452243804932\n",
      "Steps : 26600, \t Total Gen Loss : 30.298864364624023, \t Total Dis Loss : 0.0012872650986537337\n",
      "Steps : 26700, \t Total Gen Loss : 30.11529541015625, \t Total Dis Loss : 0.007084152195602655\n",
      "Steps : 26800, \t Total Gen Loss : 27.55788230895996, \t Total Dis Loss : 0.430027574300766\n",
      "Steps : 26900, \t Total Gen Loss : 30.190298080444336, \t Total Dis Loss : 0.0005784897366538644\n",
      "Steps : 27000, \t Total Gen Loss : 31.968358993530273, \t Total Dis Loss : 0.000663219834677875\n",
      "Time for epoch 4 is 306.42420172691345 sec\n",
      "Steps : 27100, \t Total Gen Loss : 28.661375045776367, \t Total Dis Loss : 0.004956888500601053\n",
      "Steps : 27200, \t Total Gen Loss : 31.132768630981445, \t Total Dis Loss : 0.00031404360197484493\n",
      "Steps : 27300, \t Total Gen Loss : 27.49183464050293, \t Total Dis Loss : 0.0006220301147550344\n",
      "Steps : 27400, \t Total Gen Loss : 29.358230590820312, \t Total Dis Loss : 0.0008367978734895587\n",
      "Steps : 27500, \t Total Gen Loss : 28.33746910095215, \t Total Dis Loss : 0.0007134564220905304\n",
      "Steps : 27600, \t Total Gen Loss : 28.849124908447266, \t Total Dis Loss : 0.00031793012749403715\n",
      "Steps : 27700, \t Total Gen Loss : 26.120471954345703, \t Total Dis Loss : 0.009081977419555187\n",
      "Steps : 27800, \t Total Gen Loss : 29.808055877685547, \t Total Dis Loss : 0.00042907256283797324\n",
      "Steps : 27900, \t Total Gen Loss : 27.95457649230957, \t Total Dis Loss : 0.0006530790124088526\n",
      "Steps : 28000, \t Total Gen Loss : 28.00988006591797, \t Total Dis Loss : 0.0005895263748243451\n",
      "Steps : 28100, \t Total Gen Loss : 27.29474639892578, \t Total Dis Loss : 0.0006396481185220182\n",
      "Steps : 28200, \t Total Gen Loss : 28.73651885986328, \t Total Dis Loss : 0.0004810912942048162\n",
      "Steps : 28300, \t Total Gen Loss : 29.45992660522461, \t Total Dis Loss : 0.00024021227727644145\n",
      "Steps : 28400, \t Total Gen Loss : 27.741788864135742, \t Total Dis Loss : 0.0009477507555857301\n",
      "Steps : 28500, \t Total Gen Loss : 27.897470474243164, \t Total Dis Loss : 0.001816151081584394\n",
      "Steps : 28600, \t Total Gen Loss : 27.274181365966797, \t Total Dis Loss : 0.004246306139975786\n",
      "Steps : 28700, \t Total Gen Loss : 29.27435874938965, \t Total Dis Loss : 0.0005230667884461582\n",
      "Steps : 28800, \t Total Gen Loss : 26.04343605041504, \t Total Dis Loss : 0.0012372415512800217\n",
      "Steps : 28900, \t Total Gen Loss : 26.13918113708496, \t Total Dis Loss : 0.0015619804617017508\n",
      "Steps : 29000, \t Total Gen Loss : 27.415935516357422, \t Total Dis Loss : 0.0009121597395278513\n",
      "Steps : 29100, \t Total Gen Loss : 28.01166534423828, \t Total Dis Loss : 0.0020244279876351357\n",
      "Steps : 29200, \t Total Gen Loss : 27.151155471801758, \t Total Dis Loss : 0.0013016615994274616\n",
      "Steps : 29300, \t Total Gen Loss : 26.304664611816406, \t Total Dis Loss : 0.003325806697830558\n",
      "Steps : 29400, \t Total Gen Loss : 27.421598434448242, \t Total Dis Loss : 0.0026909406296908855\n",
      "Steps : 29500, \t Total Gen Loss : 28.40922737121582, \t Total Dis Loss : 0.0011489037424325943\n",
      "Steps : 29600, \t Total Gen Loss : 27.912189483642578, \t Total Dis Loss : 0.002354165306314826\n",
      "Steps : 29700, \t Total Gen Loss : 29.77897834777832, \t Total Dis Loss : 0.0005915551446378231\n",
      "Steps : 29800, \t Total Gen Loss : 24.66425323486328, \t Total Dis Loss : 0.0008078378159552813\n",
      "Steps : 29900, \t Total Gen Loss : 28.71236228942871, \t Total Dis Loss : 0.003942538518458605\n",
      "Steps : 30000, \t Total Gen Loss : 30.498563766479492, \t Total Dis Loss : 0.0009577730088494718\n",
      "Steps : 30100, \t Total Gen Loss : 29.900880813598633, \t Total Dis Loss : 0.0010948363924399018\n",
      "Steps : 30200, \t Total Gen Loss : 30.26144790649414, \t Total Dis Loss : 0.0007006327505223453\n",
      "Steps : 30300, \t Total Gen Loss : 28.797273635864258, \t Total Dis Loss : 0.0011716136941686273\n",
      "Steps : 30400, \t Total Gen Loss : 28.630556106567383, \t Total Dis Loss : 0.0004017964529339224\n",
      "Steps : 30500, \t Total Gen Loss : 29.127321243286133, \t Total Dis Loss : 0.0003416956460569054\n",
      "Steps : 30600, \t Total Gen Loss : 30.805131912231445, \t Total Dis Loss : 0.0005037739174440503\n",
      "Steps : 30700, \t Total Gen Loss : 33.29438781738281, \t Total Dis Loss : 0.0013856319710612297\n",
      "Steps : 30800, \t Total Gen Loss : 32.546756744384766, \t Total Dis Loss : 0.0006683178362436593\n",
      "Steps : 30900, \t Total Gen Loss : 30.40288543701172, \t Total Dis Loss : 0.0007408321835100651\n",
      "Steps : 31000, \t Total Gen Loss : 30.346603393554688, \t Total Dis Loss : 0.00572358351200819\n",
      "Steps : 31100, \t Total Gen Loss : 28.980854034423828, \t Total Dis Loss : 0.0005190760130062699\n",
      "Steps : 31200, \t Total Gen Loss : 31.61166000366211, \t Total Dis Loss : 0.0009825234301388264\n",
      "Steps : 31300, \t Total Gen Loss : 30.9285831451416, \t Total Dis Loss : 0.001239623175933957\n",
      "Steps : 31400, \t Total Gen Loss : 28.466243743896484, \t Total Dis Loss : 0.01539838220924139\n",
      "Steps : 31500, \t Total Gen Loss : 32.90895080566406, \t Total Dis Loss : 0.0014095627702772617\n",
      "Steps : 31600, \t Total Gen Loss : 32.00484085083008, \t Total Dis Loss : 0.001266465988010168\n",
      "Steps : 31700, \t Total Gen Loss : 31.25958824157715, \t Total Dis Loss : 0.025251036509871483\n",
      "Steps : 31800, \t Total Gen Loss : 30.397592544555664, \t Total Dis Loss : 0.0005050015170127153\n",
      "Steps : 31900, \t Total Gen Loss : 30.441226959228516, \t Total Dis Loss : 0.001113750389777124\n",
      "Steps : 32000, \t Total Gen Loss : 29.58939552307129, \t Total Dis Loss : 0.0007991471793502569\n",
      "Steps : 32100, \t Total Gen Loss : 30.353302001953125, \t Total Dis Loss : 0.002528732642531395\n",
      "Steps : 32200, \t Total Gen Loss : 31.193811416625977, \t Total Dis Loss : 0.0003227654960937798\n",
      "Steps : 32300, \t Total Gen Loss : 30.513011932373047, \t Total Dis Loss : 0.00031258200760930777\n",
      "Steps : 32400, \t Total Gen Loss : 29.75765609741211, \t Total Dis Loss : 0.0004750638036057353\n",
      "Steps : 32500, \t Total Gen Loss : 30.828706741333008, \t Total Dis Loss : 0.0003126622177660465\n",
      "Steps : 32600, \t Total Gen Loss : 32.53049087524414, \t Total Dis Loss : 0.0004363062616903335\n",
      "Steps : 32700, \t Total Gen Loss : 26.166406631469727, \t Total Dis Loss : 0.0016663807909935713\n",
      "Steps : 32800, \t Total Gen Loss : 29.828807830810547, \t Total Dis Loss : 0.0023988927714526653\n",
      "Steps : 32900, \t Total Gen Loss : 29.717397689819336, \t Total Dis Loss : 0.0012669623829424381\n",
      "Steps : 33000, \t Total Gen Loss : 26.56025505065918, \t Total Dis Loss : 0.0030392503831535578\n",
      "Steps : 33100, \t Total Gen Loss : 29.9066219329834, \t Total Dis Loss : 0.010836765170097351\n",
      "Steps : 33200, \t Total Gen Loss : 29.961559295654297, \t Total Dis Loss : 0.0015436968533322215\n",
      "Steps : 33300, \t Total Gen Loss : 29.54474449157715, \t Total Dis Loss : 0.00128472363576293\n",
      "Steps : 33400, \t Total Gen Loss : 31.28702735900879, \t Total Dis Loss : 0.0022149733267724514\n",
      "Steps : 33500, \t Total Gen Loss : 29.789958953857422, \t Total Dis Loss : 0.001429894007742405\n",
      "Steps : 33600, \t Total Gen Loss : 34.32599639892578, \t Total Dis Loss : 0.00018943317991215736\n",
      "Steps : 33700, \t Total Gen Loss : 29.627859115600586, \t Total Dis Loss : 0.002673726063221693\n",
      "Time for epoch 5 is 302.06918573379517 sec\n",
      "Steps : 33800, \t Total Gen Loss : 29.917945861816406, \t Total Dis Loss : 0.001330972881987691\n",
      "Steps : 33900, \t Total Gen Loss : 30.504718780517578, \t Total Dis Loss : 0.000375849602278322\n",
      "Steps : 34000, \t Total Gen Loss : 27.721345901489258, \t Total Dis Loss : 0.00029423669911921024\n",
      "Steps : 34100, \t Total Gen Loss : 30.864267349243164, \t Total Dis Loss : 0.00306883011944592\n",
      "Steps : 34200, \t Total Gen Loss : 30.174211502075195, \t Total Dis Loss : 0.0010695797391235828\n",
      "Steps : 34300, \t Total Gen Loss : 30.75277328491211, \t Total Dis Loss : 0.00023261232126969844\n",
      "Steps : 34400, \t Total Gen Loss : 29.358644485473633, \t Total Dis Loss : 0.0004078429192304611\n",
      "Steps : 34500, \t Total Gen Loss : 28.576580047607422, \t Total Dis Loss : 0.0007827606168575585\n",
      "Steps : 34600, \t Total Gen Loss : 27.590944290161133, \t Total Dis Loss : 0.0014251202810555696\n",
      "Steps : 34700, \t Total Gen Loss : 29.94796371459961, \t Total Dis Loss : 0.0015798374079167843\n",
      "Steps : 34800, \t Total Gen Loss : 31.795663833618164, \t Total Dis Loss : 0.0009230375871993601\n",
      "Steps : 34900, \t Total Gen Loss : 28.168109893798828, \t Total Dis Loss : 0.0010019349865615368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 35000, \t Total Gen Loss : 31.0236873626709, \t Total Dis Loss : 0.0018526073545217514\n",
      "Steps : 35100, \t Total Gen Loss : 31.233840942382812, \t Total Dis Loss : 0.004121688194572926\n",
      "Steps : 35200, \t Total Gen Loss : 34.757781982421875, \t Total Dis Loss : 0.0018450117204338312\n",
      "Steps : 35300, \t Total Gen Loss : 29.79713249206543, \t Total Dis Loss : 0.0010551715968176723\n",
      "Steps : 35400, \t Total Gen Loss : 30.49118423461914, \t Total Dis Loss : 0.0008543945150449872\n",
      "Steps : 35500, \t Total Gen Loss : 30.07830810546875, \t Total Dis Loss : 0.0004498368361964822\n",
      "Steps : 35600, \t Total Gen Loss : 31.35222625732422, \t Total Dis Loss : 0.0002501527778804302\n",
      "Steps : 35700, \t Total Gen Loss : 31.009944915771484, \t Total Dis Loss : 0.0001865242375060916\n",
      "Steps : 35800, \t Total Gen Loss : 27.978103637695312, \t Total Dis Loss : 0.0003928965888917446\n",
      "Steps : 35900, \t Total Gen Loss : 29.430879592895508, \t Total Dis Loss : 0.0013359307777136564\n",
      "Steps : 36000, \t Total Gen Loss : 30.591381072998047, \t Total Dis Loss : 0.0002813452738337219\n",
      "Steps : 36100, \t Total Gen Loss : 28.209091186523438, \t Total Dis Loss : 0.0005787502159364522\n",
      "Steps : 36200, \t Total Gen Loss : 30.072418212890625, \t Total Dis Loss : 0.000528887496329844\n",
      "Steps : 36300, \t Total Gen Loss : 28.41680335998535, \t Total Dis Loss : 0.00035057857166975737\n",
      "Steps : 36400, \t Total Gen Loss : 28.863039016723633, \t Total Dis Loss : 0.0005665229400619864\n",
      "Steps : 36500, \t Total Gen Loss : 28.61488151550293, \t Total Dis Loss : 0.0005033515044488013\n",
      "Steps : 36600, \t Total Gen Loss : 25.866544723510742, \t Total Dis Loss : 0.006997487507760525\n",
      "Steps : 36700, \t Total Gen Loss : 27.31719970703125, \t Total Dis Loss : 0.0010181558318436146\n",
      "Steps : 36800, \t Total Gen Loss : 27.471529006958008, \t Total Dis Loss : 0.0008933534263633192\n",
      "Steps : 36900, \t Total Gen Loss : 29.051898956298828, \t Total Dis Loss : 0.0013230115873739123\n",
      "Steps : 37000, \t Total Gen Loss : 29.197277069091797, \t Total Dis Loss : 0.0006685298285447061\n",
      "Steps : 37100, \t Total Gen Loss : 28.33783721923828, \t Total Dis Loss : 0.0008427105494774878\n",
      "Steps : 37200, \t Total Gen Loss : 29.48226547241211, \t Total Dis Loss : 0.0007672470528632402\n",
      "Steps : 37300, \t Total Gen Loss : 25.987171173095703, \t Total Dis Loss : 0.0011945795267820358\n",
      "Steps : 37400, \t Total Gen Loss : 26.61551856994629, \t Total Dis Loss : 0.0014756381278857589\n",
      "Steps : 37500, \t Total Gen Loss : 28.11680030822754, \t Total Dis Loss : 0.00046962162014096975\n",
      "Steps : 37600, \t Total Gen Loss : 26.132976531982422, \t Total Dis Loss : 0.10748347640037537\n",
      "Steps : 37700, \t Total Gen Loss : 27.956937789916992, \t Total Dis Loss : 0.0006285469280555844\n",
      "Steps : 37800, \t Total Gen Loss : 25.790117263793945, \t Total Dis Loss : 0.1034504622220993\n",
      "Steps : 37900, \t Total Gen Loss : 27.913827896118164, \t Total Dis Loss : 0.0008997073164209723\n",
      "Steps : 38000, \t Total Gen Loss : 29.028682708740234, \t Total Dis Loss : 0.0008098548278212547\n",
      "Steps : 38100, \t Total Gen Loss : 29.565378189086914, \t Total Dis Loss : 0.000846895738504827\n",
      "Steps : 38200, \t Total Gen Loss : 28.519729614257812, \t Total Dis Loss : 0.00027613062411546707\n",
      "Steps : 38300, \t Total Gen Loss : 30.573163986206055, \t Total Dis Loss : 0.00024061549629550427\n",
      "Steps : 38400, \t Total Gen Loss : 27.898019790649414, \t Total Dis Loss : 0.0003171723219566047\n",
      "Steps : 38500, \t Total Gen Loss : 30.569931030273438, \t Total Dis Loss : 0.0003371103957761079\n",
      "Steps : 38600, \t Total Gen Loss : 28.7681941986084, \t Total Dis Loss : 0.0004641261475626379\n",
      "Steps : 38700, \t Total Gen Loss : 29.245927810668945, \t Total Dis Loss : 0.00024820625549182296\n",
      "Steps : 38800, \t Total Gen Loss : 29.030351638793945, \t Total Dis Loss : 0.0007817277801223099\n",
      "Steps : 38900, \t Total Gen Loss : 27.033262252807617, \t Total Dis Loss : 0.000990440254099667\n",
      "Steps : 39000, \t Total Gen Loss : 29.25235939025879, \t Total Dis Loss : 0.0014657949795946479\n",
      "Steps : 39100, \t Total Gen Loss : 25.963542938232422, \t Total Dis Loss : 0.7760691046714783\n",
      "Steps : 39200, \t Total Gen Loss : 27.738615036010742, \t Total Dis Loss : 0.0006926166824996471\n",
      "Steps : 39300, \t Total Gen Loss : 26.36316680908203, \t Total Dis Loss : 0.0032784058712422848\n",
      "Steps : 39400, \t Total Gen Loss : 27.90778923034668, \t Total Dis Loss : 0.0018657483160495758\n",
      "Steps : 39500, \t Total Gen Loss : 29.078466415405273, \t Total Dis Loss : 0.0002857075887732208\n",
      "Steps : 39600, \t Total Gen Loss : 27.94293212890625, \t Total Dis Loss : 0.00034010756644420326\n",
      "Steps : 39700, \t Total Gen Loss : 29.318445205688477, \t Total Dis Loss : 0.0002787135308608413\n",
      "Steps : 39800, \t Total Gen Loss : 29.14653968811035, \t Total Dis Loss : 0.0013513116864487529\n",
      "Steps : 39900, \t Total Gen Loss : 27.986474990844727, \t Total Dis Loss : 0.00018239454948343337\n",
      "Steps : 40000, \t Total Gen Loss : 29.0169620513916, \t Total Dis Loss : 0.00022344487661030143\n",
      "Steps : 40100, \t Total Gen Loss : 28.663217544555664, \t Total Dis Loss : 0.00020525703439489007\n",
      "Steps : 40200, \t Total Gen Loss : 27.791812896728516, \t Total Dis Loss : 0.0004278504056856036\n",
      "Steps : 40300, \t Total Gen Loss : 26.177858352661133, \t Total Dis Loss : 0.001578644965775311\n",
      "Steps : 40400, \t Total Gen Loss : 28.135189056396484, \t Total Dis Loss : 0.000766935758292675\n",
      "Steps : 40500, \t Total Gen Loss : 27.010711669921875, \t Total Dis Loss : 0.0006412992952391505\n",
      "Time for epoch 6 is 301.12184286117554 sec\n",
      "Steps : 40600, \t Total Gen Loss : 28.6787109375, \t Total Dis Loss : 0.0005509569891728461\n",
      "Steps : 40700, \t Total Gen Loss : 28.817541122436523, \t Total Dis Loss : 0.000835322542116046\n",
      "Steps : 40800, \t Total Gen Loss : 28.29561424255371, \t Total Dis Loss : 0.00039113545790314674\n",
      "Steps : 40900, \t Total Gen Loss : 28.297780990600586, \t Total Dis Loss : 0.0007556420750916004\n",
      "Steps : 41000, \t Total Gen Loss : 28.20623207092285, \t Total Dis Loss : 0.0005361519870348275\n",
      "Steps : 41100, \t Total Gen Loss : 28.392621994018555, \t Total Dis Loss : 0.002809572499245405\n",
      "Steps : 41200, \t Total Gen Loss : 30.479156494140625, \t Total Dis Loss : 0.0011455988278612494\n",
      "Steps : 41300, \t Total Gen Loss : 30.47435760498047, \t Total Dis Loss : 0.0007390830432996154\n",
      "Steps : 41400, \t Total Gen Loss : 32.43040466308594, \t Total Dis Loss : 0.001007424551062286\n",
      "Steps : 41500, \t Total Gen Loss : 31.15032386779785, \t Total Dis Loss : 0.0008555542444810271\n",
      "Steps : 41600, \t Total Gen Loss : 31.614458084106445, \t Total Dis Loss : 0.001780909951776266\n",
      "Steps : 41700, \t Total Gen Loss : 31.37715721130371, \t Total Dis Loss : 0.0010225593578070402\n",
      "Steps : 41800, \t Total Gen Loss : 30.191164016723633, \t Total Dis Loss : 0.0008444793056696653\n",
      "Steps : 41900, \t Total Gen Loss : 30.058364868164062, \t Total Dis Loss : 0.0006173982983455062\n",
      "Steps : 42000, \t Total Gen Loss : 29.467159271240234, \t Total Dis Loss : 0.00046737660886719823\n",
      "Steps : 42100, \t Total Gen Loss : 30.25516128540039, \t Total Dis Loss : 0.00038613725337199867\n",
      "Steps : 42200, \t Total Gen Loss : 31.401901245117188, \t Total Dis Loss : 0.0007702177972532809\n",
      "Steps : 42300, \t Total Gen Loss : 31.552452087402344, \t Total Dis Loss : 0.0001742251479299739\n",
      "Steps : 42400, \t Total Gen Loss : 31.679710388183594, \t Total Dis Loss : 0.0003658368659671396\n",
      "Steps : 42500, \t Total Gen Loss : 29.845287322998047, \t Total Dis Loss : 0.00025931777781806886\n",
      "Steps : 42600, \t Total Gen Loss : 31.745004653930664, \t Total Dis Loss : 0.00023148390755522996\n",
      "Steps : 42700, \t Total Gen Loss : 31.08150863647461, \t Total Dis Loss : 0.00016951406723819673\n",
      "Steps : 42800, \t Total Gen Loss : 30.88072395324707, \t Total Dis Loss : 0.007542099803686142\n",
      "Steps : 42900, \t Total Gen Loss : 31.010692596435547, \t Total Dis Loss : 0.000211305872653611\n",
      "Steps : 43000, \t Total Gen Loss : 30.417400360107422, \t Total Dis Loss : 0.0001746713533066213\n",
      "Steps : 43100, \t Total Gen Loss : 30.633071899414062, \t Total Dis Loss : 0.00019170298764947802\n",
      "Steps : 43200, \t Total Gen Loss : 31.155912399291992, \t Total Dis Loss : 0.00029979116516187787\n",
      "Steps : 43300, \t Total Gen Loss : 30.30666160583496, \t Total Dis Loss : 0.00038820772897452116\n",
      "Steps : 43400, \t Total Gen Loss : 30.85544204711914, \t Total Dis Loss : 0.00148236402310431\n",
      "Steps : 43500, \t Total Gen Loss : 26.971065521240234, \t Total Dis Loss : 0.00437870342284441\n",
      "Steps : 43600, \t Total Gen Loss : 31.954803466796875, \t Total Dis Loss : 0.0016022052150219679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 43700, \t Total Gen Loss : 31.058557510375977, \t Total Dis Loss : 0.006522274576127529\n",
      "Steps : 43800, \t Total Gen Loss : 28.266117095947266, \t Total Dis Loss : 0.006225984077900648\n",
      "Steps : 43900, \t Total Gen Loss : 28.983333587646484, \t Total Dis Loss : 0.0012585227377712727\n",
      "Steps : 44000, \t Total Gen Loss : 33.57740020751953, \t Total Dis Loss : 0.0006982546183280647\n",
      "Steps : 44100, \t Total Gen Loss : 32.92500686645508, \t Total Dis Loss : 0.0009705732809379697\n",
      "Steps : 44200, \t Total Gen Loss : 28.62693977355957, \t Total Dis Loss : 0.002976267831400037\n",
      "Steps : 44300, \t Total Gen Loss : 31.83908462524414, \t Total Dis Loss : 0.0007511991425417364\n",
      "Steps : 44400, \t Total Gen Loss : 32.68534851074219, \t Total Dis Loss : 0.0005504264263436198\n",
      "Steps : 44500, \t Total Gen Loss : 30.069015502929688, \t Total Dis Loss : 0.0002715448208618909\n",
      "Steps : 44600, \t Total Gen Loss : 27.355106353759766, \t Total Dis Loss : 0.002927638590335846\n",
      "Steps : 44700, \t Total Gen Loss : 28.7154541015625, \t Total Dis Loss : 0.0006329984171316028\n",
      "Steps : 44800, \t Total Gen Loss : 29.01739501953125, \t Total Dis Loss : 0.11853032559156418\n",
      "Steps : 44900, \t Total Gen Loss : 32.11474609375, \t Total Dis Loss : 0.00038376302109099925\n",
      "Steps : 45000, \t Total Gen Loss : 29.59626007080078, \t Total Dis Loss : 0.0003097026201430708\n",
      "Steps : 45100, \t Total Gen Loss : 28.59796905517578, \t Total Dis Loss : 0.0006528932135552168\n",
      "Steps : 45200, \t Total Gen Loss : 32.02218246459961, \t Total Dis Loss : 0.0010060265194624662\n",
      "Steps : 45300, \t Total Gen Loss : 31.55044174194336, \t Total Dis Loss : 0.0017689564265310764\n",
      "Steps : 45400, \t Total Gen Loss : 31.466339111328125, \t Total Dis Loss : 0.06430162489414215\n",
      "Steps : 45500, \t Total Gen Loss : 34.978912353515625, \t Total Dis Loss : 0.0005699153989553452\n",
      "Steps : 45600, \t Total Gen Loss : 30.594379425048828, \t Total Dis Loss : 0.0007523265667259693\n",
      "Steps : 45700, \t Total Gen Loss : 29.923261642456055, \t Total Dis Loss : 0.0004627663583960384\n",
      "Steps : 45800, \t Total Gen Loss : 32.54079055786133, \t Total Dis Loss : 0.00020025629783049226\n",
      "Steps : 45900, \t Total Gen Loss : 30.553794860839844, \t Total Dis Loss : 0.0007936760666780174\n",
      "Steps : 46000, \t Total Gen Loss : 30.781770706176758, \t Total Dis Loss : 0.0003711059980560094\n",
      "Steps : 46100, \t Total Gen Loss : 29.623611450195312, \t Total Dis Loss : 0.0006552367121912539\n",
      "Steps : 46200, \t Total Gen Loss : 26.470890045166016, \t Total Dis Loss : 0.0017024650005623698\n",
      "Steps : 46300, \t Total Gen Loss : 31.95493507385254, \t Total Dis Loss : 0.0005265298532322049\n",
      "Steps : 46400, \t Total Gen Loss : 34.02191925048828, \t Total Dis Loss : 0.00020414049504324794\n",
      "Steps : 46500, \t Total Gen Loss : 30.574338912963867, \t Total Dis Loss : 0.00025621827808208764\n",
      "Steps : 46600, \t Total Gen Loss : 32.23060607910156, \t Total Dis Loss : 0.00019414164125919342\n",
      "Steps : 46700, \t Total Gen Loss : 30.524351119995117, \t Total Dis Loss : 0.0006374794756993651\n",
      "Steps : 46800, \t Total Gen Loss : 33.37591552734375, \t Total Dis Loss : 0.0005005510756745934\n",
      "Steps : 46900, \t Total Gen Loss : 31.438007354736328, \t Total Dis Loss : 0.0001463927619624883\n",
      "Steps : 47000, \t Total Gen Loss : 29.963226318359375, \t Total Dis Loss : 0.00010646862210705876\n",
      "Steps : 47100, \t Total Gen Loss : 29.346546173095703, \t Total Dis Loss : 0.0018754657357931137\n",
      "Steps : 47200, \t Total Gen Loss : 30.90922737121582, \t Total Dis Loss : 0.00016223007696680725\n",
      "Time for epoch 7 is 306.3579487800598 sec\n",
      "Steps : 47300, \t Total Gen Loss : 31.869760513305664, \t Total Dis Loss : 5.803573367302306e-05\n",
      "Steps : 47400, \t Total Gen Loss : 32.263648986816406, \t Total Dis Loss : 0.00011983406147919595\n",
      "Steps : 47500, \t Total Gen Loss : 30.971410751342773, \t Total Dis Loss : 0.0003323803248349577\n",
      "Steps : 47600, \t Total Gen Loss : 30.54545021057129, \t Total Dis Loss : 0.00025413717958144844\n",
      "Steps : 47700, \t Total Gen Loss : 30.46278953552246, \t Total Dis Loss : 0.00020405984832905233\n",
      "Steps : 47800, \t Total Gen Loss : 31.032909393310547, \t Total Dis Loss : 0.0003207646659575403\n",
      "Steps : 47900, \t Total Gen Loss : 31.909679412841797, \t Total Dis Loss : 0.00018749808077700436\n",
      "Steps : 48000, \t Total Gen Loss : 31.409082412719727, \t Total Dis Loss : 0.0003777737729251385\n",
      "Steps : 48100, \t Total Gen Loss : 30.72478675842285, \t Total Dis Loss : 0.0007164538837969303\n",
      "Steps : 48200, \t Total Gen Loss : 31.180334091186523, \t Total Dis Loss : 0.0016953963786363602\n",
      "Steps : 48300, \t Total Gen Loss : 28.585521697998047, \t Total Dis Loss : 0.0011853270698338747\n",
      "Steps : 48400, \t Total Gen Loss : 30.731294631958008, \t Total Dis Loss : 0.00046114553697407246\n",
      "Steps : 48500, \t Total Gen Loss : 30.915149688720703, \t Total Dis Loss : 0.0009483376052230597\n",
      "Steps : 48600, \t Total Gen Loss : 28.8780574798584, \t Total Dis Loss : 0.0005333275184966624\n",
      "Steps : 48700, \t Total Gen Loss : 30.38683319091797, \t Total Dis Loss : 0.00030981015879660845\n",
      "Steps : 48800, \t Total Gen Loss : 28.70254898071289, \t Total Dis Loss : 0.0002871020988095552\n",
      "Steps : 48900, \t Total Gen Loss : 28.288488388061523, \t Total Dis Loss : 0.00018465919129084796\n",
      "Steps : 49000, \t Total Gen Loss : 29.03389549255371, \t Total Dis Loss : 0.00018295148038305342\n",
      "Steps : 49100, \t Total Gen Loss : 28.930614471435547, \t Total Dis Loss : 0.00017342275532428175\n",
      "Steps : 49200, \t Total Gen Loss : 31.211238861083984, \t Total Dis Loss : 0.00015891174552962184\n",
      "Steps : 49300, \t Total Gen Loss : 31.748897552490234, \t Total Dis Loss : 0.00016049684199970216\n",
      "Steps : 49400, \t Total Gen Loss : 29.773086547851562, \t Total Dis Loss : 0.00024340092204511166\n",
      "Steps : 49500, \t Total Gen Loss : 30.166959762573242, \t Total Dis Loss : 0.00017266394570469856\n",
      "Steps : 49600, \t Total Gen Loss : 31.934755325317383, \t Total Dis Loss : 0.0001370205427519977\n",
      "Steps : 49700, \t Total Gen Loss : 29.31720542907715, \t Total Dis Loss : 0.0002748275292105973\n",
      "Steps : 49800, \t Total Gen Loss : 30.6153621673584, \t Total Dis Loss : 0.00016626057913526893\n",
      "Steps : 49900, \t Total Gen Loss : 32.934268951416016, \t Total Dis Loss : 0.0005721151828765869\n",
      "Steps : 50000, \t Total Gen Loss : 28.787948608398438, \t Total Dis Loss : 0.00018203584477305412\n",
      "Steps : 50100, \t Total Gen Loss : 29.795869827270508, \t Total Dis Loss : 9.33113187784329e-05\n",
      "Steps : 50200, \t Total Gen Loss : 32.6173095703125, \t Total Dis Loss : 6.58895296510309e-05\n",
      "Steps : 50300, \t Total Gen Loss : 27.86373519897461, \t Total Dis Loss : 0.0009401630377396941\n",
      "Steps : 50400, \t Total Gen Loss : 27.32719612121582, \t Total Dis Loss : 0.0004898779443465173\n",
      "Steps : 50500, \t Total Gen Loss : 28.253074645996094, \t Total Dis Loss : 0.0005099523696117103\n",
      "Steps : 50600, \t Total Gen Loss : 27.696945190429688, \t Total Dis Loss : 0.0005785722169093788\n",
      "Steps : 50700, \t Total Gen Loss : 28.25197410583496, \t Total Dis Loss : 0.00023747337399981916\n",
      "Steps : 50800, \t Total Gen Loss : 28.407543182373047, \t Total Dis Loss : 0.00013502240472007543\n",
      "Steps : 50900, \t Total Gen Loss : 28.069440841674805, \t Total Dis Loss : 0.0002422603574814275\n",
      "Steps : 51000, \t Total Gen Loss : 30.09449577331543, \t Total Dis Loss : 0.00014107688912190497\n",
      "Steps : 51100, \t Total Gen Loss : 28.276138305664062, \t Total Dis Loss : 0.0003542949561960995\n",
      "Steps : 51200, \t Total Gen Loss : 27.4770450592041, \t Total Dis Loss : 0.00039824098348617554\n",
      "Steps : 51300, \t Total Gen Loss : 26.395326614379883, \t Total Dis Loss : 0.019785329699516296\n",
      "Steps : 51400, \t Total Gen Loss : 27.88045883178711, \t Total Dis Loss : 0.00048381261876784265\n",
      "Steps : 51500, \t Total Gen Loss : 28.185117721557617, \t Total Dis Loss : 0.0007890936685726047\n",
      "Steps : 51600, \t Total Gen Loss : 29.29996109008789, \t Total Dis Loss : 9.58173768594861e-05\n",
      "Steps : 51700, \t Total Gen Loss : 28.98777198791504, \t Total Dis Loss : 0.0004096466873306781\n",
      "Steps : 51800, \t Total Gen Loss : 28.967514038085938, \t Total Dis Loss : 0.00020663102623075247\n",
      "Steps : 51900, \t Total Gen Loss : 30.37518882751465, \t Total Dis Loss : 0.00010539968207012862\n",
      "Steps : 52000, \t Total Gen Loss : 28.526798248291016, \t Total Dis Loss : 0.00010079314233735204\n",
      "Steps : 52100, \t Total Gen Loss : 29.358802795410156, \t Total Dis Loss : 0.000704231031704694\n",
      "Steps : 52200, \t Total Gen Loss : 27.11638832092285, \t Total Dis Loss : 0.0005431831232272089\n",
      "Steps : 52300, \t Total Gen Loss : 31.01173973083496, \t Total Dis Loss : 0.00037002903991378844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 52400, \t Total Gen Loss : 28.292865753173828, \t Total Dis Loss : 0.000289047893602401\n",
      "Steps : 52500, \t Total Gen Loss : 31.048608779907227, \t Total Dis Loss : 0.00014900346286594868\n",
      "Steps : 52600, \t Total Gen Loss : 29.179443359375, \t Total Dis Loss : 0.0003940924652852118\n",
      "Steps : 52700, \t Total Gen Loss : 28.176179885864258, \t Total Dis Loss : 0.0005200684536248446\n",
      "Steps : 52800, \t Total Gen Loss : 31.897045135498047, \t Total Dis Loss : 0.00015895300020929426\n",
      "Steps : 52900, \t Total Gen Loss : 31.06897735595703, \t Total Dis Loss : 0.00010537594789639115\n",
      "Steps : 53000, \t Total Gen Loss : 28.132400512695312, \t Total Dis Loss : 0.00045639550080522895\n",
      "Steps : 53100, \t Total Gen Loss : 30.250268936157227, \t Total Dis Loss : 0.00040442950557917356\n",
      "Steps : 53200, \t Total Gen Loss : 30.44864845275879, \t Total Dis Loss : 0.00014272425323724747\n",
      "Steps : 53300, \t Total Gen Loss : 30.162342071533203, \t Total Dis Loss : 0.0004102447419427335\n",
      "Steps : 53400, \t Total Gen Loss : 28.47199821472168, \t Total Dis Loss : 0.00010091021249536425\n",
      "Steps : 53500, \t Total Gen Loss : 29.156843185424805, \t Total Dis Loss : 0.0001142829714808613\n",
      "Steps : 53600, \t Total Gen Loss : 30.55358123779297, \t Total Dis Loss : 6.345302244881168e-05\n",
      "Steps : 53700, \t Total Gen Loss : 31.272470474243164, \t Total Dis Loss : 0.0008950144983828068\n",
      "Steps : 53800, \t Total Gen Loss : 30.469921112060547, \t Total Dis Loss : 9.183066867990419e-05\n",
      "Steps : 53900, \t Total Gen Loss : 31.07933807373047, \t Total Dis Loss : 0.00012268536374904215\n",
      "Steps : 54000, \t Total Gen Loss : 30.226665496826172, \t Total Dis Loss : 0.019435454159975052\n",
      "Time for epoch 8 is 307.5778121948242 sec\n",
      "Steps : 54100, \t Total Gen Loss : 30.489768981933594, \t Total Dis Loss : 0.0003327007871121168\n",
      "Steps : 54200, \t Total Gen Loss : 29.438169479370117, \t Total Dis Loss : 0.0002190571540268138\n",
      "Steps : 54300, \t Total Gen Loss : 29.00328826904297, \t Total Dis Loss : 0.00014206534251570702\n",
      "Steps : 54400, \t Total Gen Loss : 26.80048179626465, \t Total Dis Loss : 0.0030099311843514442\n",
      "Steps : 54500, \t Total Gen Loss : 26.70811653137207, \t Total Dis Loss : 0.004717781208455563\n",
      "Steps : 54600, \t Total Gen Loss : 25.949792861938477, \t Total Dis Loss : 0.000943316554185003\n",
      "Steps : 54700, \t Total Gen Loss : 27.1126651763916, \t Total Dis Loss : 0.0004981881938874722\n",
      "Steps : 54800, \t Total Gen Loss : 30.509220123291016, \t Total Dis Loss : 7.881867350079119e-05\n",
      "Steps : 54900, \t Total Gen Loss : 30.667430877685547, \t Total Dis Loss : 0.00021433028450701386\n",
      "Steps : 55000, \t Total Gen Loss : 29.8677978515625, \t Total Dis Loss : 0.0002157710841856897\n",
      "Steps : 55100, \t Total Gen Loss : 28.692424774169922, \t Total Dis Loss : 0.00043661415111273527\n",
      "Steps : 55200, \t Total Gen Loss : 27.4383544921875, \t Total Dis Loss : 0.0015298265498131514\n",
      "Steps : 55300, \t Total Gen Loss : 28.643287658691406, \t Total Dis Loss : 0.00017461154493503273\n",
      "Steps : 55400, \t Total Gen Loss : 29.28205108642578, \t Total Dis Loss : 0.00017490774916950613\n",
      "Steps : 55500, \t Total Gen Loss : 28.27301025390625, \t Total Dis Loss : 0.007791226729750633\n",
      "Steps : 55600, \t Total Gen Loss : 25.48120880126953, \t Total Dis Loss : 0.009046702645719051\n",
      "Steps : 55700, \t Total Gen Loss : 28.573163986206055, \t Total Dis Loss : 0.0005380737711675465\n",
      "Steps : 55800, \t Total Gen Loss : 28.04993438720703, \t Total Dis Loss : 0.0002245748182758689\n",
      "Steps : 55900, \t Total Gen Loss : 28.847347259521484, \t Total Dis Loss : 0.00034297266392968595\n",
      "Steps : 56000, \t Total Gen Loss : 28.515222549438477, \t Total Dis Loss : 0.0002763423544820398\n",
      "Steps : 56100, \t Total Gen Loss : 28.52397918701172, \t Total Dis Loss : 8.736187010072172e-05\n",
      "Steps : 56200, \t Total Gen Loss : 29.79865074157715, \t Total Dis Loss : 8.194652764359489e-05\n",
      "Steps : 56300, \t Total Gen Loss : 29.381208419799805, \t Total Dis Loss : 7.951455336296931e-05\n",
      "Steps : 56400, \t Total Gen Loss : 27.19293212890625, \t Total Dis Loss : 0.0003542863414622843\n",
      "Steps : 56500, \t Total Gen Loss : 31.60437774658203, \t Total Dis Loss : 0.00010747014312073588\n",
      "Steps : 56600, \t Total Gen Loss : 30.288488388061523, \t Total Dis Loss : 0.00017562121502123773\n",
      "Steps : 56700, \t Total Gen Loss : 31.091428756713867, \t Total Dis Loss : 5.689180034096353e-05\n",
      "Steps : 56800, \t Total Gen Loss : 28.622278213500977, \t Total Dis Loss : 0.00015255807375069708\n",
      "Steps : 56900, \t Total Gen Loss : 29.59937286376953, \t Total Dis Loss : 0.00011431379243731499\n",
      "Steps : 57000, \t Total Gen Loss : 28.111764907836914, \t Total Dis Loss : 0.00017648669017944485\n",
      "Steps : 57100, \t Total Gen Loss : 29.42911148071289, \t Total Dis Loss : 0.00012885582691524178\n",
      "Steps : 57200, \t Total Gen Loss : 31.030824661254883, \t Total Dis Loss : 0.00015909397916402668\n",
      "Steps : 57300, \t Total Gen Loss : 29.42264175415039, \t Total Dis Loss : 5.495778168551624e-05\n",
      "Steps : 57400, \t Total Gen Loss : 30.02606964111328, \t Total Dis Loss : 0.0002717460156418383\n",
      "Steps : 57500, \t Total Gen Loss : 30.60455894470215, \t Total Dis Loss : 0.00024962564930319786\n",
      "Steps : 57600, \t Total Gen Loss : 29.317922592163086, \t Total Dis Loss : 0.00015546416398137808\n",
      "Steps : 57700, \t Total Gen Loss : 29.290441513061523, \t Total Dis Loss : 0.000552102574147284\n",
      "Steps : 57800, \t Total Gen Loss : 31.355043411254883, \t Total Dis Loss : 7.899462798377499e-05\n",
      "Steps : 57900, \t Total Gen Loss : 27.269054412841797, \t Total Dis Loss : 0.5794844031333923\n",
      "Steps : 58000, \t Total Gen Loss : 28.286775588989258, \t Total Dis Loss : 0.0002904243301600218\n",
      "Steps : 58100, \t Total Gen Loss : 29.776052474975586, \t Total Dis Loss : 0.0018560043536126614\n",
      "Steps : 58200, \t Total Gen Loss : 29.662044525146484, \t Total Dis Loss : 0.007400164380669594\n",
      "Steps : 58300, \t Total Gen Loss : 28.258363723754883, \t Total Dis Loss : 0.0008898797677829862\n",
      "Steps : 58400, \t Total Gen Loss : 30.177074432373047, \t Total Dis Loss : 0.0003365314332768321\n",
      "Steps : 58500, \t Total Gen Loss : 30.381969451904297, \t Total Dis Loss : 5.873968984815292e-05\n",
      "Steps : 58600, \t Total Gen Loss : 30.523849487304688, \t Total Dis Loss : 0.00020715098071377724\n",
      "Steps : 58700, \t Total Gen Loss : 26.7342472076416, \t Total Dis Loss : 1.2379143238067627\n",
      "Steps : 58800, \t Total Gen Loss : 34.26897048950195, \t Total Dis Loss : 0.0016882064519450068\n",
      "Steps : 58900, \t Total Gen Loss : 31.037731170654297, \t Total Dis Loss : 0.0016451453557237983\n",
      "Steps : 59000, \t Total Gen Loss : 30.99018096923828, \t Total Dis Loss : 0.0018356137443333864\n",
      "Steps : 59100, \t Total Gen Loss : 30.686079025268555, \t Total Dis Loss : 0.0004802861949428916\n",
      "Steps : 59200, \t Total Gen Loss : 32.681182861328125, \t Total Dis Loss : 0.00020495049830060452\n",
      "Steps : 59300, \t Total Gen Loss : 31.18791389465332, \t Total Dis Loss : 0.0031818863935768604\n",
      "Steps : 59400, \t Total Gen Loss : 33.40095901489258, \t Total Dis Loss : 0.0003721165412571281\n",
      "Steps : 59500, \t Total Gen Loss : 32.07955551147461, \t Total Dis Loss : 0.00011025569983758032\n",
      "Steps : 59600, \t Total Gen Loss : 32.768287658691406, \t Total Dis Loss : 0.0001664508890826255\n",
      "Steps : 59700, \t Total Gen Loss : 30.000986099243164, \t Total Dis Loss : 0.0002298270264873281\n",
      "Steps : 59800, \t Total Gen Loss : 30.790699005126953, \t Total Dis Loss : 0.00018008906044997275\n",
      "Steps : 59900, \t Total Gen Loss : 32.345550537109375, \t Total Dis Loss : 0.00036566422204487026\n",
      "Steps : 60000, \t Total Gen Loss : 29.757490158081055, \t Total Dis Loss : 0.0011480002431198955\n",
      "Steps : 60100, \t Total Gen Loss : 31.772653579711914, \t Total Dis Loss : 0.0011343794176355004\n",
      "Steps : 60200, \t Total Gen Loss : 31.736703872680664, \t Total Dis Loss : 0.00019441013864707202\n",
      "Steps : 60300, \t Total Gen Loss : 32.99279022216797, \t Total Dis Loss : 0.0005288358079269528\n",
      "Steps : 60400, \t Total Gen Loss : 31.669055938720703, \t Total Dis Loss : 0.002546847565099597\n",
      "Steps : 60500, \t Total Gen Loss : 30.982025146484375, \t Total Dis Loss : 0.0001904785167425871\n",
      "Steps : 60600, \t Total Gen Loss : 31.800617218017578, \t Total Dis Loss : 0.00020732080156449229\n",
      "Steps : 60700, \t Total Gen Loss : 32.66114044189453, \t Total Dis Loss : 6.438422860810533e-05\n",
      "Time for epoch 9 is 307.1445233821869 sec\n",
      "Steps : 60800, \t Total Gen Loss : 31.767982482910156, \t Total Dis Loss : 0.0001457738399039954\n",
      "Steps : 60900, \t Total Gen Loss : 32.673126220703125, \t Total Dis Loss : 0.00010072250006487593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 61000, \t Total Gen Loss : 29.157180786132812, \t Total Dis Loss : 0.0006358400569297373\n",
      "Steps : 61100, \t Total Gen Loss : 30.902145385742188, \t Total Dis Loss : 0.0008774759480729699\n",
      "Steps : 61200, \t Total Gen Loss : 30.460012435913086, \t Total Dis Loss : 0.0005020175594836473\n",
      "Steps : 61300, \t Total Gen Loss : 32.10365676879883, \t Total Dis Loss : 0.000186754681635648\n",
      "Steps : 61400, \t Total Gen Loss : 31.186077117919922, \t Total Dis Loss : 0.00039447465678676963\n",
      "Steps : 61500, \t Total Gen Loss : 29.478015899658203, \t Total Dis Loss : 0.00036215470754541457\n",
      "Steps : 61600, \t Total Gen Loss : 32.96250534057617, \t Total Dis Loss : 6.439910794142634e-05\n",
      "Steps : 61700, \t Total Gen Loss : 30.750110626220703, \t Total Dis Loss : 9.939721348928288e-05\n",
      "Steps : 61800, \t Total Gen Loss : 30.609638214111328, \t Total Dis Loss : 0.0001139320302172564\n",
      "Steps : 61900, \t Total Gen Loss : 35.15682601928711, \t Total Dis Loss : 0.2986530363559723\n",
      "Steps : 62000, \t Total Gen Loss : 28.82422637939453, \t Total Dis Loss : 0.008754327893257141\n",
      "Steps : 62100, \t Total Gen Loss : 32.11774826049805, \t Total Dis Loss : 0.00047303628525696695\n",
      "Steps : 62200, \t Total Gen Loss : 31.31172752380371, \t Total Dis Loss : 0.000791228492744267\n",
      "Steps : 62300, \t Total Gen Loss : 31.933826446533203, \t Total Dis Loss : 4.8051988414954394e-05\n",
      "Steps : 62400, \t Total Gen Loss : 32.18948745727539, \t Total Dis Loss : 0.0003528520173858851\n",
      "Steps : 62500, \t Total Gen Loss : 31.159618377685547, \t Total Dis Loss : 0.0001498788478784263\n",
      "Steps : 62600, \t Total Gen Loss : 32.33517074584961, \t Total Dis Loss : 7.009123510215431e-05\n",
      "Steps : 62700, \t Total Gen Loss : 29.62673568725586, \t Total Dis Loss : 8.186847844626755e-05\n",
      "Steps : 62800, \t Total Gen Loss : 30.110815048217773, \t Total Dis Loss : 0.0004610128526110202\n",
      "Steps : 62900, \t Total Gen Loss : 29.35572052001953, \t Total Dis Loss : 0.00017808742995839566\n",
      "Steps : 63000, \t Total Gen Loss : 28.88861656188965, \t Total Dis Loss : 0.00011309431283734739\n",
      "Steps : 63100, \t Total Gen Loss : 29.783037185668945, \t Total Dis Loss : 7.385872595477849e-05\n",
      "Steps : 63200, \t Total Gen Loss : 29.777729034423828, \t Total Dis Loss : 0.00012566112854983658\n",
      "Steps : 63300, \t Total Gen Loss : 30.465885162353516, \t Total Dis Loss : 0.0003672981110867113\n",
      "Steps : 63400, \t Total Gen Loss : 26.607437133789062, \t Total Dis Loss : 0.0012369544710963964\n",
      "Steps : 63500, \t Total Gen Loss : 30.97524642944336, \t Total Dis Loss : 0.0005220593884587288\n",
      "Steps : 63600, \t Total Gen Loss : 27.59294891357422, \t Total Dis Loss : 0.0005438370862975717\n",
      "Steps : 63700, \t Total Gen Loss : 26.439292907714844, \t Total Dis Loss : 0.0016815293347463012\n",
      "Steps : 63800, \t Total Gen Loss : 28.029590606689453, \t Total Dis Loss : 0.00036613273550756276\n",
      "Steps : 63900, \t Total Gen Loss : 29.767763137817383, \t Total Dis Loss : 0.00017222225142177194\n",
      "Steps : 64000, \t Total Gen Loss : 29.505830764770508, \t Total Dis Loss : 0.00013058021431788802\n",
      "Steps : 64100, \t Total Gen Loss : 29.136699676513672, \t Total Dis Loss : 0.0001854065922088921\n",
      "Steps : 64200, \t Total Gen Loss : 31.719942092895508, \t Total Dis Loss : 0.00013920594938099384\n",
      "Steps : 64300, \t Total Gen Loss : 32.16797637939453, \t Total Dis Loss : 0.00011380671639926732\n",
      "Steps : 64400, \t Total Gen Loss : 29.17943000793457, \t Total Dis Loss : 0.00014763801300432533\n",
      "Steps : 64500, \t Total Gen Loss : 31.21542739868164, \t Total Dis Loss : 6.165934610180557e-05\n",
      "Steps : 64600, \t Total Gen Loss : 31.74153709411621, \t Total Dis Loss : 6.892683450132608e-05\n",
      "Steps : 64700, \t Total Gen Loss : 30.001094818115234, \t Total Dis Loss : 0.00022771157091483474\n",
      "Steps : 64800, \t Total Gen Loss : 29.834327697753906, \t Total Dis Loss : 0.00012601949856616557\n",
      "Steps : 64900, \t Total Gen Loss : 30.610116958618164, \t Total Dis Loss : 0.0001260192657355219\n",
      "Steps : 65000, \t Total Gen Loss : 29.318403244018555, \t Total Dis Loss : 0.0003182498912792653\n",
      "Steps : 65100, \t Total Gen Loss : 31.531160354614258, \t Total Dis Loss : 0.00018183261272497475\n",
      "Steps : 65200, \t Total Gen Loss : 30.587909698486328, \t Total Dis Loss : 0.0005978049011901021\n",
      "Steps : 65300, \t Total Gen Loss : 28.84902000427246, \t Total Dis Loss : 0.0011689270613715053\n",
      "Steps : 65400, \t Total Gen Loss : 29.016984939575195, \t Total Dis Loss : 0.0003320524701848626\n",
      "Steps : 65500, \t Total Gen Loss : 30.050771713256836, \t Total Dis Loss : 0.00059289182536304\n",
      "Steps : 65600, \t Total Gen Loss : 26.395219802856445, \t Total Dis Loss : 0.0002668876841198653\n",
      "Steps : 65700, \t Total Gen Loss : 26.741191864013672, \t Total Dis Loss : 0.0004257171240169555\n",
      "Steps : 65800, \t Total Gen Loss : 27.651939392089844, \t Total Dis Loss : 0.00038910566945560277\n",
      "Steps : 65900, \t Total Gen Loss : 28.92549705505371, \t Total Dis Loss : 0.0003098187444265932\n",
      "Steps : 66000, \t Total Gen Loss : 30.608203887939453, \t Total Dis Loss : 0.00011675530549837276\n",
      "Steps : 66100, \t Total Gen Loss : 27.666595458984375, \t Total Dis Loss : 0.0007300326833501458\n",
      "Steps : 66200, \t Total Gen Loss : 27.183805465698242, \t Total Dis Loss : 0.00034286233130842447\n",
      "Steps : 66300, \t Total Gen Loss : 30.308012008666992, \t Total Dis Loss : 0.0003540842735674232\n",
      "Steps : 66400, \t Total Gen Loss : 27.77798080444336, \t Total Dis Loss : 0.00034952961141243577\n",
      "Steps : 66500, \t Total Gen Loss : 26.820451736450195, \t Total Dis Loss : 0.00036934492527507246\n",
      "Steps : 66600, \t Total Gen Loss : 29.95227813720703, \t Total Dis Loss : 0.0002131128712790087\n",
      "Steps : 66700, \t Total Gen Loss : 29.87137794494629, \t Total Dis Loss : 0.017693955451250076\n",
      "Steps : 66800, \t Total Gen Loss : 33.80567169189453, \t Total Dis Loss : 7.786450441926718e-05\n",
      "Steps : 66900, \t Total Gen Loss : 31.57649803161621, \t Total Dis Loss : 0.001134051475673914\n",
      "Steps : 67000, \t Total Gen Loss : 31.754934310913086, \t Total Dis Loss : 0.0001245497987838462\n",
      "Steps : 67100, \t Total Gen Loss : 28.794212341308594, \t Total Dis Loss : 0.0002631457755342126\n",
      "Steps : 67200, \t Total Gen Loss : 29.973876953125, \t Total Dis Loss : 0.0001872905995696783\n",
      "Steps : 67300, \t Total Gen Loss : 28.149961471557617, \t Total Dis Loss : 0.0003302866534795612\n",
      "Steps : 67400, \t Total Gen Loss : 29.277755737304688, \t Total Dis Loss : 0.0003775391378439963\n",
      "Steps : 67500, \t Total Gen Loss : 31.044933319091797, \t Total Dis Loss : 0.0001324670884059742\n",
      "Time for epoch 10 is 299.65389013290405 sec\n",
      "Steps : 67600, \t Total Gen Loss : 32.622703552246094, \t Total Dis Loss : 0.00022834190167486668\n",
      "Steps : 67700, \t Total Gen Loss : 28.1473331451416, \t Total Dis Loss : 0.00013781309826299548\n",
      "Steps : 67800, \t Total Gen Loss : 29.300838470458984, \t Total Dis Loss : 0.001563099678605795\n",
      "Steps : 67900, \t Total Gen Loss : 26.361963272094727, \t Total Dis Loss : 0.001082831178791821\n",
      "Steps : 68000, \t Total Gen Loss : 28.002180099487305, \t Total Dis Loss : 0.00017468071018811315\n",
      "Steps : 68100, \t Total Gen Loss : 29.063329696655273, \t Total Dis Loss : 0.00021461672440636903\n",
      "Steps : 68200, \t Total Gen Loss : 29.862201690673828, \t Total Dis Loss : 0.0004017625469714403\n",
      "Steps : 68300, \t Total Gen Loss : 30.429691314697266, \t Total Dis Loss : 0.0004290569049771875\n",
      "Steps : 68400, \t Total Gen Loss : 29.90926742553711, \t Total Dis Loss : 0.00015829529729671776\n",
      "Steps : 68500, \t Total Gen Loss : 29.86195945739746, \t Total Dis Loss : 0.0012199595803394914\n",
      "Steps : 68600, \t Total Gen Loss : 28.926998138427734, \t Total Dis Loss : 0.00037467264337465167\n",
      "Steps : 68700, \t Total Gen Loss : 30.457077026367188, \t Total Dis Loss : 0.0002347836270928383\n",
      "Steps : 68800, \t Total Gen Loss : 29.23443031311035, \t Total Dis Loss : 0.00010738790297182277\n",
      "Steps : 68900, \t Total Gen Loss : 27.449417114257812, \t Total Dis Loss : 0.000982870114967227\n",
      "Steps : 69000, \t Total Gen Loss : 31.107736587524414, \t Total Dis Loss : 0.00017170880164485425\n",
      "Steps : 69100, \t Total Gen Loss : 28.689502716064453, \t Total Dis Loss : 0.00014361807552631944\n",
      "Steps : 69200, \t Total Gen Loss : 29.12322235107422, \t Total Dis Loss : 0.00015308926231227815\n",
      "Steps : 69300, \t Total Gen Loss : 30.947887420654297, \t Total Dis Loss : 0.00010560612281551585\n",
      "Steps : 69400, \t Total Gen Loss : 31.445608139038086, \t Total Dis Loss : 5.7007498980965465e-05\n",
      "Steps : 69500, \t Total Gen Loss : 29.921932220458984, \t Total Dis Loss : 6.186119571793824e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 69600, \t Total Gen Loss : 31.959444046020508, \t Total Dis Loss : 8.55909165693447e-05\n",
      "Steps : 69700, \t Total Gen Loss : 30.781848907470703, \t Total Dis Loss : 5.3931609727442265e-05\n",
      "Steps : 69800, \t Total Gen Loss : 28.916593551635742, \t Total Dis Loss : 9.092713298741728e-05\n",
      "Steps : 69900, \t Total Gen Loss : 28.960783004760742, \t Total Dis Loss : 0.00042580184526741505\n",
      "Steps : 70000, \t Total Gen Loss : 30.0216121673584, \t Total Dis Loss : 0.0002596010162960738\n",
      "Steps : 70100, \t Total Gen Loss : 30.081762313842773, \t Total Dis Loss : 0.00011777861800510436\n",
      "Steps : 70200, \t Total Gen Loss : 31.99252700805664, \t Total Dis Loss : 8.155462273862213e-05\n",
      "Steps : 70300, \t Total Gen Loss : 30.020492553710938, \t Total Dis Loss : 7.658856338821352e-05\n",
      "Steps : 70400, \t Total Gen Loss : 30.18057632446289, \t Total Dis Loss : 6.878787826281041e-05\n",
      "Steps : 70500, \t Total Gen Loss : 29.550765991210938, \t Total Dis Loss : 0.00010292678780388087\n",
      "Steps : 70600, \t Total Gen Loss : 30.78022575378418, \t Total Dis Loss : 0.00012336191139183939\n",
      "Steps : 70700, \t Total Gen Loss : 30.439149856567383, \t Total Dis Loss : 9.101118484977633e-05\n",
      "Steps : 70800, \t Total Gen Loss : 29.557754516601562, \t Total Dis Loss : 0.00022231716138776392\n",
      "Steps : 70900, \t Total Gen Loss : 29.2802734375, \t Total Dis Loss : 0.00010658015526132658\n",
      "Steps : 71000, \t Total Gen Loss : 28.70264434814453, \t Total Dis Loss : 7.558910147054121e-05\n",
      "Steps : 71100, \t Total Gen Loss : 30.918312072753906, \t Total Dis Loss : 2.993389352923259e-05\n",
      "Steps : 71200, \t Total Gen Loss : 31.81072998046875, \t Total Dis Loss : 0.0010099266655743122\n",
      "Steps : 71300, \t Total Gen Loss : 31.888269424438477, \t Total Dis Loss : 5.553982191486284e-05\n",
      "Steps : 71400, \t Total Gen Loss : 30.727628707885742, \t Total Dis Loss : 5.592727393377572e-05\n",
      "Steps : 71500, \t Total Gen Loss : 30.653425216674805, \t Total Dis Loss : 0.00014849936997052282\n",
      "Steps : 71600, \t Total Gen Loss : 29.929807662963867, \t Total Dis Loss : 9.545377542963251e-05\n",
      "Steps : 71700, \t Total Gen Loss : 31.55328941345215, \t Total Dis Loss : 7.164548151195049e-05\n",
      "Steps : 71800, \t Total Gen Loss : 31.367712020874023, \t Total Dis Loss : 0.00014294253196567297\n",
      "Steps : 71900, \t Total Gen Loss : 30.86338996887207, \t Total Dis Loss : 0.0003404528251849115\n",
      "Steps : 72000, \t Total Gen Loss : 27.88822364807129, \t Total Dis Loss : 0.16272224485874176\n",
      "Steps : 72100, \t Total Gen Loss : 33.96108627319336, \t Total Dis Loss : 0.1603836864233017\n",
      "Steps : 72200, \t Total Gen Loss : 29.729736328125, \t Total Dis Loss : 0.0007957019843161106\n",
      "Steps : 72300, \t Total Gen Loss : 30.694059371948242, \t Total Dis Loss : 0.0002536984975449741\n",
      "Steps : 72400, \t Total Gen Loss : 30.756412506103516, \t Total Dis Loss : 0.00039803789695724845\n",
      "Steps : 72500, \t Total Gen Loss : 33.84451675415039, \t Total Dis Loss : 0.0010852871928364038\n",
      "Steps : 72600, \t Total Gen Loss : 31.264375686645508, \t Total Dis Loss : 0.000625311047770083\n",
      "Steps : 72700, \t Total Gen Loss : 30.52192497253418, \t Total Dis Loss : 0.00043976010056212544\n",
      "Steps : 72800, \t Total Gen Loss : 31.453357696533203, \t Total Dis Loss : 0.0004688891349360347\n",
      "Steps : 72900, \t Total Gen Loss : 31.391998291015625, \t Total Dis Loss : 0.0002566548064351082\n",
      "Steps : 73000, \t Total Gen Loss : 32.3656120300293, \t Total Dis Loss : 0.00027953172684647143\n",
      "Steps : 73100, \t Total Gen Loss : 33.54144287109375, \t Total Dis Loss : 0.0001679727720329538\n",
      "Steps : 73200, \t Total Gen Loss : 33.81019592285156, \t Total Dis Loss : 0.00014012122119311243\n",
      "Steps : 73300, \t Total Gen Loss : 33.09053039550781, \t Total Dis Loss : 0.00017932552145794034\n",
      "Steps : 73400, \t Total Gen Loss : 33.03447341918945, \t Total Dis Loss : 0.0001889247796498239\n",
      "Steps : 73500, \t Total Gen Loss : 31.62215232849121, \t Total Dis Loss : 6.560766632901505e-05\n",
      "Steps : 73600, \t Total Gen Loss : 30.43950080871582, \t Total Dis Loss : 0.00018989230738952756\n",
      "Steps : 73700, \t Total Gen Loss : 33.27479934692383, \t Total Dis Loss : 7.382322655757889e-05\n",
      "Steps : 73800, \t Total Gen Loss : 31.363271713256836, \t Total Dis Loss : 9.465579933021218e-05\n",
      "Steps : 73900, \t Total Gen Loss : 32.53284454345703, \t Total Dis Loss : 0.00017443338583689183\n",
      "Steps : 74000, \t Total Gen Loss : 33.04753112792969, \t Total Dis Loss : 9.826535097090527e-05\n",
      "Steps : 74100, \t Total Gen Loss : 34.317657470703125, \t Total Dis Loss : 0.00038519574445672333\n",
      "Steps : 74200, \t Total Gen Loss : 30.53945541381836, \t Total Dis Loss : 0.00035317667061463\n",
      "Time for epoch 11 is 305.5190284252167 sec\n",
      "Steps : 74300, \t Total Gen Loss : 29.3375186920166, \t Total Dis Loss : 0.0009581319754943252\n",
      "Steps : 74400, \t Total Gen Loss : 30.654516220092773, \t Total Dis Loss : 0.0005477113300003111\n",
      "Steps : 74500, \t Total Gen Loss : 30.232736587524414, \t Total Dis Loss : 0.004604478366672993\n",
      "Steps : 74600, \t Total Gen Loss : 30.331329345703125, \t Total Dis Loss : 0.00027044687885791063\n",
      "Steps : 74700, \t Total Gen Loss : 30.53322982788086, \t Total Dis Loss : 0.0004509699356276542\n",
      "Steps : 74800, \t Total Gen Loss : 30.07120132446289, \t Total Dis Loss : 0.0004962029051966965\n",
      "Steps : 74900, \t Total Gen Loss : 29.71739959716797, \t Total Dis Loss : 0.00024303741520270705\n",
      "Steps : 75000, \t Total Gen Loss : 33.1093635559082, \t Total Dis Loss : 0.00011074406211264431\n",
      "Steps : 75100, \t Total Gen Loss : 29.456130981445312, \t Total Dis Loss : 0.00012625742238014936\n",
      "Steps : 75200, \t Total Gen Loss : 33.831146240234375, \t Total Dis Loss : 9.295316704083234e-05\n",
      "Steps : 75300, \t Total Gen Loss : 31.554155349731445, \t Total Dis Loss : 0.0007161940447986126\n",
      "Steps : 75400, \t Total Gen Loss : 31.124021530151367, \t Total Dis Loss : 0.001698760432191193\n",
      "Steps : 75500, \t Total Gen Loss : 31.961196899414062, \t Total Dis Loss : 0.003266906598582864\n",
      "Steps : 75600, \t Total Gen Loss : 29.856279373168945, \t Total Dis Loss : 0.0006749366875737906\n",
      "Steps : 75700, \t Total Gen Loss : 34.08308792114258, \t Total Dis Loss : 0.0001551342138554901\n",
      "Steps : 75800, \t Total Gen Loss : 30.65456771850586, \t Total Dis Loss : 0.0007337980205193162\n",
      "Steps : 75900, \t Total Gen Loss : 30.31765365600586, \t Total Dis Loss : 0.0008604571921750903\n",
      "Steps : 76000, \t Total Gen Loss : 29.51227569580078, \t Total Dis Loss : 0.00014857127098366618\n",
      "Steps : 76100, \t Total Gen Loss : 30.839435577392578, \t Total Dis Loss : 0.00040297998930327594\n",
      "Steps : 76200, \t Total Gen Loss : 32.321022033691406, \t Total Dis Loss : 0.0003590760170482099\n",
      "Steps : 76300, \t Total Gen Loss : 29.783309936523438, \t Total Dis Loss : 0.00029751835973002017\n",
      "Steps : 76400, \t Total Gen Loss : 31.074752807617188, \t Total Dis Loss : 0.0012275781482458115\n",
      "Steps : 76500, \t Total Gen Loss : 33.559852600097656, \t Total Dis Loss : 0.0003663057868834585\n",
      "Steps : 76600, \t Total Gen Loss : 29.735910415649414, \t Total Dis Loss : 0.00018958210421260446\n",
      "Steps : 76700, \t Total Gen Loss : 29.864229202270508, \t Total Dis Loss : 0.00012691225856542587\n",
      "Steps : 76800, \t Total Gen Loss : 32.50675964355469, \t Total Dis Loss : 0.0001649337209528312\n",
      "Steps : 76900, \t Total Gen Loss : 29.98848533630371, \t Total Dis Loss : 0.0002986372564919293\n",
      "Steps : 77000, \t Total Gen Loss : 30.295263290405273, \t Total Dis Loss : 0.0002909835893660784\n",
      "Steps : 77100, \t Total Gen Loss : 31.630882263183594, \t Total Dis Loss : 0.00023834750754758716\n",
      "Steps : 77200, \t Total Gen Loss : 29.368179321289062, \t Total Dis Loss : 0.0006187612889334559\n",
      "Steps : 77300, \t Total Gen Loss : 29.305347442626953, \t Total Dis Loss : 0.0012785306898877025\n",
      "Steps : 77400, \t Total Gen Loss : 31.418251037597656, \t Total Dis Loss : 0.00025078305043280125\n",
      "Steps : 77500, \t Total Gen Loss : 26.787267684936523, \t Total Dis Loss : 0.005537022370845079\n",
      "Steps : 77600, \t Total Gen Loss : 32.53693389892578, \t Total Dis Loss : 0.00043860942241735756\n",
      "Steps : 77700, \t Total Gen Loss : 36.751522064208984, \t Total Dis Loss : 0.000907178211491555\n",
      "Steps : 77800, \t Total Gen Loss : 35.883731842041016, \t Total Dis Loss : 8.121364953694865e-05\n",
      "Steps : 77900, \t Total Gen Loss : 31.80004119873047, \t Total Dis Loss : 0.0035584927536547184\n",
      "Steps : 78000, \t Total Gen Loss : 33.2345085144043, \t Total Dis Loss : 0.0019439696334302425\n",
      "Steps : 78100, \t Total Gen Loss : 37.49538803100586, \t Total Dis Loss : 0.0006188156548887491\n",
      "Steps : 78200, \t Total Gen Loss : 36.057735443115234, \t Total Dis Loss : 0.001556427450850606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 78300, \t Total Gen Loss : 33.343441009521484, \t Total Dis Loss : 0.0030959895811975002\n",
      "Steps : 78400, \t Total Gen Loss : 33.91400909423828, \t Total Dis Loss : 0.0009246058762073517\n",
      "Steps : 78500, \t Total Gen Loss : 30.805356979370117, \t Total Dis Loss : 0.0005777563783340156\n",
      "Steps : 78600, \t Total Gen Loss : 33.26847839355469, \t Total Dis Loss : 0.00041751735261641443\n",
      "Steps : 78700, \t Total Gen Loss : 32.224403381347656, \t Total Dis Loss : 0.10432939231395721\n",
      "Steps : 78800, \t Total Gen Loss : 31.04926300048828, \t Total Dis Loss : 0.010193626396358013\n",
      "Steps : 78900, \t Total Gen Loss : 32.41581726074219, \t Total Dis Loss : 0.0017621032893657684\n",
      "Steps : 79000, \t Total Gen Loss : 34.6241569519043, \t Total Dis Loss : 0.0033686941023916006\n",
      "Steps : 79100, \t Total Gen Loss : 30.59458351135254, \t Total Dis Loss : 0.006857736501842737\n",
      "Steps : 79200, \t Total Gen Loss : 30.27542495727539, \t Total Dis Loss : 0.003913213033229113\n",
      "Steps : 79300, \t Total Gen Loss : 33.733463287353516, \t Total Dis Loss : 0.0014926260337233543\n",
      "Steps : 79400, \t Total Gen Loss : 30.1901798248291, \t Total Dis Loss : 0.001169667113572359\n",
      "Steps : 79500, \t Total Gen Loss : 32.750152587890625, \t Total Dis Loss : 0.0007310091750696301\n",
      "Steps : 79600, \t Total Gen Loss : 33.159881591796875, \t Total Dis Loss : 0.0015855844831094146\n",
      "Steps : 79700, \t Total Gen Loss : 32.89679718017578, \t Total Dis Loss : 0.002770045306533575\n",
      "Steps : 79800, \t Total Gen Loss : 34.23653030395508, \t Total Dis Loss : 0.0005452587502077222\n",
      "Steps : 79900, \t Total Gen Loss : 30.9617862701416, \t Total Dis Loss : 0.0025909384712576866\n",
      "Steps : 80000, \t Total Gen Loss : 31.115205764770508, \t Total Dis Loss : 0.0012880134163424373\n",
      "Steps : 80100, \t Total Gen Loss : 34.279415130615234, \t Total Dis Loss : 0.010888999328017235\n",
      "Steps : 80200, \t Total Gen Loss : 31.811939239501953, \t Total Dis Loss : 0.01160089485347271\n",
      "Steps : 80300, \t Total Gen Loss : 32.45940017700195, \t Total Dis Loss : 0.0023057637736201286\n",
      "Steps : 80400, \t Total Gen Loss : 30.340801239013672, \t Total Dis Loss : 0.0040888614021241665\n",
      "Steps : 80500, \t Total Gen Loss : 32.909759521484375, \t Total Dis Loss : 0.0009614733862690628\n",
      "Steps : 80600, \t Total Gen Loss : 30.490846633911133, \t Total Dis Loss : 0.0013066718820482492\n",
      "Steps : 80700, \t Total Gen Loss : 31.38654899597168, \t Total Dis Loss : 0.0008350329007953405\n",
      "Steps : 80800, \t Total Gen Loss : 31.413963317871094, \t Total Dis Loss : 0.003741299733519554\n",
      "Steps : 80900, \t Total Gen Loss : 34.31211853027344, \t Total Dis Loss : 0.0016661010449752212\n",
      "Steps : 81000, \t Total Gen Loss : 31.819488525390625, \t Total Dis Loss : 0.005372464656829834\n",
      "Time for epoch 12 is 307.8397316932678 sec\n",
      "Steps : 81100, \t Total Gen Loss : 33.19651794433594, \t Total Dis Loss : 0.0011672067921608686\n",
      "Steps : 81200, \t Total Gen Loss : 32.841583251953125, \t Total Dis Loss : 0.0015257420018315315\n",
      "Steps : 81300, \t Total Gen Loss : 31.832231521606445, \t Total Dis Loss : 0.0010970786679536104\n",
      "Steps : 81400, \t Total Gen Loss : 33.01670837402344, \t Total Dis Loss : 0.0005270481342449784\n",
      "Steps : 81500, \t Total Gen Loss : 30.424118041992188, \t Total Dis Loss : 0.0007032093126326799\n",
      "Steps : 81600, \t Total Gen Loss : 33.51126480102539, \t Total Dis Loss : 0.003043112577870488\n",
      "Steps : 81700, \t Total Gen Loss : 30.797338485717773, \t Total Dis Loss : 0.013785794377326965\n",
      "Steps : 81800, \t Total Gen Loss : 33.260066986083984, \t Total Dis Loss : 0.0012250951258465648\n",
      "Steps : 81900, \t Total Gen Loss : 31.79213523864746, \t Total Dis Loss : 0.001456060679629445\n",
      "Steps : 82000, \t Total Gen Loss : 32.96382522583008, \t Total Dis Loss : 0.0005045182770118117\n",
      "Steps : 82100, \t Total Gen Loss : 33.729122161865234, \t Total Dis Loss : 0.0010988159338012338\n",
      "Steps : 82200, \t Total Gen Loss : 33.23706817626953, \t Total Dis Loss : 0.0006249534199014306\n",
      "Steps : 82300, \t Total Gen Loss : 36.41481399536133, \t Total Dis Loss : 0.00011953691864619032\n",
      "Steps : 82400, \t Total Gen Loss : 33.0259895324707, \t Total Dis Loss : 0.0007371988031081855\n",
      "Steps : 82500, \t Total Gen Loss : 30.291122436523438, \t Total Dis Loss : 0.0006168946856632829\n",
      "Steps : 82600, \t Total Gen Loss : 32.70405578613281, \t Total Dis Loss : 0.0008747442625463009\n",
      "Steps : 82700, \t Total Gen Loss : 32.392608642578125, \t Total Dis Loss : 0.006877915933728218\n",
      "Steps : 82800, \t Total Gen Loss : 32.927154541015625, \t Total Dis Loss : 0.000524152594152838\n",
      "Steps : 82900, \t Total Gen Loss : 31.2690372467041, \t Total Dis Loss : 0.0007361418101936579\n",
      "Steps : 83000, \t Total Gen Loss : 34.196189880371094, \t Total Dis Loss : 0.0015822246205061674\n",
      "Steps : 83100, \t Total Gen Loss : 32.71623992919922, \t Total Dis Loss : 0.0008061609114520252\n",
      "Steps : 83200, \t Total Gen Loss : 32.628047943115234, \t Total Dis Loss : 0.0003127980453427881\n",
      "Steps : 83300, \t Total Gen Loss : 31.8179988861084, \t Total Dis Loss : 0.00041813039570115507\n",
      "Steps : 83400, \t Total Gen Loss : 30.25461769104004, \t Total Dis Loss : 0.0006538901943713427\n",
      "Steps : 83500, \t Total Gen Loss : 35.2903938293457, \t Total Dis Loss : 0.0019760008435696363\n",
      "Steps : 83600, \t Total Gen Loss : 29.82001495361328, \t Total Dis Loss : 0.0010567015269771218\n",
      "Steps : 83700, \t Total Gen Loss : 30.92281723022461, \t Total Dis Loss : 0.0010906197130680084\n",
      "Steps : 83800, \t Total Gen Loss : 30.616840362548828, \t Total Dis Loss : 0.0004293730016797781\n",
      "Steps : 83900, \t Total Gen Loss : 32.48392105102539, \t Total Dis Loss : 0.0005589528009295464\n",
      "Steps : 84000, \t Total Gen Loss : 29.180540084838867, \t Total Dis Loss : 0.001521832775324583\n",
      "Steps : 84100, \t Total Gen Loss : 33.0272331237793, \t Total Dis Loss : 0.0006151580600999296\n",
      "Steps : 84200, \t Total Gen Loss : 30.522424697875977, \t Total Dis Loss : 0.0006915717385709286\n",
      "Steps : 84300, \t Total Gen Loss : 30.999649047851562, \t Total Dis Loss : 0.0028972183354198933\n",
      "Steps : 84400, \t Total Gen Loss : 31.423965454101562, \t Total Dis Loss : 0.0008611432858742774\n",
      "Steps : 84500, \t Total Gen Loss : 30.320043563842773, \t Total Dis Loss : 0.0005274147260934114\n",
      "Steps : 84600, \t Total Gen Loss : 30.638160705566406, \t Total Dis Loss : 0.03830963000655174\n",
      "Steps : 84700, \t Total Gen Loss : 31.9874210357666, \t Total Dis Loss : 0.00031918217428028584\n",
      "Steps : 84800, \t Total Gen Loss : 30.057565689086914, \t Total Dis Loss : 0.0005253861891105771\n",
      "Steps : 84900, \t Total Gen Loss : 30.66062355041504, \t Total Dis Loss : 0.0004998603835701942\n",
      "Steps : 85000, \t Total Gen Loss : 31.363601684570312, \t Total Dis Loss : 0.0002697408781386912\n",
      "Steps : 85100, \t Total Gen Loss : 32.61090850830078, \t Total Dis Loss : 0.0021900050342082977\n",
      "Steps : 85200, \t Total Gen Loss : 30.6688289642334, \t Total Dis Loss : 0.001039508730173111\n",
      "Steps : 85300, \t Total Gen Loss : 36.00210952758789, \t Total Dis Loss : 0.0008490282343700528\n",
      "Steps : 85400, \t Total Gen Loss : 31.544837951660156, \t Total Dis Loss : 0.0013091997243463993\n",
      "Steps : 85500, \t Total Gen Loss : 30.259584426879883, \t Total Dis Loss : 0.0013862894847989082\n",
      "Steps : 85600, \t Total Gen Loss : 31.958820343017578, \t Total Dis Loss : 0.0026059618685394526\n",
      "Steps : 85700, \t Total Gen Loss : 29.292695999145508, \t Total Dis Loss : 0.002233280101791024\n",
      "Steps : 85800, \t Total Gen Loss : 32.17912292480469, \t Total Dis Loss : 0.0029960002284497023\n",
      "Steps : 85900, \t Total Gen Loss : 30.599843978881836, \t Total Dis Loss : 0.0008638284634798765\n",
      "Steps : 86000, \t Total Gen Loss : 32.15907669067383, \t Total Dis Loss : 0.0005466918228194118\n",
      "Steps : 86100, \t Total Gen Loss : 31.498079299926758, \t Total Dis Loss : 0.0007298270356841385\n",
      "Steps : 86200, \t Total Gen Loss : 33.91203689575195, \t Total Dis Loss : 0.0005420242669060826\n",
      "Steps : 86300, \t Total Gen Loss : 33.302734375, \t Total Dis Loss : 0.0005048476159572601\n",
      "Steps : 86400, \t Total Gen Loss : 32.3896484375, \t Total Dis Loss : 0.0010147967841476202\n",
      "Steps : 86500, \t Total Gen Loss : 32.090545654296875, \t Total Dis Loss : 0.0002155366528313607\n",
      "Steps : 86600, \t Total Gen Loss : 31.11690902709961, \t Total Dis Loss : 0.0007828675443306565\n",
      "Steps : 86700, \t Total Gen Loss : 31.023147583007812, \t Total Dis Loss : 0.0007197565864771605\n",
      "Steps : 86800, \t Total Gen Loss : 33.51044845581055, \t Total Dis Loss : 0.0005820644437335432\n",
      "Steps : 86900, \t Total Gen Loss : 34.26802062988281, \t Total Dis Loss : 0.00031286501325666904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 87000, \t Total Gen Loss : 33.16390609741211, \t Total Dis Loss : 0.0006462499732151628\n",
      "Steps : 87100, \t Total Gen Loss : 33.29837417602539, \t Total Dis Loss : 0.00020556579693220556\n",
      "Steps : 87200, \t Total Gen Loss : 32.07916259765625, \t Total Dis Loss : 0.00017673047841526568\n",
      "Steps : 87300, \t Total Gen Loss : 33.04389190673828, \t Total Dis Loss : 0.0004375680291559547\n",
      "Steps : 87400, \t Total Gen Loss : 33.540321350097656, \t Total Dis Loss : 0.0003460726875346154\n",
      "Steps : 87500, \t Total Gen Loss : 31.49864387512207, \t Total Dis Loss : 0.0005135653773322701\n",
      "Steps : 87600, \t Total Gen Loss : 34.95033645629883, \t Total Dis Loss : 0.00031819287687540054\n",
      "Steps : 87700, \t Total Gen Loss : 29.048856735229492, \t Total Dis Loss : 0.004119683522731066\n",
      "Time for epoch 13 is 302.15448904037476 sec\n",
      "Steps : 87800, \t Total Gen Loss : 32.71617126464844, \t Total Dis Loss : 0.0006608079420402646\n",
      "Steps : 87900, \t Total Gen Loss : 30.065826416015625, \t Total Dis Loss : 0.0009633911540731788\n",
      "Steps : 88000, \t Total Gen Loss : 32.24769592285156, \t Total Dis Loss : 0.0008626293274573982\n",
      "Steps : 88100, \t Total Gen Loss : 30.5907039642334, \t Total Dis Loss : 0.00360982702113688\n",
      "Steps : 88200, \t Total Gen Loss : 27.480594635009766, \t Total Dis Loss : 0.00168625230435282\n",
      "Steps : 88300, \t Total Gen Loss : 32.42390823364258, \t Total Dis Loss : 0.0008824048563838005\n",
      "Steps : 88400, \t Total Gen Loss : 33.74456024169922, \t Total Dis Loss : 0.0006132267299108207\n",
      "Steps : 88500, \t Total Gen Loss : 32.5256233215332, \t Total Dis Loss : 0.0006492093671113253\n",
      "Steps : 88600, \t Total Gen Loss : 31.513477325439453, \t Total Dis Loss : 0.0006233846070244908\n",
      "Steps : 88700, \t Total Gen Loss : 30.096811294555664, \t Total Dis Loss : 0.0007598093361593783\n",
      "Steps : 88800, \t Total Gen Loss : 29.255962371826172, \t Total Dis Loss : 0.0006191389984451234\n",
      "Steps : 88900, \t Total Gen Loss : 31.49797821044922, \t Total Dis Loss : 0.000327065063174814\n",
      "Steps : 89000, \t Total Gen Loss : 30.91375160217285, \t Total Dis Loss : 0.0002924454747699201\n",
      "Steps : 89100, \t Total Gen Loss : 31.54697608947754, \t Total Dis Loss : 0.00022253277711570263\n",
      "Steps : 89200, \t Total Gen Loss : 31.72304344177246, \t Total Dis Loss : 0.00036857681698165834\n",
      "Steps : 89300, \t Total Gen Loss : 31.132761001586914, \t Total Dis Loss : 0.0004290352517273277\n",
      "Steps : 89400, \t Total Gen Loss : 31.41036605834961, \t Total Dis Loss : 0.0008443120750598609\n",
      "Steps : 89500, \t Total Gen Loss : 33.72958755493164, \t Total Dis Loss : 0.0013267301255837083\n",
      "Steps : 89600, \t Total Gen Loss : 32.26750564575195, \t Total Dis Loss : 0.0003471590462140739\n",
      "Steps : 89700, \t Total Gen Loss : 33.58198928833008, \t Total Dis Loss : 0.00019474253349471837\n",
      "Steps : 89800, \t Total Gen Loss : 31.315799713134766, \t Total Dis Loss : 0.0003723195695783943\n",
      "Steps : 89900, \t Total Gen Loss : 33.172611236572266, \t Total Dis Loss : 0.000228017961489968\n",
      "Steps : 90000, \t Total Gen Loss : 32.86686325073242, \t Total Dis Loss : 0.00029257184360176325\n",
      "Steps : 90100, \t Total Gen Loss : 32.16810989379883, \t Total Dis Loss : 0.0006351497722789645\n",
      "Steps : 90200, \t Total Gen Loss : 35.228294372558594, \t Total Dis Loss : 0.00021070311777293682\n",
      "Steps : 90300, \t Total Gen Loss : 33.170047760009766, \t Total Dis Loss : 0.00016218842938542366\n",
      "Steps : 90400, \t Total Gen Loss : 31.976503372192383, \t Total Dis Loss : 0.0017749957041814923\n",
      "Steps : 90500, \t Total Gen Loss : 35.24050521850586, \t Total Dis Loss : 0.00264188414439559\n",
      "Steps : 90600, \t Total Gen Loss : 33.18964385986328, \t Total Dis Loss : 0.0018093226244673133\n",
      "Steps : 90700, \t Total Gen Loss : 30.41004753112793, \t Total Dis Loss : 0.0007125575793907046\n",
      "Steps : 90800, \t Total Gen Loss : 35.27482604980469, \t Total Dis Loss : 0.0009842256549745798\n",
      "Steps : 90900, \t Total Gen Loss : 30.71927833557129, \t Total Dis Loss : 0.0014573235530406237\n",
      "Steps : 91000, \t Total Gen Loss : 35.48540115356445, \t Total Dis Loss : 0.0004548816941678524\n",
      "Steps : 91100, \t Total Gen Loss : 33.828495025634766, \t Total Dis Loss : 0.0002547510084696114\n",
      "Steps : 91200, \t Total Gen Loss : 32.667518615722656, \t Total Dis Loss : 0.000344185798894614\n",
      "Steps : 91300, \t Total Gen Loss : 32.72209548950195, \t Total Dis Loss : 0.0003941543400287628\n",
      "Steps : 91400, \t Total Gen Loss : 31.565956115722656, \t Total Dis Loss : 0.0010219807736575603\n",
      "Steps : 91500, \t Total Gen Loss : 32.0390625, \t Total Dis Loss : 0.0003047066566068679\n",
      "Steps : 91600, \t Total Gen Loss : 31.670700073242188, \t Total Dis Loss : 0.0009584931540302932\n",
      "Steps : 91700, \t Total Gen Loss : 33.57038879394531, \t Total Dis Loss : 0.0004471266584005207\n",
      "Steps : 91800, \t Total Gen Loss : 30.770221710205078, \t Total Dis Loss : 0.0003685576084535569\n",
      "Steps : 91900, \t Total Gen Loss : 31.711822509765625, \t Total Dis Loss : 0.00039225423824973404\n",
      "Steps : 92000, \t Total Gen Loss : 34.41789627075195, \t Total Dis Loss : 0.0003442772140260786\n",
      "Steps : 92100, \t Total Gen Loss : 32.181373596191406, \t Total Dis Loss : 0.0002991134824696928\n",
      "Steps : 92200, \t Total Gen Loss : 35.12879180908203, \t Total Dis Loss : 0.0004209940670989454\n",
      "Steps : 92300, \t Total Gen Loss : 32.348602294921875, \t Total Dis Loss : 0.0005227593937888741\n",
      "Steps : 92400, \t Total Gen Loss : 32.955665588378906, \t Total Dis Loss : 0.00015285983681678772\n",
      "Steps : 92500, \t Total Gen Loss : 33.53327178955078, \t Total Dis Loss : 0.0003172951110173017\n",
      "Steps : 92600, \t Total Gen Loss : 31.671289443969727, \t Total Dis Loss : 0.0004799672751687467\n",
      "Steps : 92700, \t Total Gen Loss : 33.53025817871094, \t Total Dis Loss : 0.0005000013625249267\n",
      "Steps : 92800, \t Total Gen Loss : 31.682405471801758, \t Total Dis Loss : 0.0006656523328274488\n",
      "Steps : 92900, \t Total Gen Loss : 32.94947052001953, \t Total Dis Loss : 0.0007672694628126919\n",
      "Steps : 93000, \t Total Gen Loss : 32.672019958496094, \t Total Dis Loss : 0.0004435424052644521\n",
      "Steps : 93100, \t Total Gen Loss : 32.562171936035156, \t Total Dis Loss : 0.00024407805176451802\n",
      "Steps : 93200, \t Total Gen Loss : 32.529808044433594, \t Total Dis Loss : 0.0017430174630135298\n",
      "Steps : 93300, \t Total Gen Loss : 30.435312271118164, \t Total Dis Loss : 0.0005161260487511754\n",
      "Steps : 93400, \t Total Gen Loss : 31.9747257232666, \t Total Dis Loss : 0.001950655598193407\n",
      "Steps : 93500, \t Total Gen Loss : 32.945762634277344, \t Total Dis Loss : 0.0005693096318282187\n",
      "Steps : 93600, \t Total Gen Loss : 32.48322677612305, \t Total Dis Loss : 0.00033405618160031736\n",
      "Steps : 93700, \t Total Gen Loss : 33.51728820800781, \t Total Dis Loss : 0.0006885629263706505\n",
      "Steps : 93800, \t Total Gen Loss : 35.23332595825195, \t Total Dis Loss : 0.00041776805301196873\n",
      "Steps : 93900, \t Total Gen Loss : 33.689002990722656, \t Total Dis Loss : 0.0001602200500201434\n",
      "Steps : 94000, \t Total Gen Loss : 32.161781311035156, \t Total Dis Loss : 0.000403865851694718\n",
      "Steps : 94100, \t Total Gen Loss : 35.12023162841797, \t Total Dis Loss : 0.00017847827984951437\n",
      "Steps : 94200, \t Total Gen Loss : 34.145301818847656, \t Total Dis Loss : 0.0009572873241268098\n",
      "Steps : 94300, \t Total Gen Loss : 34.630035400390625, \t Total Dis Loss : 0.00013032590504735708\n",
      "Steps : 94400, \t Total Gen Loss : 33.80503845214844, \t Total Dis Loss : 0.00014676527644041926\n",
      "Steps : 94500, \t Total Gen Loss : 29.928058624267578, \t Total Dis Loss : 0.8030054569244385\n",
      "Time for epoch 14 is 305.59308075904846 sec\n",
      "Steps : 94600, \t Total Gen Loss : 33.645606994628906, \t Total Dis Loss : 0.0029076975770294666\n",
      "Steps : 94700, \t Total Gen Loss : 32.827274322509766, \t Total Dis Loss : 0.0012897218111902475\n",
      "Steps : 94800, \t Total Gen Loss : 35.410911560058594, \t Total Dis Loss : 0.0002035659708781168\n",
      "Steps : 94900, \t Total Gen Loss : 34.2004280090332, \t Total Dis Loss : 0.0006977731827646494\n",
      "Steps : 95000, \t Total Gen Loss : 32.352210998535156, \t Total Dis Loss : 0.0007857365999370813\n",
      "Steps : 95100, \t Total Gen Loss : 34.963890075683594, \t Total Dis Loss : 0.0016513648442924023\n",
      "Steps : 95200, \t Total Gen Loss : 33.500308990478516, \t Total Dis Loss : 0.0006748454179614782\n",
      "Steps : 95300, \t Total Gen Loss : 35.59687423706055, \t Total Dis Loss : 0.0005441062967292964\n",
      "Steps : 95400, \t Total Gen Loss : 36.46803665161133, \t Total Dis Loss : 0.0012590406695380807\n",
      "Steps : 95500, \t Total Gen Loss : 34.063602447509766, \t Total Dis Loss : 0.0008188635110855103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 95600, \t Total Gen Loss : 32.89784240722656, \t Total Dis Loss : 0.0007004672661423683\n",
      "Steps : 95700, \t Total Gen Loss : 37.70600891113281, \t Total Dis Loss : 0.000561353808734566\n",
      "Steps : 95800, \t Total Gen Loss : 31.41295051574707, \t Total Dis Loss : 0.0006226070690900087\n",
      "Steps : 95900, \t Total Gen Loss : 32.31907272338867, \t Total Dis Loss : 0.0005302269710227847\n",
      "Steps : 96000, \t Total Gen Loss : 34.82505798339844, \t Total Dis Loss : 0.00025006892974488437\n",
      "Steps : 96100, \t Total Gen Loss : 36.66408920288086, \t Total Dis Loss : 0.00026076281210407615\n",
      "Steps : 96200, \t Total Gen Loss : 34.24776077270508, \t Total Dis Loss : 0.0010829375823959708\n",
      "Steps : 96300, \t Total Gen Loss : 32.69718933105469, \t Total Dis Loss : 0.0009758603409864008\n",
      "Steps : 96400, \t Total Gen Loss : 34.197940826416016, \t Total Dis Loss : 0.0007173575577326119\n",
      "Steps : 96500, \t Total Gen Loss : 31.237674713134766, \t Total Dis Loss : 0.0003671942977234721\n",
      "Steps : 96600, \t Total Gen Loss : 30.773414611816406, \t Total Dis Loss : 0.0004331581003498286\n",
      "Steps : 96700, \t Total Gen Loss : 33.473175048828125, \t Total Dis Loss : 9.784242865862325e-05\n",
      "Steps : 96800, \t Total Gen Loss : 32.001670837402344, \t Total Dis Loss : 0.0001426556846126914\n",
      "Steps : 96900, \t Total Gen Loss : 32.20755386352539, \t Total Dis Loss : 0.0007226775051094592\n",
      "Steps : 97000, \t Total Gen Loss : 30.419885635375977, \t Total Dis Loss : 0.00046763496357016265\n",
      "Steps : 97100, \t Total Gen Loss : 30.67194366455078, \t Total Dis Loss : 0.00017229910008609295\n",
      "Steps : 97200, \t Total Gen Loss : 32.449119567871094, \t Total Dis Loss : 0.0003683220420498401\n",
      "Steps : 97300, \t Total Gen Loss : 32.91024398803711, \t Total Dis Loss : 0.00016924562805797905\n",
      "Steps : 97400, \t Total Gen Loss : 31.768035888671875, \t Total Dis Loss : 0.00022620195522904396\n",
      "Steps : 97500, \t Total Gen Loss : 34.935997009277344, \t Total Dis Loss : 0.00021605784422717988\n",
      "Steps : 97600, \t Total Gen Loss : 33.49586868286133, \t Total Dis Loss : 0.0003536049334798008\n",
      "Steps : 97700, \t Total Gen Loss : 33.70345687866211, \t Total Dis Loss : 0.0002546006871853024\n",
      "Steps : 97800, \t Total Gen Loss : 31.442718505859375, \t Total Dis Loss : 0.00028107495745643973\n",
      "Steps : 97900, \t Total Gen Loss : 29.762548446655273, \t Total Dis Loss : 0.0004167714505456388\n",
      "Steps : 98000, \t Total Gen Loss : 30.342437744140625, \t Total Dis Loss : 0.001194196054711938\n",
      "Steps : 98100, \t Total Gen Loss : 32.01634216308594, \t Total Dis Loss : 0.00044061552034690976\n",
      "Steps : 98200, \t Total Gen Loss : 31.649099349975586, \t Total Dis Loss : 0.0012309708399698138\n",
      "Steps : 98300, \t Total Gen Loss : 32.04058074951172, \t Total Dis Loss : 0.00040630355942994356\n",
      "Steps : 98400, \t Total Gen Loss : 33.28066635131836, \t Total Dis Loss : 0.00018843324505724013\n",
      "Steps : 98500, \t Total Gen Loss : 33.46807098388672, \t Total Dis Loss : 0.00040866443305276334\n",
      "Steps : 98600, \t Total Gen Loss : 33.71073913574219, \t Total Dis Loss : 0.0007640571566298604\n",
      "Steps : 98700, \t Total Gen Loss : 32.05846405029297, \t Total Dis Loss : 0.0010209651663899422\n",
      "Steps : 98800, \t Total Gen Loss : 32.26544189453125, \t Total Dis Loss : 0.0005456742364913225\n",
      "Steps : 98900, \t Total Gen Loss : 33.637290954589844, \t Total Dis Loss : 0.0005755802267231047\n",
      "Steps : 99000, \t Total Gen Loss : 34.18581771850586, \t Total Dis Loss : 0.00040386285400018096\n",
      "Steps : 99100, \t Total Gen Loss : 30.921031951904297, \t Total Dis Loss : 0.0014560308773070574\n",
      "Steps : 99200, \t Total Gen Loss : 32.48680877685547, \t Total Dis Loss : 0.0008621683809906244\n",
      "Steps : 99300, \t Total Gen Loss : 31.348461151123047, \t Total Dis Loss : 0.00043416168773546815\n",
      "Steps : 99400, \t Total Gen Loss : 32.75802993774414, \t Total Dis Loss : 0.0003376017266418785\n",
      "Steps : 99500, \t Total Gen Loss : 33.98046112060547, \t Total Dis Loss : 0.00013184193812776357\n",
      "Steps : 99600, \t Total Gen Loss : 32.438323974609375, \t Total Dis Loss : 0.000498989422339946\n",
      "Steps : 99700, \t Total Gen Loss : 33.720237731933594, \t Total Dis Loss : 0.0001552821631776169\n",
      "Steps : 99800, \t Total Gen Loss : 32.38689422607422, \t Total Dis Loss : 0.0003719342639669776\n",
      "Steps : 99900, \t Total Gen Loss : 29.62455940246582, \t Total Dis Loss : 0.00048117159167304635\n",
      "Steps : 100000, \t Total Gen Loss : 29.044084548950195, \t Total Dis Loss : 0.00638150330632925\n",
      "Steps : 100100, \t Total Gen Loss : 30.07250213623047, \t Total Dis Loss : 0.0007374804699793458\n",
      "Steps : 100200, \t Total Gen Loss : 30.368457794189453, \t Total Dis Loss : 0.0008676630677655339\n",
      "Steps : 100300, \t Total Gen Loss : 32.85240936279297, \t Total Dis Loss : 0.00022493676806334406\n",
      "Steps : 100400, \t Total Gen Loss : 32.11647415161133, \t Total Dis Loss : 0.00031402386957779527\n",
      "Steps : 100500, \t Total Gen Loss : 32.82105255126953, \t Total Dis Loss : 0.00020109020988456905\n",
      "Steps : 100600, \t Total Gen Loss : 31.44198989868164, \t Total Dis Loss : 0.000398402800783515\n",
      "Steps : 100700, \t Total Gen Loss : 35.334617614746094, \t Total Dis Loss : 0.0005010432796552777\n",
      "Steps : 100800, \t Total Gen Loss : 31.657691955566406, \t Total Dis Loss : 9.794064681045711e-05\n",
      "Steps : 100900, \t Total Gen Loss : 30.583847045898438, \t Total Dis Loss : 0.00012187514948891476\n",
      "Steps : 101000, \t Total Gen Loss : 33.11502456665039, \t Total Dis Loss : 0.000707081169821322\n",
      "Steps : 101100, \t Total Gen Loss : 31.8210391998291, \t Total Dis Loss : 0.0005715785664506257\n",
      "Steps : 101200, \t Total Gen Loss : 30.473989486694336, \t Total Dis Loss : 0.00040057767182588577\n",
      "Time for epoch 15 is 304.4484124183655 sec\n",
      "Steps : 101300, \t Total Gen Loss : 30.968830108642578, \t Total Dis Loss : 0.0004467793623916805\n",
      "Steps : 101400, \t Total Gen Loss : 30.95138168334961, \t Total Dis Loss : 0.0003830899077001959\n",
      "Steps : 101500, \t Total Gen Loss : 31.61463737487793, \t Total Dis Loss : 0.0002745090168900788\n",
      "Steps : 101600, \t Total Gen Loss : 31.574535369873047, \t Total Dis Loss : 0.00024273096641991287\n",
      "Steps : 101700, \t Total Gen Loss : 33.420814514160156, \t Total Dis Loss : 0.0002355604083277285\n",
      "Steps : 101800, \t Total Gen Loss : 33.06365966796875, \t Total Dis Loss : 0.00023605926253367215\n",
      "Steps : 101900, \t Total Gen Loss : 32.82089614868164, \t Total Dis Loss : 0.0006537748849950731\n",
      "Steps : 102000, \t Total Gen Loss : 33.49899673461914, \t Total Dis Loss : 0.00040986138628795743\n",
      "Steps : 102100, \t Total Gen Loss : 31.27315902709961, \t Total Dis Loss : 0.0003285018028691411\n",
      "Steps : 102200, \t Total Gen Loss : 32.310237884521484, \t Total Dis Loss : 0.00021745124831795692\n",
      "Steps : 102300, \t Total Gen Loss : 33.562808990478516, \t Total Dis Loss : 0.00043976132292300463\n",
      "Steps : 102400, \t Total Gen Loss : 26.978450775146484, \t Total Dis Loss : 1.076632022857666\n",
      "Steps : 102500, \t Total Gen Loss : 29.53047752380371, \t Total Dis Loss : 0.0024096376728266478\n",
      "Steps : 102600, \t Total Gen Loss : 31.108428955078125, \t Total Dis Loss : 0.0008832654566504061\n",
      "Steps : 102700, \t Total Gen Loss : 30.982635498046875, \t Total Dis Loss : 0.0016795755364000797\n",
      "Steps : 102800, \t Total Gen Loss : 31.615184783935547, \t Total Dis Loss : 0.0004604632849805057\n",
      "Steps : 102900, \t Total Gen Loss : 31.91867446899414, \t Total Dis Loss : 0.0003093637351412326\n",
      "Steps : 103000, \t Total Gen Loss : 32.217594146728516, \t Total Dis Loss : 0.0002470454201102257\n",
      "Steps : 103100, \t Total Gen Loss : 31.70740509033203, \t Total Dis Loss : 0.000565505470149219\n",
      "Steps : 103200, \t Total Gen Loss : 32.41230773925781, \t Total Dis Loss : 0.0003833276277873665\n",
      "Steps : 103300, \t Total Gen Loss : 31.593894958496094, \t Total Dis Loss : 0.0012638133484870195\n",
      "Steps : 103400, \t Total Gen Loss : 30.471532821655273, \t Total Dis Loss : 0.9651220440864563\n",
      "Steps : 103500, \t Total Gen Loss : 33.507835388183594, \t Total Dis Loss : 0.0006737671210430562\n",
      "Steps : 103600, \t Total Gen Loss : 33.50181198120117, \t Total Dis Loss : 0.0009786965092644095\n",
      "Steps : 103700, \t Total Gen Loss : 33.103431701660156, \t Total Dis Loss : 0.0009345976868644357\n",
      "Steps : 103800, \t Total Gen Loss : 31.829452514648438, \t Total Dis Loss : 0.0007585035637021065\n",
      "Steps : 103900, \t Total Gen Loss : 32.61616134643555, \t Total Dis Loss : 0.00047466240357607603\n",
      "Steps : 104000, \t Total Gen Loss : 32.38792419433594, \t Total Dis Loss : 0.0010955177713185549\n",
      "Steps : 104100, \t Total Gen Loss : 29.820070266723633, \t Total Dis Loss : 0.0005546512547880411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 104200, \t Total Gen Loss : 31.851078033447266, \t Total Dis Loss : 0.0003635350149124861\n",
      "Steps : 104300, \t Total Gen Loss : 32.441680908203125, \t Total Dis Loss : 0.0005400383961386979\n",
      "Steps : 104400, \t Total Gen Loss : 32.905296325683594, \t Total Dis Loss : 0.002747524995356798\n",
      "Steps : 104500, \t Total Gen Loss : 34.046791076660156, \t Total Dis Loss : 0.0002956985554192215\n",
      "Steps : 104600, \t Total Gen Loss : 31.79631996154785, \t Total Dis Loss : 0.0009739297674968839\n",
      "Steps : 104700, \t Total Gen Loss : 28.7874755859375, \t Total Dis Loss : 0.0010027977405115962\n",
      "Steps : 104800, \t Total Gen Loss : 27.909095764160156, \t Total Dis Loss : 0.007260642945766449\n",
      "Steps : 104900, \t Total Gen Loss : 31.5988826751709, \t Total Dis Loss : 0.0008165487088263035\n",
      "Steps : 105000, \t Total Gen Loss : 30.727855682373047, \t Total Dis Loss : 0.0008045912254601717\n",
      "Steps : 105100, \t Total Gen Loss : 34.078617095947266, \t Total Dis Loss : 0.00024959418806247413\n",
      "Steps : 105200, \t Total Gen Loss : 33.263710021972656, \t Total Dis Loss : 0.000222955466597341\n",
      "Steps : 105300, \t Total Gen Loss : 34.252037048339844, \t Total Dis Loss : 0.002194490749388933\n",
      "Steps : 105400, \t Total Gen Loss : 35.40724563598633, \t Total Dis Loss : 0.0012572244741022587\n",
      "Steps : 105500, \t Total Gen Loss : 31.002946853637695, \t Total Dis Loss : 0.00039335127803497016\n",
      "Steps : 105600, \t Total Gen Loss : 32.31264114379883, \t Total Dis Loss : 0.0003893477551173419\n",
      "Steps : 105700, \t Total Gen Loss : 35.43056106567383, \t Total Dis Loss : 0.000171134845004417\n",
      "Steps : 105800, \t Total Gen Loss : 32.06979751586914, \t Total Dis Loss : 0.0004342926840763539\n",
      "Steps : 105900, \t Total Gen Loss : 29.930437088012695, \t Total Dis Loss : 0.0007121088565327227\n",
      "Steps : 106000, \t Total Gen Loss : 33.62360382080078, \t Total Dis Loss : 0.0002770286810118705\n",
      "Steps : 106100, \t Total Gen Loss : 31.929912567138672, \t Total Dis Loss : 0.00027116466662846506\n",
      "Steps : 106200, \t Total Gen Loss : 31.765544891357422, \t Total Dis Loss : 0.00029555734363384545\n",
      "Steps : 106300, \t Total Gen Loss : 32.91926956176758, \t Total Dis Loss : 0.00018345710122957826\n",
      "Steps : 106400, \t Total Gen Loss : 32.60784912109375, \t Total Dis Loss : 0.0002742291253525764\n",
      "Steps : 106500, \t Total Gen Loss : 32.50098419189453, \t Total Dis Loss : 0.00027496303664520383\n",
      "Steps : 106600, \t Total Gen Loss : 33.15860366821289, \t Total Dis Loss : 0.00016650414909236133\n",
      "Steps : 106700, \t Total Gen Loss : 31.49732208251953, \t Total Dis Loss : 0.0007929347921162844\n",
      "Steps : 106800, \t Total Gen Loss : 29.498769760131836, \t Total Dis Loss : 0.0005006392602808774\n",
      "Steps : 106900, \t Total Gen Loss : 31.530885696411133, \t Total Dis Loss : 0.0005535206873901188\n",
      "Steps : 107000, \t Total Gen Loss : 32.990840911865234, \t Total Dis Loss : 0.00039976579137146473\n",
      "Steps : 107100, \t Total Gen Loss : 29.88359260559082, \t Total Dis Loss : 0.0005839489167556167\n",
      "Steps : 107200, \t Total Gen Loss : 31.524883270263672, \t Total Dis Loss : 0.00047997970250435174\n",
      "Steps : 107300, \t Total Gen Loss : 33.29511642456055, \t Total Dis Loss : 0.0021083035971969366\n",
      "Steps : 107400, \t Total Gen Loss : 35.82823944091797, \t Total Dis Loss : 0.00025237168301828206\n",
      "Steps : 107500, \t Total Gen Loss : 33.589969635009766, \t Total Dis Loss : 0.0005164939793758094\n",
      "Steps : 107600, \t Total Gen Loss : 32.29033660888672, \t Total Dis Loss : 0.00045974564272910357\n",
      "Steps : 107700, \t Total Gen Loss : 32.78287124633789, \t Total Dis Loss : 0.0005220556631684303\n",
      "Steps : 107800, \t Total Gen Loss : 33.04963684082031, \t Total Dis Loss : 0.0005656811990775168\n",
      "Steps : 107900, \t Total Gen Loss : 32.89274215698242, \t Total Dis Loss : 0.0004982751561328769\n",
      "Steps : 108000, \t Total Gen Loss : 32.03559494018555, \t Total Dis Loss : 0.00025214836932718754\n",
      "Time for epoch 16 is 307.04909563064575 sec\n",
      "Steps : 108100, \t Total Gen Loss : 34.05937194824219, \t Total Dis Loss : 0.00019489576516207308\n",
      "Steps : 108200, \t Total Gen Loss : 34.82447052001953, \t Total Dis Loss : 8.392894233111292e-05\n",
      "Steps : 108300, \t Total Gen Loss : 30.543912887573242, \t Total Dis Loss : 0.00024776210193522274\n",
      "Steps : 108400, \t Total Gen Loss : 29.42194938659668, \t Total Dis Loss : 0.0007264144369401038\n",
      "Steps : 108500, \t Total Gen Loss : 31.560474395751953, \t Total Dis Loss : 0.00027047941694036126\n",
      "Steps : 108600, \t Total Gen Loss : 31.484771728515625, \t Total Dis Loss : 0.0006874189712107182\n",
      "Steps : 108700, \t Total Gen Loss : 31.912572860717773, \t Total Dis Loss : 0.00026164166047237813\n",
      "Steps : 108800, \t Total Gen Loss : 30.114791870117188, \t Total Dis Loss : 0.001016982365399599\n",
      "Steps : 108900, \t Total Gen Loss : 31.896255493164062, \t Total Dis Loss : 0.0003048153594136238\n",
      "Steps : 109000, \t Total Gen Loss : 29.753498077392578, \t Total Dis Loss : 0.0027113528922200203\n",
      "Steps : 109100, \t Total Gen Loss : 29.40242576599121, \t Total Dis Loss : 0.0011746393283829093\n",
      "Steps : 109200, \t Total Gen Loss : 33.22026824951172, \t Total Dis Loss : 0.0002002217952394858\n",
      "Steps : 109300, \t Total Gen Loss : 32.21413040161133, \t Total Dis Loss : 0.0005235657445155084\n",
      "Steps : 109400, \t Total Gen Loss : 31.08521842956543, \t Total Dis Loss : 0.0006201070500537753\n",
      "Steps : 109500, \t Total Gen Loss : 31.54645538330078, \t Total Dis Loss : 0.0017388995038345456\n",
      "Steps : 109600, \t Total Gen Loss : 31.836503982543945, \t Total Dis Loss : 0.00042401894461363554\n",
      "Steps : 109700, \t Total Gen Loss : 32.43059539794922, \t Total Dis Loss : 0.00027517013950273395\n",
      "Steps : 109800, \t Total Gen Loss : 30.761964797973633, \t Total Dis Loss : 0.0003514797717798501\n",
      "Steps : 109900, \t Total Gen Loss : 33.39895248413086, \t Total Dis Loss : 0.00140376144554466\n",
      "Steps : 110000, \t Total Gen Loss : 32.629520416259766, \t Total Dis Loss : 0.000413600355386734\n",
      "Steps : 110100, \t Total Gen Loss : 30.99700927734375, \t Total Dis Loss : 0.00043621519580483437\n",
      "Steps : 110200, \t Total Gen Loss : 32.8833122253418, \t Total Dis Loss : 0.00027340135420672596\n",
      "Steps : 110300, \t Total Gen Loss : 31.407028198242188, \t Total Dis Loss : 0.00019279864500276744\n",
      "Steps : 110400, \t Total Gen Loss : 30.606481552124023, \t Total Dis Loss : 0.00015331718896050006\n",
      "Steps : 110500, \t Total Gen Loss : 31.48528289794922, \t Total Dis Loss : 0.00023434756440110505\n",
      "Steps : 110600, \t Total Gen Loss : 31.504789352416992, \t Total Dis Loss : 0.0002738389594014734\n",
      "Steps : 110700, \t Total Gen Loss : 33.81195068359375, \t Total Dis Loss : 0.0005260315956547856\n",
      "Steps : 110800, \t Total Gen Loss : 31.90192413330078, \t Total Dis Loss : 0.0012202607467770576\n",
      "Steps : 110900, \t Total Gen Loss : 33.58978271484375, \t Total Dis Loss : 0.0001607529993634671\n",
      "Steps : 111000, \t Total Gen Loss : 33.74346160888672, \t Total Dis Loss : 0.00013156846398487687\n",
      "Steps : 111100, \t Total Gen Loss : 32.63857650756836, \t Total Dis Loss : 8.2163474871777e-05\n",
      "Steps : 111200, \t Total Gen Loss : 31.60124969482422, \t Total Dis Loss : 0.0002581561275292188\n",
      "Steps : 111300, \t Total Gen Loss : 31.404869079589844, \t Total Dis Loss : 0.0003925944911316037\n",
      "Steps : 111400, \t Total Gen Loss : 34.335853576660156, \t Total Dis Loss : 0.0002817657368723303\n",
      "Steps : 111500, \t Total Gen Loss : 30.161102294921875, \t Total Dis Loss : 0.01889059506356716\n",
      "Steps : 111600, \t Total Gen Loss : 33.26617431640625, \t Total Dis Loss : 0.0006690794252790511\n",
      "Steps : 111700, \t Total Gen Loss : 31.761878967285156, \t Total Dis Loss : 0.0005716262385249138\n",
      "Steps : 111800, \t Total Gen Loss : 34.212852478027344, \t Total Dis Loss : 0.00027336261700838804\n",
      "Steps : 111900, \t Total Gen Loss : 31.850513458251953, \t Total Dis Loss : 0.0008618776919320226\n",
      "Steps : 112000, \t Total Gen Loss : 32.94739532470703, \t Total Dis Loss : 0.0002588684146758169\n",
      "Steps : 112100, \t Total Gen Loss : 33.41900634765625, \t Total Dis Loss : 0.00013614130148198456\n",
      "Steps : 112200, \t Total Gen Loss : 32.488956451416016, \t Total Dis Loss : 9.940350719261914e-05\n",
      "Steps : 112300, \t Total Gen Loss : 31.979482650756836, \t Total Dis Loss : 0.00023556842643301934\n",
      "Steps : 112400, \t Total Gen Loss : 33.48194885253906, \t Total Dis Loss : 0.0002160668227588758\n",
      "Steps : 112500, \t Total Gen Loss : 29.961380004882812, \t Total Dis Loss : 0.00455024279654026\n",
      "Steps : 112600, \t Total Gen Loss : 30.5290584564209, \t Total Dis Loss : 0.00021452319924719632\n",
      "Steps : 112700, \t Total Gen Loss : 33.12127685546875, \t Total Dis Loss : 0.0003284170525148511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 112800, \t Total Gen Loss : 30.770580291748047, \t Total Dis Loss : 0.00018734560580924153\n",
      "Steps : 112900, \t Total Gen Loss : 31.918479919433594, \t Total Dis Loss : 0.00020131048222538084\n",
      "Steps : 113000, \t Total Gen Loss : 30.526885986328125, \t Total Dis Loss : 0.00014537926472257823\n",
      "Steps : 113100, \t Total Gen Loss : 31.744653701782227, \t Total Dis Loss : 0.00015621307829860598\n",
      "Steps : 113200, \t Total Gen Loss : 31.71796417236328, \t Total Dis Loss : 0.00017182585725095123\n",
      "Steps : 113300, \t Total Gen Loss : 31.901012420654297, \t Total Dis Loss : 0.000497013155836612\n",
      "Steps : 113400, \t Total Gen Loss : 32.165409088134766, \t Total Dis Loss : 0.00023165676975622773\n",
      "Steps : 113500, \t Total Gen Loss : 32.69413757324219, \t Total Dis Loss : 0.0002888671588152647\n",
      "Steps : 113600, \t Total Gen Loss : 30.863040924072266, \t Total Dis Loss : 0.0024311081506311893\n",
      "Steps : 113700, \t Total Gen Loss : 31.145267486572266, \t Total Dis Loss : 0.0007579234661534429\n",
      "Steps : 113800, \t Total Gen Loss : 27.563831329345703, \t Total Dis Loss : 0.00028313667280599475\n",
      "Steps : 113900, \t Total Gen Loss : 30.125202178955078, \t Total Dis Loss : 0.0010606099385768175\n",
      "Steps : 114000, \t Total Gen Loss : 30.964082717895508, \t Total Dis Loss : 0.0012539265444502234\n",
      "Steps : 114100, \t Total Gen Loss : 29.40857696533203, \t Total Dis Loss : 0.0011658844305202365\n",
      "Steps : 114200, \t Total Gen Loss : 30.769201278686523, \t Total Dis Loss : 0.00028790030046366155\n",
      "Steps : 114300, \t Total Gen Loss : 30.557924270629883, \t Total Dis Loss : 0.00046557903988286853\n",
      "Steps : 114400, \t Total Gen Loss : 33.126182556152344, \t Total Dis Loss : 0.0005599780124612153\n",
      "Steps : 114500, \t Total Gen Loss : 34.119972229003906, \t Total Dis Loss : 0.00029979305691085756\n",
      "Steps : 114600, \t Total Gen Loss : 29.761499404907227, \t Total Dis Loss : 0.0007742475718259811\n",
      "Steps : 114700, \t Total Gen Loss : 31.107759475708008, \t Total Dis Loss : 0.00017444328113924712\n",
      "Time for epoch 17 is 307.62815380096436 sec\n",
      "Steps : 114800, \t Total Gen Loss : 32.20092010498047, \t Total Dis Loss : 0.0002306045644218102\n",
      "Steps : 114900, \t Total Gen Loss : 32.95469284057617, \t Total Dis Loss : 0.00013216516526881605\n",
      "Steps : 115000, \t Total Gen Loss : 31.18968391418457, \t Total Dis Loss : 0.00048278490430675447\n",
      "Steps : 115100, \t Total Gen Loss : 29.689786911010742, \t Total Dis Loss : 0.002476754132658243\n",
      "Steps : 115200, \t Total Gen Loss : 31.995304107666016, \t Total Dis Loss : 0.0005648302030749619\n",
      "Steps : 115300, \t Total Gen Loss : 33.07526779174805, \t Total Dis Loss : 0.0003075928252656013\n",
      "Steps : 115400, \t Total Gen Loss : 31.030244827270508, \t Total Dis Loss : 0.00032987602753564715\n",
      "Steps : 115500, \t Total Gen Loss : 29.977718353271484, \t Total Dis Loss : 0.0002486016892362386\n",
      "Steps : 115600, \t Total Gen Loss : 33.797889709472656, \t Total Dis Loss : 0.00028162170201539993\n",
      "Steps : 115700, \t Total Gen Loss : 32.83918380737305, \t Total Dis Loss : 0.0002114228846039623\n",
      "Steps : 115800, \t Total Gen Loss : 33.54202651977539, \t Total Dis Loss : 0.00013855035649612546\n",
      "Steps : 115900, \t Total Gen Loss : 32.29098129272461, \t Total Dis Loss : 0.0001328896323684603\n",
      "Steps : 116000, \t Total Gen Loss : 33.615116119384766, \t Total Dis Loss : 0.0001461764331907034\n",
      "Steps : 116100, \t Total Gen Loss : 33.5548210144043, \t Total Dis Loss : 0.00015869466005824506\n",
      "Steps : 116200, \t Total Gen Loss : 33.05128479003906, \t Total Dis Loss : 0.00016890483675524592\n",
      "Steps : 116300, \t Total Gen Loss : 28.80770492553711, \t Total Dis Loss : 0.0007409368408843875\n",
      "Steps : 116400, \t Total Gen Loss : 30.935453414916992, \t Total Dis Loss : 0.0004760697192978114\n",
      "Steps : 116500, \t Total Gen Loss : 30.545074462890625, \t Total Dis Loss : 0.0003539749886840582\n",
      "Steps : 116600, \t Total Gen Loss : 29.67706871032715, \t Total Dis Loss : 0.0008265531505458057\n",
      "Steps : 116700, \t Total Gen Loss : 31.14554786682129, \t Total Dis Loss : 0.0003595211310312152\n",
      "Steps : 116800, \t Total Gen Loss : 28.777006149291992, \t Total Dis Loss : 0.0017903130501508713\n",
      "Steps : 116900, \t Total Gen Loss : 29.665006637573242, \t Total Dis Loss : 0.00034343753941357136\n",
      "Steps : 117000, \t Total Gen Loss : 32.23700714111328, \t Total Dis Loss : 0.0003950430836994201\n",
      "Steps : 117100, \t Total Gen Loss : 30.19344711303711, \t Total Dis Loss : 0.000810943660326302\n",
      "Steps : 117200, \t Total Gen Loss : 29.775981903076172, \t Total Dis Loss : 0.000902782310731709\n",
      "Steps : 117300, \t Total Gen Loss : 30.05504608154297, \t Total Dis Loss : 0.0006594531005248427\n",
      "Steps : 117400, \t Total Gen Loss : 32.617252349853516, \t Total Dis Loss : 0.0006265169358812273\n",
      "Steps : 117500, \t Total Gen Loss : 30.497779846191406, \t Total Dis Loss : 0.0009591815760359168\n",
      "Steps : 117600, \t Total Gen Loss : 30.86802101135254, \t Total Dis Loss : 0.0006578941247425973\n",
      "Steps : 117700, \t Total Gen Loss : 34.676612854003906, \t Total Dis Loss : 0.0002960598503705114\n",
      "Steps : 117800, \t Total Gen Loss : 34.50773239135742, \t Total Dis Loss : 0.00010031842248281464\n",
      "Steps : 117900, \t Total Gen Loss : 34.14715576171875, \t Total Dis Loss : 0.00043597299372777343\n",
      "Steps : 118000, \t Total Gen Loss : 29.04962158203125, \t Total Dis Loss : 0.024001283571124077\n",
      "Steps : 118100, \t Total Gen Loss : 30.005695343017578, \t Total Dis Loss : 0.0006746557774022222\n",
      "Steps : 118200, \t Total Gen Loss : 33.32296371459961, \t Total Dis Loss : 0.0015688390703871846\n",
      "Steps : 118300, \t Total Gen Loss : 30.276744842529297, \t Total Dis Loss : 0.00043412644299678504\n",
      "Steps : 118400, \t Total Gen Loss : 33.78193283081055, \t Total Dis Loss : 0.0007672178908251226\n",
      "Steps : 118500, \t Total Gen Loss : 32.57444381713867, \t Total Dis Loss : 0.002605237066745758\n",
      "Steps : 118600, \t Total Gen Loss : 32.56505584716797, \t Total Dis Loss : 0.00036403993726707995\n",
      "Steps : 118700, \t Total Gen Loss : 33.3938102722168, \t Total Dis Loss : 9.772418707143515e-05\n",
      "Steps : 118800, \t Total Gen Loss : 34.11335372924805, \t Total Dis Loss : 6.719106022501364e-05\n",
      "Steps : 118900, \t Total Gen Loss : 33.497318267822266, \t Total Dis Loss : 0.0007771970704197884\n",
      "Steps : 119000, \t Total Gen Loss : 31.647525787353516, \t Total Dis Loss : 0.00012954202247783542\n",
      "Steps : 119100, \t Total Gen Loss : 31.666332244873047, \t Total Dis Loss : 0.00021749056759290397\n",
      "Steps : 119200, \t Total Gen Loss : 32.200401306152344, \t Total Dis Loss : 0.00013353511167224497\n",
      "Steps : 119300, \t Total Gen Loss : 31.698673248291016, \t Total Dis Loss : 0.0001139753294410184\n",
      "Steps : 119400, \t Total Gen Loss : 34.965309143066406, \t Total Dis Loss : 0.0001890000858111307\n",
      "Steps : 119500, \t Total Gen Loss : 31.186861038208008, \t Total Dis Loss : 0.00014812694280408323\n",
      "Steps : 119600, \t Total Gen Loss : 35.808475494384766, \t Total Dis Loss : 0.0001087502168957144\n",
      "Steps : 119700, \t Total Gen Loss : 32.218849182128906, \t Total Dis Loss : 0.00012379800318740308\n",
      "Steps : 119800, \t Total Gen Loss : 33.17820358276367, \t Total Dis Loss : 0.0023723826743662357\n",
      "Steps : 119900, \t Total Gen Loss : 34.660797119140625, \t Total Dis Loss : 0.00018072070088237524\n",
      "Steps : 120000, \t Total Gen Loss : 31.83287811279297, \t Total Dis Loss : 0.00038903189124539495\n",
      "Steps : 120100, \t Total Gen Loss : 32.614681243896484, \t Total Dis Loss : 0.00023785207304172218\n",
      "Steps : 120200, \t Total Gen Loss : 30.464248657226562, \t Total Dis Loss : 0.00023570888151880354\n",
      "Steps : 120300, \t Total Gen Loss : 31.646099090576172, \t Total Dis Loss : 0.00019147968851029873\n",
      "Steps : 120400, \t Total Gen Loss : 30.12705421447754, \t Total Dis Loss : 0.0004570437013171613\n",
      "Steps : 120500, \t Total Gen Loss : 31.910831451416016, \t Total Dis Loss : 0.00019549949502106756\n",
      "Steps : 120600, \t Total Gen Loss : 33.79131317138672, \t Total Dis Loss : 0.00018292092136107385\n",
      "Steps : 120700, \t Total Gen Loss : 32.44880294799805, \t Total Dis Loss : 0.00017561893037054688\n",
      "Steps : 120800, \t Total Gen Loss : 33.79928970336914, \t Total Dis Loss : 0.00033114099642261863\n",
      "Steps : 120900, \t Total Gen Loss : 33.80168914794922, \t Total Dis Loss : 9.581405902281404e-05\n",
      "Steps : 121000, \t Total Gen Loss : 31.963571548461914, \t Total Dis Loss : 0.0005589356296695769\n",
      "Steps : 121100, \t Total Gen Loss : 29.451217651367188, \t Total Dis Loss : 0.00019960834470111877\n",
      "Steps : 121200, \t Total Gen Loss : 31.23044204711914, \t Total Dis Loss : 0.00013531594595406204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 121300, \t Total Gen Loss : 31.382665634155273, \t Total Dis Loss : 0.0001820057659642771\n",
      "Steps : 121400, \t Total Gen Loss : 28.724010467529297, \t Total Dis Loss : 0.0010169295128434896\n",
      "Steps : 121500, \t Total Gen Loss : 30.90595245361328, \t Total Dis Loss : 0.0005952034844085574\n",
      "Time for epoch 18 is 307.16548371315 sec\n",
      "Steps : 121600, \t Total Gen Loss : 31.68805503845215, \t Total Dis Loss : 0.0005109893972985446\n",
      "Steps : 121700, \t Total Gen Loss : 31.514833450317383, \t Total Dis Loss : 0.00040628324495628476\n",
      "Steps : 121800, \t Total Gen Loss : 28.883346557617188, \t Total Dis Loss : 0.0020203811582177877\n",
      "Steps : 121900, \t Total Gen Loss : 29.427616119384766, \t Total Dis Loss : 0.0004414586292114109\n",
      "Steps : 122000, \t Total Gen Loss : 33.769222259521484, \t Total Dis Loss : 0.0002696665469557047\n",
      "Steps : 122100, \t Total Gen Loss : 33.22295379638672, \t Total Dis Loss : 0.00016600916569586843\n",
      "Steps : 122200, \t Total Gen Loss : 30.714080810546875, \t Total Dis Loss : 0.0020337498281151056\n",
      "Steps : 122300, \t Total Gen Loss : 31.09014129638672, \t Total Dis Loss : 0.0004430348053574562\n",
      "Steps : 122400, \t Total Gen Loss : 33.31388473510742, \t Total Dis Loss : 0.00048375356709584594\n",
      "Steps : 122500, \t Total Gen Loss : 33.64645767211914, \t Total Dis Loss : 0.0001256456016562879\n",
      "Steps : 122600, \t Total Gen Loss : 36.333919525146484, \t Total Dis Loss : 0.0004936328623443842\n",
      "Steps : 122700, \t Total Gen Loss : 34.09410095214844, \t Total Dis Loss : 0.00035490485606715083\n",
      "Steps : 122800, \t Total Gen Loss : 32.76976776123047, \t Total Dis Loss : 0.00018362862465437502\n",
      "Steps : 122900, \t Total Gen Loss : 33.17879867553711, \t Total Dis Loss : 0.0001429375261068344\n",
      "Steps : 123000, \t Total Gen Loss : 32.03284454345703, \t Total Dis Loss : 0.0019771712832152843\n",
      "Steps : 123100, \t Total Gen Loss : 32.6097297668457, \t Total Dis Loss : 0.0005194809637032449\n",
      "Steps : 123200, \t Total Gen Loss : 31.31205940246582, \t Total Dis Loss : 0.0002859403030015528\n",
      "Steps : 123300, \t Total Gen Loss : 32.24520492553711, \t Total Dis Loss : 0.001102678244933486\n",
      "Steps : 123400, \t Total Gen Loss : 30.885087966918945, \t Total Dis Loss : 0.0012597665190696716\n",
      "Steps : 123500, \t Total Gen Loss : 31.74358558654785, \t Total Dis Loss : 0.0005278821336105466\n",
      "Steps : 123600, \t Total Gen Loss : 30.04282569885254, \t Total Dis Loss : 0.0002193577092839405\n",
      "Steps : 123700, \t Total Gen Loss : 33.22169494628906, \t Total Dis Loss : 0.000319150771247223\n",
      "Steps : 123800, \t Total Gen Loss : 32.66373062133789, \t Total Dis Loss : 0.00017479859525337815\n",
      "Steps : 123900, \t Total Gen Loss : 31.264785766601562, \t Total Dis Loss : 0.0003155685553792864\n",
      "Steps : 124000, \t Total Gen Loss : 31.733501434326172, \t Total Dis Loss : 0.00025986158289015293\n",
      "Steps : 124100, \t Total Gen Loss : 30.86500358581543, \t Total Dis Loss : 0.00027517025591805577\n",
      "Steps : 124200, \t Total Gen Loss : 33.38128662109375, \t Total Dis Loss : 0.00011348645784892142\n",
      "Steps : 124300, \t Total Gen Loss : 34.18453598022461, \t Total Dis Loss : 5.451434844871983e-05\n",
      "Steps : 124400, \t Total Gen Loss : 33.28498840332031, \t Total Dis Loss : 0.0001327942154603079\n",
      "Steps : 124500, \t Total Gen Loss : 34.41044235229492, \t Total Dis Loss : 0.00012373989738989621\n",
      "Steps : 124600, \t Total Gen Loss : 34.526065826416016, \t Total Dis Loss : 0.00012981319741811603\n",
      "Steps : 124700, \t Total Gen Loss : 32.23111343383789, \t Total Dis Loss : 0.0003494244592729956\n",
      "Steps : 124800, \t Total Gen Loss : 30.394777297973633, \t Total Dis Loss : 0.00022572129091713578\n",
      "Steps : 124900, \t Total Gen Loss : 29.645751953125, \t Total Dis Loss : 0.0007693640072830021\n",
      "Steps : 125000, \t Total Gen Loss : 30.75807762145996, \t Total Dis Loss : 0.0005793336313217878\n",
      "Steps : 125100, \t Total Gen Loss : 30.62698745727539, \t Total Dis Loss : 0.004181466531008482\n",
      "Steps : 125200, \t Total Gen Loss : 30.777482986450195, \t Total Dis Loss : 0.0005670757964253426\n",
      "Steps : 125300, \t Total Gen Loss : 31.771060943603516, \t Total Dis Loss : 0.0006215240573510528\n",
      "Steps : 125400, \t Total Gen Loss : 32.69374084472656, \t Total Dis Loss : 0.0003032522217836231\n",
      "Steps : 125500, \t Total Gen Loss : 32.61405944824219, \t Total Dis Loss : 0.00080563296796754\n",
      "Steps : 125600, \t Total Gen Loss : 31.65483856201172, \t Total Dis Loss : 0.00046484096674248576\n",
      "Steps : 125700, \t Total Gen Loss : 33.50636672973633, \t Total Dis Loss : 0.0003320511896163225\n",
      "Steps : 125800, \t Total Gen Loss : 29.436199188232422, \t Total Dis Loss : 0.001276631373912096\n",
      "Steps : 125900, \t Total Gen Loss : 33.21944808959961, \t Total Dis Loss : 0.02821897529065609\n",
      "Steps : 126000, \t Total Gen Loss : 33.72053527832031, \t Total Dis Loss : 6.598996696993709e-05\n",
      "Steps : 126100, \t Total Gen Loss : 32.2936897277832, \t Total Dis Loss : 0.0001518109638709575\n",
      "Steps : 126200, \t Total Gen Loss : 34.506629943847656, \t Total Dis Loss : 0.00013061263598501682\n",
      "Steps : 126300, \t Total Gen Loss : 33.60890579223633, \t Total Dis Loss : 0.000214749263250269\n",
      "Steps : 126400, \t Total Gen Loss : 31.567790985107422, \t Total Dis Loss : 0.00038398057222366333\n",
      "Steps : 126500, \t Total Gen Loss : 29.66033172607422, \t Total Dis Loss : 0.0012054096441715956\n",
      "Steps : 126600, \t Total Gen Loss : 29.25926399230957, \t Total Dis Loss : 0.004485865589231253\n",
      "Steps : 126700, \t Total Gen Loss : 28.857677459716797, \t Total Dis Loss : 0.0019219222012907267\n",
      "Steps : 126800, \t Total Gen Loss : 31.05699348449707, \t Total Dis Loss : 0.0005747846444137394\n",
      "Steps : 126900, \t Total Gen Loss : 34.62862777709961, \t Total Dis Loss : 5.232427065493539e-05\n",
      "Steps : 127000, \t Total Gen Loss : 31.32772445678711, \t Total Dis Loss : 0.0006804214790463448\n",
      "Steps : 127100, \t Total Gen Loss : 31.468942642211914, \t Total Dis Loss : 0.00020300174946896732\n",
      "Steps : 127200, \t Total Gen Loss : 28.367868423461914, \t Total Dis Loss : 0.0017525358125567436\n",
      "Steps : 127300, \t Total Gen Loss : 31.118497848510742, \t Total Dis Loss : 0.00018262970843352377\n",
      "Steps : 127400, \t Total Gen Loss : 33.11069107055664, \t Total Dis Loss : 0.0007206700975075364\n",
      "Steps : 127500, \t Total Gen Loss : 33.954925537109375, \t Total Dis Loss : 0.00011276252917014062\n",
      "Steps : 127600, \t Total Gen Loss : 30.64313316345215, \t Total Dis Loss : 0.00010688633483368903\n",
      "Steps : 127700, \t Total Gen Loss : 34.50680923461914, \t Total Dis Loss : 7.356757851084694e-05\n",
      "Steps : 127800, \t Total Gen Loss : 34.77077102661133, \t Total Dis Loss : 7.893644215073436e-05\n",
      "Steps : 127900, \t Total Gen Loss : 32.55228042602539, \t Total Dis Loss : 0.0001846296654548496\n",
      "Steps : 128000, \t Total Gen Loss : 35.99321746826172, \t Total Dis Loss : 0.002014234894886613\n",
      "Steps : 128100, \t Total Gen Loss : 32.2932014465332, \t Total Dis Loss : 0.0007405028445646167\n",
      "Steps : 128200, \t Total Gen Loss : 30.56298065185547, \t Total Dis Loss : 0.0003214531170669943\n",
      "Time for epoch 19 is 306.81401896476746 sec\n",
      "Steps : 128300, \t Total Gen Loss : 35.32060623168945, \t Total Dis Loss : 0.00018656997417565435\n",
      "Steps : 128400, \t Total Gen Loss : 32.656864166259766, \t Total Dis Loss : 0.00022602357785217464\n",
      "Steps : 128500, \t Total Gen Loss : 30.422462463378906, \t Total Dis Loss : 0.00032375246519222856\n",
      "Steps : 128600, \t Total Gen Loss : 32.515052795410156, \t Total Dis Loss : 0.0008901581168174744\n",
      "Steps : 128700, \t Total Gen Loss : 33.819068908691406, \t Total Dis Loss : 0.00019642262486740947\n",
      "Steps : 128800, \t Total Gen Loss : 35.690948486328125, \t Total Dis Loss : 0.0015763475093990564\n",
      "Steps : 128900, \t Total Gen Loss : 36.782630920410156, \t Total Dis Loss : 0.0006526463548652828\n",
      "Steps : 129000, \t Total Gen Loss : 31.92239761352539, \t Total Dis Loss : 0.0004351282259449363\n",
      "Steps : 129100, \t Total Gen Loss : 34.564476013183594, \t Total Dis Loss : 0.00041104608681052923\n",
      "Steps : 129200, \t Total Gen Loss : 31.981069564819336, \t Total Dis Loss : 0.0033718887716531754\n",
      "Steps : 129300, \t Total Gen Loss : 31.829395294189453, \t Total Dis Loss : 0.0015763663686811924\n",
      "Steps : 129400, \t Total Gen Loss : 31.346370697021484, \t Total Dis Loss : 0.0005608872161246836\n",
      "Steps : 129500, \t Total Gen Loss : 29.483013153076172, \t Total Dis Loss : 0.0005410676822066307\n",
      "Steps : 129600, \t Total Gen Loss : 31.48544692993164, \t Total Dis Loss : 0.0003742366679944098\n",
      "Steps : 129700, \t Total Gen Loss : 30.982776641845703, \t Total Dis Loss : 0.0031769501511007547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 129800, \t Total Gen Loss : 28.734766006469727, \t Total Dis Loss : 0.0005189592484384775\n",
      "Steps : 129900, \t Total Gen Loss : 31.859344482421875, \t Total Dis Loss : 0.00046296342043206096\n",
      "Steps : 130000, \t Total Gen Loss : 31.052867889404297, \t Total Dis Loss : 0.0004655339289456606\n",
      "Steps : 130100, \t Total Gen Loss : 32.47140884399414, \t Total Dis Loss : 0.0002068207977572456\n",
      "Steps : 130200, \t Total Gen Loss : 32.396968841552734, \t Total Dis Loss : 0.00028215895872563124\n",
      "Steps : 130300, \t Total Gen Loss : 31.83030128479004, \t Total Dis Loss : 0.0009908127831295133\n",
      "Steps : 130400, \t Total Gen Loss : 30.024364471435547, \t Total Dis Loss : 0.00020718888845294714\n",
      "Steps : 130500, \t Total Gen Loss : 32.537254333496094, \t Total Dis Loss : 0.00044821208575740457\n",
      "Steps : 130600, \t Total Gen Loss : 32.856422424316406, \t Total Dis Loss : 0.00037292257184162736\n",
      "Steps : 130700, \t Total Gen Loss : 31.3322696685791, \t Total Dis Loss : 0.0011997109977528453\n",
      "Steps : 130800, \t Total Gen Loss : 30.792247772216797, \t Total Dis Loss : 0.000826433883048594\n",
      "Steps : 130900, \t Total Gen Loss : 29.663211822509766, \t Total Dis Loss : 0.0007537407800555229\n",
      "Steps : 131000, \t Total Gen Loss : 32.71332931518555, \t Total Dis Loss : 0.0003581879136618227\n",
      "Steps : 131100, \t Total Gen Loss : 31.220722198486328, \t Total Dis Loss : 0.0002626466448418796\n",
      "Steps : 131200, \t Total Gen Loss : 31.265459060668945, \t Total Dis Loss : 0.00012533743574749678\n",
      "Steps : 131300, \t Total Gen Loss : 32.24665069580078, \t Total Dis Loss : 0.00012203121150378138\n",
      "Steps : 131400, \t Total Gen Loss : 31.20320701599121, \t Total Dis Loss : 0.0004488825798034668\n",
      "Steps : 131500, \t Total Gen Loss : 31.756668090820312, \t Total Dis Loss : 0.00174919911660254\n",
      "Steps : 131600, \t Total Gen Loss : 32.844520568847656, \t Total Dis Loss : 0.00032384326914325356\n",
      "Steps : 131700, \t Total Gen Loss : 29.76968002319336, \t Total Dis Loss : 0.0012133934069424868\n",
      "Steps : 131800, \t Total Gen Loss : 32.69194030761719, \t Total Dis Loss : 0.0004863847861997783\n",
      "Steps : 131900, \t Total Gen Loss : 31.63823127746582, \t Total Dis Loss : 0.00018884407472796738\n",
      "Steps : 132000, \t Total Gen Loss : 30.325029373168945, \t Total Dis Loss : 0.00042784420656971633\n",
      "Steps : 132100, \t Total Gen Loss : 31.70644187927246, \t Total Dis Loss : 0.000665119499899447\n",
      "Steps : 132200, \t Total Gen Loss : 32.883949279785156, \t Total Dis Loss : 0.0003188862756360322\n",
      "Steps : 132300, \t Total Gen Loss : 34.04692077636719, \t Total Dis Loss : 0.0001530526060378179\n",
      "Steps : 132400, \t Total Gen Loss : 30.573955535888672, \t Total Dis Loss : 0.0002563337911851704\n",
      "Steps : 132500, \t Total Gen Loss : 32.078006744384766, \t Total Dis Loss : 0.00019215120119042695\n",
      "Steps : 132600, \t Total Gen Loss : 30.568073272705078, \t Total Dis Loss : 0.0005360936047509313\n",
      "Steps : 132700, \t Total Gen Loss : 33.92547607421875, \t Total Dis Loss : 0.00016056459571700543\n",
      "Steps : 132800, \t Total Gen Loss : 34.713661193847656, \t Total Dis Loss : 0.00022719208209309727\n",
      "Steps : 132900, \t Total Gen Loss : 35.01665496826172, \t Total Dis Loss : 0.0002259665634483099\n",
      "Steps : 133000, \t Total Gen Loss : 33.2578010559082, \t Total Dis Loss : 0.00011626045306911692\n",
      "Steps : 133100, \t Total Gen Loss : 34.98334884643555, \t Total Dis Loss : 9.653250162955374e-05\n",
      "Steps : 133200, \t Total Gen Loss : 30.5260066986084, \t Total Dis Loss : 8.882494876161218e-05\n",
      "Steps : 133300, \t Total Gen Loss : 31.03233528137207, \t Total Dis Loss : 0.000132537359604612\n",
      "Steps : 133400, \t Total Gen Loss : 33.837432861328125, \t Total Dis Loss : 0.0003879099967889488\n",
      "Steps : 133500, \t Total Gen Loss : 31.406509399414062, \t Total Dis Loss : 0.0003267481515649706\n",
      "Steps : 133600, \t Total Gen Loss : 31.768211364746094, \t Total Dis Loss : 0.00023112219059839845\n",
      "Steps : 133700, \t Total Gen Loss : 31.526456832885742, \t Total Dis Loss : 0.00019003449415322393\n",
      "Steps : 133800, \t Total Gen Loss : 34.50465774536133, \t Total Dis Loss : 0.0003014457179233432\n",
      "Steps : 133900, \t Total Gen Loss : 34.67506790161133, \t Total Dis Loss : 9.570778638590127e-05\n",
      "Steps : 134000, \t Total Gen Loss : 31.050626754760742, \t Total Dis Loss : 0.00013543234672397375\n",
      "Steps : 134100, \t Total Gen Loss : 32.414451599121094, \t Total Dis Loss : 0.00011427717981860042\n",
      "Steps : 134200, \t Total Gen Loss : 33.26982498168945, \t Total Dis Loss : 0.0001580654934514314\n",
      "Steps : 134300, \t Total Gen Loss : 32.748348236083984, \t Total Dis Loss : 9.363776189275086e-05\n",
      "Steps : 134400, \t Total Gen Loss : 33.05039978027344, \t Total Dis Loss : 0.00013946404214948416\n",
      "Steps : 134500, \t Total Gen Loss : 35.26496887207031, \t Total Dis Loss : 6.0516700614243746e-05\n",
      "Steps : 134600, \t Total Gen Loss : 33.81578826904297, \t Total Dis Loss : 0.00013650346954818815\n",
      "Steps : 134700, \t Total Gen Loss : 33.70032501220703, \t Total Dis Loss : 5.2890409278916195e-05\n",
      "Steps : 134800, \t Total Gen Loss : 31.219213485717773, \t Total Dis Loss : 0.0008702786872163415\n",
      "Steps : 134900, \t Total Gen Loss : 32.779449462890625, \t Total Dis Loss : 0.00016111115110106766\n",
      "Steps : 135000, \t Total Gen Loss : 33.27351760864258, \t Total Dis Loss : 6.819030386395752e-05\n",
      "Time for epoch 20 is 305.190310716629 sec\n",
      "Steps : 135100, \t Total Gen Loss : 32.636634826660156, \t Total Dis Loss : 0.0003265590639784932\n",
      "Steps : 135200, \t Total Gen Loss : 32.02280807495117, \t Total Dis Loss : 0.00019630523456726223\n",
      "Steps : 135300, \t Total Gen Loss : 29.191722869873047, \t Total Dis Loss : 0.0005472730263136327\n",
      "Steps : 135400, \t Total Gen Loss : 29.80770492553711, \t Total Dis Loss : 0.0006655628676526248\n",
      "Steps : 135500, \t Total Gen Loss : 33.41218566894531, \t Total Dis Loss : 0.00021143004414625466\n",
      "Steps : 135600, \t Total Gen Loss : 36.91127395629883, \t Total Dis Loss : 0.00071075523737818\n",
      "Steps : 135700, \t Total Gen Loss : 34.203369140625, \t Total Dis Loss : 0.0003559420583769679\n",
      "Steps : 135800, \t Total Gen Loss : 35.07350540161133, \t Total Dis Loss : 0.0001023733348120004\n",
      "Steps : 135900, \t Total Gen Loss : 33.11674118041992, \t Total Dis Loss : 0.00018104666378349066\n",
      "Steps : 136000, \t Total Gen Loss : 31.335865020751953, \t Total Dis Loss : 7.833717972971499e-05\n",
      "Steps : 136100, \t Total Gen Loss : 31.9290771484375, \t Total Dis Loss : 0.0001937236520461738\n",
      "Steps : 136200, \t Total Gen Loss : 30.475753784179688, \t Total Dis Loss : 0.00037024327320978045\n",
      "Steps : 136300, \t Total Gen Loss : 31.974180221557617, \t Total Dis Loss : 0.00011284410720691085\n",
      "Steps : 136400, \t Total Gen Loss : 34.46564865112305, \t Total Dis Loss : 0.000153992761624977\n",
      "Steps : 136500, \t Total Gen Loss : 34.46968078613281, \t Total Dis Loss : 2.752353248069994e-05\n",
      "Steps : 136600, \t Total Gen Loss : 33.86146926879883, \t Total Dis Loss : 4.701911529991776e-05\n",
      "Steps : 136700, \t Total Gen Loss : 29.594350814819336, \t Total Dis Loss : 0.0018650081474334002\n",
      "Steps : 136800, \t Total Gen Loss : 31.18045425415039, \t Total Dis Loss : 0.00022315375099424273\n",
      "Steps : 136900, \t Total Gen Loss : 31.774459838867188, \t Total Dis Loss : 0.00013384022167883813\n",
      "Steps : 137000, \t Total Gen Loss : 31.66678237915039, \t Total Dis Loss : 0.00047909194836393\n",
      "Steps : 137100, \t Total Gen Loss : 32.26339340209961, \t Total Dis Loss : 0.0002903168206103146\n",
      "Steps : 137200, \t Total Gen Loss : 30.02264976501465, \t Total Dis Loss : 0.0008855120977386832\n",
      "Steps : 137300, \t Total Gen Loss : 30.13419532775879, \t Total Dis Loss : 0.0004254894738551229\n",
      "Steps : 137400, \t Total Gen Loss : 31.80003547668457, \t Total Dis Loss : 0.0003021530283149332\n",
      "Steps : 137500, \t Total Gen Loss : 32.97036361694336, \t Total Dis Loss : 0.00043980294140055776\n",
      "Steps : 137600, \t Total Gen Loss : 30.004695892333984, \t Total Dis Loss : 0.0026383448857814074\n",
      "Steps : 137700, \t Total Gen Loss : 29.81964111328125, \t Total Dis Loss : 0.0007470913697034121\n",
      "Steps : 137800, \t Total Gen Loss : 31.26181411743164, \t Total Dis Loss : 0.00023906235583126545\n",
      "Steps : 137900, \t Total Gen Loss : 31.322782516479492, \t Total Dis Loss : 0.00022828474175184965\n",
      "Steps : 138000, \t Total Gen Loss : 31.215469360351562, \t Total Dis Loss : 0.0011746896198019385\n",
      "Steps : 138100, \t Total Gen Loss : 33.250858306884766, \t Total Dis Loss : 0.0014933778438717127\n",
      "Steps : 138200, \t Total Gen Loss : 32.935096740722656, \t Total Dis Loss : 0.0004106760025024414\n",
      "Steps : 138300, \t Total Gen Loss : 33.756195068359375, \t Total Dis Loss : 0.00033155063283629715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 138400, \t Total Gen Loss : 30.698795318603516, \t Total Dis Loss : 0.0004897941253148019\n",
      "Steps : 138500, \t Total Gen Loss : 30.688627243041992, \t Total Dis Loss : 0.00029595737578347325\n",
      "Steps : 138600, \t Total Gen Loss : 31.451271057128906, \t Total Dis Loss : 0.0010163390543311834\n",
      "Steps : 138700, \t Total Gen Loss : 30.81139373779297, \t Total Dis Loss : 0.0005786871770396829\n",
      "Steps : 138800, \t Total Gen Loss : 29.26211166381836, \t Total Dis Loss : 0.0006051298696547747\n",
      "Steps : 138900, \t Total Gen Loss : 32.12343978881836, \t Total Dis Loss : 0.00022039873874746263\n",
      "Steps : 139000, \t Total Gen Loss : 31.466060638427734, \t Total Dis Loss : 0.00013583438703790307\n",
      "Steps : 139100, \t Total Gen Loss : 32.96074295043945, \t Total Dis Loss : 0.00011842078674817458\n",
      "Steps : 139200, \t Total Gen Loss : 33.43058776855469, \t Total Dis Loss : 0.0002140262076864019\n",
      "Steps : 139300, \t Total Gen Loss : 31.05634307861328, \t Total Dis Loss : 0.00015589642862323672\n",
      "Steps : 139400, \t Total Gen Loss : 32.829124450683594, \t Total Dis Loss : 0.00016370796947740018\n",
      "Steps : 139500, \t Total Gen Loss : 31.736021041870117, \t Total Dis Loss : 0.00032205163734033704\n",
      "Steps : 139600, \t Total Gen Loss : 32.90049362182617, \t Total Dis Loss : 0.00015867115871515125\n",
      "Steps : 139700, \t Total Gen Loss : 33.19194030761719, \t Total Dis Loss : 0.00038599633262492716\n",
      "Steps : 139800, \t Total Gen Loss : 32.26651382446289, \t Total Dis Loss : 0.00014566973550245166\n",
      "Steps : 139900, \t Total Gen Loss : 29.41231346130371, \t Total Dis Loss : 0.2757616639137268\n",
      "Steps : 140000, \t Total Gen Loss : 34.118446350097656, \t Total Dis Loss : 0.0005625229678116739\n",
      "Steps : 140100, \t Total Gen Loss : 31.21940040588379, \t Total Dis Loss : 0.00044172522029839456\n",
      "Steps : 140200, \t Total Gen Loss : 34.421470642089844, \t Total Dis Loss : 0.00027856166707351804\n",
      "Steps : 140300, \t Total Gen Loss : 30.621658325195312, \t Total Dis Loss : 0.0006989066023379564\n",
      "Steps : 140400, \t Total Gen Loss : 33.108436584472656, \t Total Dis Loss : 0.0001098784850910306\n",
      "Steps : 140500, \t Total Gen Loss : 30.76091194152832, \t Total Dis Loss : 0.00014002458192408085\n",
      "Steps : 140600, \t Total Gen Loss : 33.039485931396484, \t Total Dis Loss : 0.00020573682559188455\n",
      "Steps : 140700, \t Total Gen Loss : 30.90285301208496, \t Total Dis Loss : 0.0004129481967538595\n",
      "Steps : 140800, \t Total Gen Loss : 31.19194793701172, \t Total Dis Loss : 0.00029144546715542674\n",
      "Steps : 140900, \t Total Gen Loss : 32.33141326904297, \t Total Dis Loss : 0.0003496816789265722\n",
      "Steps : 141000, \t Total Gen Loss : 29.936552047729492, \t Total Dis Loss : 0.0003708166186697781\n",
      "Steps : 141100, \t Total Gen Loss : 32.549163818359375, \t Total Dis Loss : 0.00020849356951657683\n",
      "Steps : 141200, \t Total Gen Loss : 31.378280639648438, \t Total Dis Loss : 0.00016824367048684508\n",
      "Steps : 141300, \t Total Gen Loss : 31.225448608398438, \t Total Dis Loss : 0.0001855269365478307\n",
      "Steps : 141400, \t Total Gen Loss : 35.720558166503906, \t Total Dis Loss : 0.0001301637094002217\n",
      "Steps : 141500, \t Total Gen Loss : 34.18920135498047, \t Total Dis Loss : 0.00018975292914547026\n",
      "Steps : 141600, \t Total Gen Loss : 31.242290496826172, \t Total Dis Loss : 0.000573255936615169\n",
      "Steps : 141700, \t Total Gen Loss : 32.23595428466797, \t Total Dis Loss : 0.0002777189074549824\n",
      "Time for epoch 21 is 307.1453893184662 sec\n",
      "Steps : 141800, \t Total Gen Loss : 33.40118408203125, \t Total Dis Loss : 0.00019759885617531836\n",
      "Steps : 141900, \t Total Gen Loss : 34.87769317626953, \t Total Dis Loss : 0.00015005945169832557\n",
      "Steps : 142000, \t Total Gen Loss : 32.94593811035156, \t Total Dis Loss : 0.0001707165502011776\n",
      "Steps : 142100, \t Total Gen Loss : 32.60447311401367, \t Total Dis Loss : 0.00021088027278892696\n",
      "Steps : 142200, \t Total Gen Loss : 34.506187438964844, \t Total Dis Loss : 0.00012837797112297267\n",
      "Steps : 142300, \t Total Gen Loss : 33.50624084472656, \t Total Dis Loss : 0.0001090324149117805\n",
      "Steps : 142400, \t Total Gen Loss : 33.11772537231445, \t Total Dis Loss : 0.0011600640136748552\n",
      "Steps : 142500, \t Total Gen Loss : 32.56376647949219, \t Total Dis Loss : 0.0001669908524490893\n",
      "Steps : 142600, \t Total Gen Loss : 34.94490432739258, \t Total Dis Loss : 0.0001383462076773867\n",
      "Steps : 142700, \t Total Gen Loss : 32.729286193847656, \t Total Dis Loss : 0.00011433001782279462\n",
      "Steps : 142800, \t Total Gen Loss : 33.059600830078125, \t Total Dis Loss : 8.904274727683514e-05\n",
      "Steps : 142900, \t Total Gen Loss : 34.36837387084961, \t Total Dis Loss : 7.55637011025101e-05\n",
      "Steps : 143000, \t Total Gen Loss : 33.63941192626953, \t Total Dis Loss : 0.00011778478801716119\n",
      "Steps : 143100, \t Total Gen Loss : 31.201412200927734, \t Total Dis Loss : 0.0007459170883521438\n",
      "Steps : 143200, \t Total Gen Loss : 33.304779052734375, \t Total Dis Loss : 0.0002513619838282466\n",
      "Steps : 143300, \t Total Gen Loss : 32.05288314819336, \t Total Dis Loss : 0.00016276224050670862\n",
      "Steps : 143400, \t Total Gen Loss : 32.08433151245117, \t Total Dis Loss : 0.000549925840459764\n",
      "Steps : 143500, \t Total Gen Loss : 31.204113006591797, \t Total Dis Loss : 0.000375322881154716\n",
      "Steps : 143600, \t Total Gen Loss : 31.594942092895508, \t Total Dis Loss : 0.00018046221521217376\n",
      "Steps : 143700, \t Total Gen Loss : 34.116607666015625, \t Total Dis Loss : 8.774529851507396e-05\n",
      "Steps : 143800, \t Total Gen Loss : 32.17966842651367, \t Total Dis Loss : 0.0004683062725234777\n",
      "Steps : 143900, \t Total Gen Loss : 31.44670295715332, \t Total Dis Loss : 0.00023269747907761484\n",
      "Steps : 144000, \t Total Gen Loss : 34.182926177978516, \t Total Dis Loss : 8.035406062845141e-05\n",
      "Steps : 144100, \t Total Gen Loss : 31.876699447631836, \t Total Dis Loss : 0.0008126666070893407\n",
      "Steps : 144200, \t Total Gen Loss : 29.95752716064453, \t Total Dis Loss : 0.0004418463504407555\n",
      "Steps : 144300, \t Total Gen Loss : 30.381105422973633, \t Total Dis Loss : 0.00023844775569159538\n",
      "Steps : 144400, \t Total Gen Loss : 32.70659637451172, \t Total Dis Loss : 0.00023486225109081715\n",
      "Steps : 144500, \t Total Gen Loss : 33.29021072387695, \t Total Dis Loss : 0.0012478973949328065\n",
      "Steps : 144600, \t Total Gen Loss : 32.836334228515625, \t Total Dis Loss : 0.0002412565954728052\n",
      "Steps : 144700, \t Total Gen Loss : 34.33749771118164, \t Total Dis Loss : 0.0004859493637923151\n",
      "Steps : 144800, \t Total Gen Loss : 34.33821487426758, \t Total Dis Loss : 0.00044887690455652773\n",
      "Steps : 144900, \t Total Gen Loss : 32.17750549316406, \t Total Dis Loss : 0.00023709278320893645\n",
      "Steps : 145000, \t Total Gen Loss : 30.165756225585938, \t Total Dis Loss : 0.0008704177453182638\n",
      "Steps : 145100, \t Total Gen Loss : 33.28429412841797, \t Total Dis Loss : 0.00039786967681720853\n",
      "Steps : 145200, \t Total Gen Loss : 32.268402099609375, \t Total Dis Loss : 0.00033684351365081966\n",
      "Steps : 145300, \t Total Gen Loss : 30.855581283569336, \t Total Dis Loss : 0.00015800056280568242\n",
      "Steps : 145400, \t Total Gen Loss : 32.77854537963867, \t Total Dis Loss : 0.0001242521102540195\n",
      "Steps : 145500, \t Total Gen Loss : 31.31856918334961, \t Total Dis Loss : 0.00013038587349001318\n",
      "Steps : 145600, \t Total Gen Loss : 32.55647277832031, \t Total Dis Loss : 0.0001021785574266687\n",
      "Steps : 145700, \t Total Gen Loss : 32.843658447265625, \t Total Dis Loss : 0.00011080994590884075\n",
      "Steps : 145800, \t Total Gen Loss : 33.41917037963867, \t Total Dis Loss : 0.00010274379746988416\n",
      "Steps : 145900, \t Total Gen Loss : 33.45691680908203, \t Total Dis Loss : 5.5643879022682086e-05\n",
      "Steps : 146000, \t Total Gen Loss : 28.722888946533203, \t Total Dis Loss : 0.01651313342154026\n",
      "Steps : 146100, \t Total Gen Loss : 27.554433822631836, \t Total Dis Loss : 0.0029542907141149044\n",
      "Steps : 146200, \t Total Gen Loss : 28.03155517578125, \t Total Dis Loss : 0.0008565130410715938\n",
      "Steps : 146300, \t Total Gen Loss : 32.286441802978516, \t Total Dis Loss : 0.000435783585999161\n",
      "Steps : 146400, \t Total Gen Loss : 28.491674423217773, \t Total Dis Loss : 0.00041609659092500806\n",
      "Steps : 146500, \t Total Gen Loss : 34.47505187988281, \t Total Dis Loss : 5.096416498417966e-05\n",
      "Steps : 146600, \t Total Gen Loss : 31.41329574584961, \t Total Dis Loss : 0.0007322748424485326\n",
      "Steps : 146700, \t Total Gen Loss : 30.648210525512695, \t Total Dis Loss : 0.0005202017491683364\n",
      "Steps : 146800, \t Total Gen Loss : 33.06869888305664, \t Total Dis Loss : 0.00017328510875813663\n",
      "Steps : 146900, \t Total Gen Loss : 32.593441009521484, \t Total Dis Loss : 0.00022894878929946572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 147000, \t Total Gen Loss : 32.510257720947266, \t Total Dis Loss : 0.00012668817362282425\n",
      "Steps : 147100, \t Total Gen Loss : 31.278274536132812, \t Total Dis Loss : 0.0001038782938849181\n",
      "Steps : 147200, \t Total Gen Loss : 31.202791213989258, \t Total Dis Loss : 7.948315760586411e-05\n",
      "Steps : 147300, \t Total Gen Loss : 28.08773422241211, \t Total Dis Loss : 0.0006274074548855424\n",
      "Steps : 147400, \t Total Gen Loss : 30.724559783935547, \t Total Dis Loss : 0.0006360343541018665\n",
      "Steps : 147500, \t Total Gen Loss : 32.59265899658203, \t Total Dis Loss : 0.0002487027086317539\n",
      "Steps : 147600, \t Total Gen Loss : 32.36465072631836, \t Total Dis Loss : 0.0001381582987960428\n",
      "Steps : 147700, \t Total Gen Loss : 31.207548141479492, \t Total Dis Loss : 0.0002976940886583179\n",
      "Steps : 147800, \t Total Gen Loss : 31.328929901123047, \t Total Dis Loss : 0.00011928902677027509\n",
      "Steps : 147900, \t Total Gen Loss : 32.050758361816406, \t Total Dis Loss : 0.00015037640696391463\n",
      "Steps : 148000, \t Total Gen Loss : 30.949840545654297, \t Total Dis Loss : 0.00024859249242581427\n",
      "Steps : 148100, \t Total Gen Loss : 30.874225616455078, \t Total Dis Loss : 0.0001448014663765207\n",
      "Steps : 148200, \t Total Gen Loss : 30.712644577026367, \t Total Dis Loss : 0.00014868010475765914\n",
      "Steps : 148300, \t Total Gen Loss : 31.589677810668945, \t Total Dis Loss : 0.00024610746186226606\n",
      "Steps : 148400, \t Total Gen Loss : 32.29317855834961, \t Total Dis Loss : 0.00012583183706738055\n",
      "Steps : 148500, \t Total Gen Loss : 30.262531280517578, \t Total Dis Loss : 0.03987912833690643\n",
      "Time for epoch 22 is 302.641747713089 sec\n",
      "Steps : 148600, \t Total Gen Loss : 29.988391876220703, \t Total Dis Loss : 0.0001240889250766486\n",
      "Steps : 148700, \t Total Gen Loss : 32.07112121582031, \t Total Dis Loss : 8.148016786435619e-05\n",
      "Steps : 148800, \t Total Gen Loss : 30.72016143798828, \t Total Dis Loss : 7.720904250163585e-05\n",
      "Steps : 148900, \t Total Gen Loss : 31.968875885009766, \t Total Dis Loss : 0.00017766692326404154\n",
      "Steps : 149000, \t Total Gen Loss : 32.841888427734375, \t Total Dis Loss : 3.772001582547091e-05\n",
      "Steps : 149100, \t Total Gen Loss : 30.42413330078125, \t Total Dis Loss : 7.229445327538997e-05\n",
      "Steps : 149200, \t Total Gen Loss : 30.67437171936035, \t Total Dis Loss : 5.411387246567756e-05\n",
      "Steps : 149300, \t Total Gen Loss : 31.953678131103516, \t Total Dis Loss : 3.8379534089472145e-05\n",
      "Steps : 149400, \t Total Gen Loss : 31.871191024780273, \t Total Dis Loss : 4.147870640736073e-05\n",
      "Steps : 149500, \t Total Gen Loss : 30.995494842529297, \t Total Dis Loss : 4.181604890618473e-05\n",
      "Steps : 149600, \t Total Gen Loss : 31.56507110595703, \t Total Dis Loss : 0.00027937133563682437\n",
      "Steps : 149700, \t Total Gen Loss : 32.87082290649414, \t Total Dis Loss : 2.587284143373836e-05\n",
      "Steps : 149800, \t Total Gen Loss : 30.49989128112793, \t Total Dis Loss : 4.957529381499626e-05\n",
      "Steps : 149900, \t Total Gen Loss : 29.91754913330078, \t Total Dis Loss : 0.00036052995710633695\n",
      "Steps : 150000, \t Total Gen Loss : 28.257720947265625, \t Total Dis Loss : 0.00031322293216362596\n",
      "Steps : 150100, \t Total Gen Loss : 29.748247146606445, \t Total Dis Loss : 0.00020317947200965136\n",
      "Steps : 150200, \t Total Gen Loss : 31.56873893737793, \t Total Dis Loss : 4.316342528909445e-05\n",
      "Steps : 150300, \t Total Gen Loss : 29.396629333496094, \t Total Dis Loss : 6.843253504484892e-05\n",
      "Steps : 150400, \t Total Gen Loss : 31.793115615844727, \t Total Dis Loss : 3.4910997783299536e-05\n",
      "Steps : 150500, \t Total Gen Loss : 28.720523834228516, \t Total Dis Loss : 0.00045198516454547644\n",
      "Steps : 150600, \t Total Gen Loss : 29.714675903320312, \t Total Dis Loss : 0.00015702527889516205\n",
      "Steps : 150700, \t Total Gen Loss : 30.886619567871094, \t Total Dis Loss : 0.00014493244816549122\n",
      "Steps : 150800, \t Total Gen Loss : 30.67428970336914, \t Total Dis Loss : 0.00011501924018375576\n",
      "Steps : 150900, \t Total Gen Loss : 30.3950252532959, \t Total Dis Loss : 0.00020178848353680223\n",
      "Steps : 151000, \t Total Gen Loss : 30.895450592041016, \t Total Dis Loss : 0.00010851093247765675\n",
      "Steps : 151100, \t Total Gen Loss : 30.336225509643555, \t Total Dis Loss : 0.0004335731209721416\n",
      "Steps : 151200, \t Total Gen Loss : 28.803136825561523, \t Total Dis Loss : 0.00037308657192625105\n",
      "Steps : 151300, \t Total Gen Loss : 29.65304946899414, \t Total Dis Loss : 0.00021760094386991113\n",
      "Steps : 151400, \t Total Gen Loss : 30.309423446655273, \t Total Dis Loss : 0.00021419064432848245\n",
      "Steps : 151500, \t Total Gen Loss : 30.540369033813477, \t Total Dis Loss : 0.00010820199531735852\n",
      "Steps : 151600, \t Total Gen Loss : 29.189285278320312, \t Total Dis Loss : 7.044081576168537e-05\n",
      "Steps : 151700, \t Total Gen Loss : 30.727964401245117, \t Total Dis Loss : 4.712417648988776e-05\n",
      "Steps : 151800, \t Total Gen Loss : 31.493104934692383, \t Total Dis Loss : 6.00904859311413e-05\n",
      "Steps : 151900, \t Total Gen Loss : 31.04572296142578, \t Total Dis Loss : 5.4372176236938685e-05\n",
      "Steps : 152000, \t Total Gen Loss : 28.837139129638672, \t Total Dis Loss : 0.0014474873896688223\n",
      "Steps : 152100, \t Total Gen Loss : 29.342395782470703, \t Total Dis Loss : 0.001070806640200317\n",
      "Steps : 152200, \t Total Gen Loss : 29.18997573852539, \t Total Dis Loss : 0.0002681055339053273\n",
      "Steps : 152300, \t Total Gen Loss : 31.86861801147461, \t Total Dis Loss : 8.701543265487999e-05\n",
      "Steps : 152400, \t Total Gen Loss : 29.448699951171875, \t Total Dis Loss : 0.00012152652197983116\n",
      "Steps : 152500, \t Total Gen Loss : 30.574430465698242, \t Total Dis Loss : 0.0002199601149186492\n",
      "Steps : 152600, \t Total Gen Loss : 30.973176956176758, \t Total Dis Loss : 3.373837171238847e-05\n",
      "Steps : 152700, \t Total Gen Loss : 32.59280776977539, \t Total Dis Loss : 8.043501293286681e-05\n",
      "Steps : 152800, \t Total Gen Loss : 30.714784622192383, \t Total Dis Loss : 8.162602898664773e-05\n",
      "Steps : 152900, \t Total Gen Loss : 32.2796630859375, \t Total Dis Loss : 8.953541691880673e-05\n",
      "Steps : 153000, \t Total Gen Loss : 30.769142150878906, \t Total Dis Loss : 7.113334140740335e-05\n",
      "Steps : 153100, \t Total Gen Loss : 30.838518142700195, \t Total Dis Loss : 0.00018719916988629848\n",
      "Steps : 153200, \t Total Gen Loss : 30.604475021362305, \t Total Dis Loss : 0.00011439940135460347\n",
      "Steps : 153300, \t Total Gen Loss : 31.83708381652832, \t Total Dis Loss : 9.300622332375497e-05\n",
      "Steps : 153400, \t Total Gen Loss : 30.84548568725586, \t Total Dis Loss : 7.877157622715458e-05\n",
      "Steps : 153500, \t Total Gen Loss : 32.16861343383789, \t Total Dis Loss : 5.44891445315443e-05\n",
      "Steps : 153600, \t Total Gen Loss : 31.427698135375977, \t Total Dis Loss : 9.33875489863567e-05\n",
      "Steps : 153700, \t Total Gen Loss : 29.868314743041992, \t Total Dis Loss : 0.0005275007570162416\n",
      "Steps : 153800, \t Total Gen Loss : 32.16604995727539, \t Total Dis Loss : 4.822694245376624e-05\n",
      "Steps : 153900, \t Total Gen Loss : 32.33415603637695, \t Total Dis Loss : 7.117120549082756e-05\n",
      "Steps : 154000, \t Total Gen Loss : 28.685688018798828, \t Total Dis Loss : 0.00019946547399740666\n",
      "Steps : 154100, \t Total Gen Loss : 30.686725616455078, \t Total Dis Loss : 0.00012182045611552894\n",
      "Steps : 154200, \t Total Gen Loss : 31.942893981933594, \t Total Dis Loss : 4.741763405036181e-05\n",
      "Steps : 154300, \t Total Gen Loss : 31.680522918701172, \t Total Dis Loss : 5.6768865761114284e-05\n",
      "Steps : 154400, \t Total Gen Loss : 31.194412231445312, \t Total Dis Loss : 5.2175833843648434e-05\n",
      "Steps : 154500, \t Total Gen Loss : 30.066993713378906, \t Total Dis Loss : 8.96246638149023e-05\n",
      "Steps : 154600, \t Total Gen Loss : 28.965484619140625, \t Total Dis Loss : 0.00015801642439328134\n",
      "Steps : 154700, \t Total Gen Loss : 28.373910903930664, \t Total Dis Loss : 0.00018162683409173042\n",
      "Steps : 154800, \t Total Gen Loss : 30.647781372070312, \t Total Dis Loss : 0.00022385844204109162\n",
      "Steps : 154900, \t Total Gen Loss : 28.234416961669922, \t Total Dis Loss : 0.00044960895320400596\n",
      "Steps : 155000, \t Total Gen Loss : 29.093576431274414, \t Total Dis Loss : 0.000368642300600186\n",
      "Steps : 155100, \t Total Gen Loss : 29.100662231445312, \t Total Dis Loss : 6.502315955003724e-05\n",
      "Steps : 155200, \t Total Gen Loss : 28.469125747680664, \t Total Dis Loss : 7.218047539936379e-05\n",
      "Time for epoch 23 is 304.8957738876343 sec\n",
      "Steps : 155300, \t Total Gen Loss : 30.336645126342773, \t Total Dis Loss : 8.630842785350978e-05\n",
      "Steps : 155400, \t Total Gen Loss : 31.452905654907227, \t Total Dis Loss : 3.203502274118364e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 155500, \t Total Gen Loss : 29.04053497314453, \t Total Dis Loss : 0.000658885866869241\n",
      "Steps : 155600, \t Total Gen Loss : 30.686452865600586, \t Total Dis Loss : 0.0001689411437837407\n",
      "Steps : 155700, \t Total Gen Loss : 30.793701171875, \t Total Dis Loss : 9.794437210075557e-05\n",
      "Steps : 155800, \t Total Gen Loss : 31.19927406311035, \t Total Dis Loss : 8.997915574582294e-05\n",
      "Steps : 155900, \t Total Gen Loss : 29.987136840820312, \t Total Dis Loss : 9.121338371187449e-05\n",
      "Steps : 156000, \t Total Gen Loss : 32.56650161743164, \t Total Dis Loss : 2.3048982257023454e-05\n",
      "Steps : 156100, \t Total Gen Loss : 31.627002716064453, \t Total Dis Loss : 0.0001306449994444847\n",
      "Steps : 156200, \t Total Gen Loss : 28.520153045654297, \t Total Dis Loss : 0.0005527733010239899\n",
      "Steps : 156300, \t Total Gen Loss : 31.646953582763672, \t Total Dis Loss : 0.0001601692201802507\n",
      "Steps : 156400, \t Total Gen Loss : 29.4268798828125, \t Total Dis Loss : 0.00023454776965081692\n",
      "Steps : 156500, \t Total Gen Loss : 30.75141716003418, \t Total Dis Loss : 9.170902922051027e-05\n",
      "Steps : 156600, \t Total Gen Loss : 32.15272521972656, \t Total Dis Loss : 0.00012321522808633745\n",
      "Steps : 156700, \t Total Gen Loss : 31.71797752380371, \t Total Dis Loss : 5.0617883971426636e-05\n",
      "Steps : 156800, \t Total Gen Loss : 30.333423614501953, \t Total Dis Loss : 0.00013870061957277358\n",
      "Steps : 156900, \t Total Gen Loss : 30.02838897705078, \t Total Dis Loss : 7.089959399309009e-05\n",
      "Steps : 157000, \t Total Gen Loss : 33.28824234008789, \t Total Dis Loss : 4.4187705498188734e-05\n",
      "Steps : 157100, \t Total Gen Loss : 32.197689056396484, \t Total Dis Loss : 4.1513314499752596e-05\n",
      "Steps : 157200, \t Total Gen Loss : 32.83343505859375, \t Total Dis Loss : 1.498709934821818e-05\n",
      "Steps : 157300, \t Total Gen Loss : 33.67473220825195, \t Total Dis Loss : 1.2222814802953508e-05\n",
      "Steps : 157400, \t Total Gen Loss : 33.157657623291016, \t Total Dis Loss : 3.0835326469969004e-05\n",
      "Steps : 157500, \t Total Gen Loss : 32.467369079589844, \t Total Dis Loss : 7.515624020015821e-05\n",
      "Steps : 157600, \t Total Gen Loss : 30.87397003173828, \t Total Dis Loss : 8.61085718497634e-05\n",
      "Steps : 157700, \t Total Gen Loss : 31.13888931274414, \t Total Dis Loss : 6.117152224760503e-05\n",
      "Steps : 157800, \t Total Gen Loss : 31.51639175415039, \t Total Dis Loss : 9.265189146390185e-05\n",
      "Steps : 157900, \t Total Gen Loss : 32.08208465576172, \t Total Dis Loss : 0.00012062922905897722\n",
      "Steps : 158000, \t Total Gen Loss : 27.976661682128906, \t Total Dis Loss : 0.0009793147910386324\n",
      "Steps : 158100, \t Total Gen Loss : 31.568958282470703, \t Total Dis Loss : 8.319071639562026e-05\n",
      "Steps : 158200, \t Total Gen Loss : 29.8909854888916, \t Total Dis Loss : 0.0002491647901479155\n",
      "Steps : 158300, \t Total Gen Loss : 31.35204315185547, \t Total Dis Loss : 0.00012277602218091488\n",
      "Steps : 158400, \t Total Gen Loss : 29.616043090820312, \t Total Dis Loss : 0.00034315933589823544\n",
      "Steps : 158500, \t Total Gen Loss : 28.013643264770508, \t Total Dis Loss : 9.349121683044359e-05\n",
      "Steps : 158600, \t Total Gen Loss : 28.63273811340332, \t Total Dis Loss : 0.00037388555938377976\n",
      "Steps : 158700, \t Total Gen Loss : 29.306299209594727, \t Total Dis Loss : 0.0001341217866865918\n",
      "Steps : 158800, \t Total Gen Loss : 30.179813385009766, \t Total Dis Loss : 8.340917702298611e-05\n",
      "Steps : 158900, \t Total Gen Loss : 30.909038543701172, \t Total Dis Loss : 9.3633214419242e-05\n",
      "Steps : 159000, \t Total Gen Loss : 28.937490463256836, \t Total Dis Loss : 0.00021055883553344756\n",
      "Steps : 159100, \t Total Gen Loss : 28.55894660949707, \t Total Dis Loss : 0.0005185664049349725\n",
      "Steps : 159200, \t Total Gen Loss : 31.51083755493164, \t Total Dis Loss : 6.152036803541705e-05\n",
      "Steps : 159300, \t Total Gen Loss : 32.39459228515625, \t Total Dis Loss : 7.387993537122384e-05\n",
      "Steps : 159400, \t Total Gen Loss : 31.996837615966797, \t Total Dis Loss : 0.00013428772217594087\n",
      "Steps : 159500, \t Total Gen Loss : 31.41803550720215, \t Total Dis Loss : 5.407903154264204e-05\n",
      "Steps : 159600, \t Total Gen Loss : 30.274614334106445, \t Total Dis Loss : 0.0001018829716485925\n",
      "Steps : 159700, \t Total Gen Loss : 28.869064331054688, \t Total Dis Loss : 0.0005550914793275297\n",
      "Steps : 159800, \t Total Gen Loss : 28.946815490722656, \t Total Dis Loss : 0.00014882242248859257\n",
      "Steps : 159900, \t Total Gen Loss : 31.148038864135742, \t Total Dis Loss : 0.0001307608763454482\n",
      "Steps : 160000, \t Total Gen Loss : 30.56036949157715, \t Total Dis Loss : 8.99988881428726e-05\n",
      "Steps : 160100, \t Total Gen Loss : 31.223419189453125, \t Total Dis Loss : 0.00010867009405046701\n",
      "Steps : 160200, \t Total Gen Loss : 32.131446838378906, \t Total Dis Loss : 5.79569605179131e-05\n",
      "Steps : 160300, \t Total Gen Loss : 32.4942626953125, \t Total Dis Loss : 6.9649082433898e-05\n",
      "Steps : 160400, \t Total Gen Loss : 31.60577392578125, \t Total Dis Loss : 7.470264245057479e-05\n",
      "Steps : 160500, \t Total Gen Loss : 31.200586318969727, \t Total Dis Loss : 6.441378354793414e-05\n",
      "Steps : 160600, \t Total Gen Loss : 30.138708114624023, \t Total Dis Loss : 0.0005547184846363962\n",
      "Steps : 160700, \t Total Gen Loss : 31.677715301513672, \t Total Dis Loss : 0.00019932359282393008\n",
      "Steps : 160800, \t Total Gen Loss : 29.624238967895508, \t Total Dis Loss : 0.00023781419440638274\n",
      "Steps : 160900, \t Total Gen Loss : 29.431806564331055, \t Total Dis Loss : 5.569382483372465e-05\n",
      "Steps : 161000, \t Total Gen Loss : 32.496315002441406, \t Total Dis Loss : 5.941635026829317e-05\n",
      "Steps : 161100, \t Total Gen Loss : 30.23291015625, \t Total Dis Loss : 5.341784708434716e-05\n",
      "Steps : 161200, \t Total Gen Loss : 32.32712936401367, \t Total Dis Loss : 3.887678394676186e-05\n",
      "Steps : 161300, \t Total Gen Loss : 32.2234992980957, \t Total Dis Loss : 3.7381003494374454e-05\n",
      "Steps : 161400, \t Total Gen Loss : 32.973793029785156, \t Total Dis Loss : 2.694560680538416e-05\n",
      "Steps : 161500, \t Total Gen Loss : 33.723365783691406, \t Total Dis Loss : 2.3790278646629304e-05\n",
      "Steps : 161600, \t Total Gen Loss : 31.016557693481445, \t Total Dis Loss : 7.110148726496845e-05\n",
      "Steps : 161700, \t Total Gen Loss : 31.556724548339844, \t Total Dis Loss : 1.614213215361815e-05\n",
      "Steps : 161800, \t Total Gen Loss : 32.11378479003906, \t Total Dis Loss : 1.650705235078931e-05\n",
      "Steps : 161900, \t Total Gen Loss : 32.10930633544922, \t Total Dis Loss : 5.200103623792529e-05\n",
      "Steps : 162000, \t Total Gen Loss : 32.923500061035156, \t Total Dis Loss : 2.1614589059026912e-05\n",
      "Time for epoch 24 is 305.40483689308167 sec\n",
      "Steps : 162100, \t Total Gen Loss : 31.83991813659668, \t Total Dis Loss : 7.447844836860895e-05\n",
      "Steps : 162200, \t Total Gen Loss : 34.21442794799805, \t Total Dis Loss : 4.3315638322383165e-05\n",
      "Steps : 162300, \t Total Gen Loss : 31.332061767578125, \t Total Dis Loss : 4.172224726062268e-05\n",
      "Steps : 162400, \t Total Gen Loss : 32.1950798034668, \t Total Dis Loss : 2.3823879018891603e-05\n",
      "Steps : 162500, \t Total Gen Loss : 35.619693756103516, \t Total Dis Loss : 0.017239991575479507\n",
      "Steps : 162600, \t Total Gen Loss : 31.681406021118164, \t Total Dis Loss : 4.1561652324162424e-05\n",
      "Steps : 162700, \t Total Gen Loss : 30.7346134185791, \t Total Dis Loss : 0.0016831543762236834\n",
      "Steps : 162800, \t Total Gen Loss : 29.655073165893555, \t Total Dis Loss : 0.0008432313334196806\n",
      "Steps : 162900, \t Total Gen Loss : 31.8381290435791, \t Total Dis Loss : 3.392010330571793e-05\n",
      "Steps : 163000, \t Total Gen Loss : 30.903564453125, \t Total Dis Loss : 0.00014946584997233003\n",
      "Steps : 163100, \t Total Gen Loss : 29.452953338623047, \t Total Dis Loss : 0.0003856669063679874\n",
      "Steps : 163200, \t Total Gen Loss : 30.505252838134766, \t Total Dis Loss : 0.00022840703604742885\n",
      "Steps : 163300, \t Total Gen Loss : 28.92941665649414, \t Total Dis Loss : 0.00016580075316596776\n",
      "Steps : 163400, \t Total Gen Loss : 31.306217193603516, \t Total Dis Loss : 0.00012489626533351839\n",
      "Steps : 163500, \t Total Gen Loss : 29.67081642150879, \t Total Dis Loss : 0.0004639974795281887\n",
      "Steps : 163600, \t Total Gen Loss : 29.72049331665039, \t Total Dis Loss : 0.0002960512356366962\n",
      "Steps : 163700, \t Total Gen Loss : 30.108325958251953, \t Total Dis Loss : 0.00018943691975437105\n",
      "Steps : 163800, \t Total Gen Loss : 30.120899200439453, \t Total Dis Loss : 0.00014805879618506879\n",
      "Steps : 163900, \t Total Gen Loss : 31.057952880859375, \t Total Dis Loss : 7.051637658150867e-05\n",
      "Steps : 164000, \t Total Gen Loss : 29.34342384338379, \t Total Dis Loss : 9.327866428066045e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 164100, \t Total Gen Loss : 32.22942352294922, \t Total Dis Loss : 3.550700057530776e-05\n",
      "Steps : 164200, \t Total Gen Loss : 32.119171142578125, \t Total Dis Loss : 5.995057290419936e-05\n",
      "Steps : 164300, \t Total Gen Loss : 30.30748748779297, \t Total Dis Loss : 6.735348870279267e-05\n",
      "Steps : 164400, \t Total Gen Loss : 30.657669067382812, \t Total Dis Loss : 0.00044273745152167976\n",
      "Steps : 164500, \t Total Gen Loss : 30.671560287475586, \t Total Dis Loss : 0.00023681075253989547\n",
      "Steps : 164600, \t Total Gen Loss : 32.11431884765625, \t Total Dis Loss : 0.00011125521268695593\n",
      "Steps : 164700, \t Total Gen Loss : 29.101436614990234, \t Total Dis Loss : 0.0005813828320242465\n",
      "Steps : 164800, \t Total Gen Loss : 28.69032096862793, \t Total Dis Loss : 0.00094133015954867\n",
      "Steps : 164900, \t Total Gen Loss : 31.27481460571289, \t Total Dis Loss : 0.00014368793927133083\n",
      "Steps : 165000, \t Total Gen Loss : 29.18867301940918, \t Total Dis Loss : 0.00045809868606738746\n",
      "Steps : 165100, \t Total Gen Loss : 29.63304901123047, \t Total Dis Loss : 0.00013134766777511686\n",
      "Steps : 165200, \t Total Gen Loss : 30.652515411376953, \t Total Dis Loss : 0.000328894384438172\n",
      "Steps : 165300, \t Total Gen Loss : 31.19335174560547, \t Total Dis Loss : 0.00018976532737724483\n",
      "Steps : 165400, \t Total Gen Loss : 31.675947189331055, \t Total Dis Loss : 0.00012142580817453563\n",
      "Steps : 165500, \t Total Gen Loss : 31.206762313842773, \t Total Dis Loss : 0.00047723285388201475\n",
      "Steps : 165600, \t Total Gen Loss : 35.558753967285156, \t Total Dis Loss : 5.5926058848854154e-05\n",
      "Steps : 165700, \t Total Gen Loss : 32.167903900146484, \t Total Dis Loss : 6.752018816769123e-05\n",
      "Steps : 165800, \t Total Gen Loss : 30.164464950561523, \t Total Dis Loss : 0.0002916155499406159\n",
      "Steps : 165900, \t Total Gen Loss : 28.476226806640625, \t Total Dis Loss : 0.0003055573906749487\n",
      "Steps : 166000, \t Total Gen Loss : 31.945730209350586, \t Total Dis Loss : 3.929298327420838e-05\n",
      "Steps : 166100, \t Total Gen Loss : 30.24323844909668, \t Total Dis Loss : 0.0011171717196702957\n",
      "Steps : 166200, \t Total Gen Loss : 32.40718460083008, \t Total Dis Loss : 0.0014616738772019744\n",
      "Steps : 166300, \t Total Gen Loss : 34.63670349121094, \t Total Dis Loss : 1.2796540431736503e-05\n",
      "Steps : 166400, \t Total Gen Loss : 31.549762725830078, \t Total Dis Loss : 0.0001287254854105413\n",
      "Steps : 166500, \t Total Gen Loss : 31.252460479736328, \t Total Dis Loss : 0.0002628373331390321\n",
      "Steps : 166600, \t Total Gen Loss : 27.46072006225586, \t Total Dis Loss : 0.0004334381374064833\n",
      "Steps : 166700, \t Total Gen Loss : 28.564908981323242, \t Total Dis Loss : 0.0005824558902531862\n",
      "Steps : 166800, \t Total Gen Loss : 31.179811477661133, \t Total Dis Loss : 0.00015598832396790385\n",
      "Steps : 166900, \t Total Gen Loss : 30.954381942749023, \t Total Dis Loss : 6.708921864628792e-05\n",
      "Steps : 167000, \t Total Gen Loss : 32.281700134277344, \t Total Dis Loss : 0.0019233983475714922\n",
      "Steps : 167100, \t Total Gen Loss : 30.090290069580078, \t Total Dis Loss : 0.00021197920432314277\n",
      "Steps : 167200, \t Total Gen Loss : 32.542850494384766, \t Total Dis Loss : 5.7477525842841715e-05\n",
      "Steps : 167300, \t Total Gen Loss : 30.633861541748047, \t Total Dis Loss : 0.00014625304902438074\n",
      "Steps : 167400, \t Total Gen Loss : 30.75938606262207, \t Total Dis Loss : 0.0005008151638321579\n",
      "Steps : 167500, \t Total Gen Loss : 29.358877182006836, \t Total Dis Loss : 0.000221675043576397\n",
      "Steps : 167600, \t Total Gen Loss : 29.976970672607422, \t Total Dis Loss : 0.00014112499775364995\n",
      "Steps : 167700, \t Total Gen Loss : 31.554529190063477, \t Total Dis Loss : 0.0001220975536853075\n",
      "Steps : 167800, \t Total Gen Loss : 31.049083709716797, \t Total Dis Loss : 5.2142058848403394e-05\n",
      "Steps : 167900, \t Total Gen Loss : 30.26742935180664, \t Total Dis Loss : 5.947006502537988e-05\n",
      "Steps : 168000, \t Total Gen Loss : 31.213665008544922, \t Total Dis Loss : 0.00039967813063412905\n",
      "Steps : 168100, \t Total Gen Loss : 27.49055290222168, \t Total Dis Loss : 0.00045079138362780213\n",
      "Steps : 168200, \t Total Gen Loss : 29.840951919555664, \t Total Dis Loss : 0.0003837455005850643\n",
      "Steps : 168300, \t Total Gen Loss : 31.68406867980957, \t Total Dis Loss : 3.731124161276966e-05\n",
      "Steps : 168400, \t Total Gen Loss : 31.76834487915039, \t Total Dis Loss : 0.00014077838568482548\n",
      "Steps : 168500, \t Total Gen Loss : 31.3178653717041, \t Total Dis Loss : 0.00019809752120636404\n",
      "Steps : 168600, \t Total Gen Loss : 30.401756286621094, \t Total Dis Loss : 0.00045725537347607315\n",
      "Steps : 168700, \t Total Gen Loss : 27.963186264038086, \t Total Dis Loss : 0.00017698803276289254\n",
      "Time for epoch 25 is 307.7883629798889 sec\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 25\n",
    "steps = 0\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for images, labels in train_dataset:\n",
    "        steps += 1\n",
    "        gen_loss, disc_loss = train_step(images)\n",
    "        \n",
    "        if steps % 100 == 0:\n",
    "            print ('Steps : {}, \\t Total Gen Loss : {}, \\t Total Dis Loss : {}'.format(steps, gen_loss.numpy(), disc_loss.numpy()))\n",
    "        \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_path)\n",
    "        \n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "- 학습 도중 저장된 Checkpoint를 아래와 같이 활용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x7f9aac391790>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate(test_dataset, set_lambda=0.9):\n",
    "    an_scores = []\n",
    "    gt_labels = []\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(test_dataset):\n",
    "        generated_images = generator(x_batch_train, training=True)\n",
    "        _, feat_real = discriminator(x_batch_train, training=True)\n",
    "        _, feat_fake = discriminator(generated_images, training=True)\n",
    "\n",
    "        generated_images, feat_real, feat_fake = generated_images.numpy(), feat_real.numpy(), feat_fake.numpy()        \n",
    "\n",
    "        rec = abs(x_batch_train - generated_images)\n",
    "        lat = (feat_real - feat_fake) ** 2\n",
    "\n",
    "        rec = tf.reduce_sum(rec, [1,2,3])\n",
    "        lat = tf.reduce_sum(lat, [1,2,3])\n",
    "        \n",
    "        error = (set_lambda * tf.cast(rec, tf.float32)) + ((1 - set_lambda) * tf.cast(lat, tf.float32))\n",
    "        \n",
    "        an_scores.append(error)\n",
    "        gt_labels.append(y_batch_train)\n",
    "        \n",
    "    an_scores = np.concatenate(an_scores, axis=0).reshape([-1])\n",
    "    gt_labels = np.concatenate(gt_labels, axis=0).reshape([-1])\n",
    "    \n",
    "    an_scores = (an_scores - np.amin(an_scores)) / (np.amax(an_scores) - np.amin(an_scores))\n",
    "    \n",
    "    return an_scores, gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000 16000\n"
     ]
    }
   ],
   "source": [
    "an_scores, gt_labels = _evaluate(test_dataset)\n",
    "\n",
    "print(len(an_scores), len(gt_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아래와 같이 테스트 결과를 원래 라벨에 따라 anomaly 데이터와 normal 데이터로 나누어 따로 분석해 봅니다.\n",
    "\n",
    "- 라벨에 따라 anomaly score의 분포가 다르게 나타나는지를 검증해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000,)\n",
      "(7000,)\n"
     ]
    }
   ],
   "source": [
    "normal = []\n",
    "anormaly = []\n",
    "for score, label in zip(an_scores, gt_labels):\n",
    "    if label == 0:\n",
    "        anormaly.append(score)\n",
    "    else:\n",
    "        normal.append(score)\n",
    "\n",
    "normal = np.array(normal)\n",
    "print(normal.shape)\n",
    "anormaly = np.array(anormaly)\n",
    "print(anormaly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(normal, bins=np.linspace(0.0, 1.0, num=100))\n",
    "plt.hist(anormaly, bins=np.linspace(0.0, 1.0, num=100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.57367873 0.53687155\n",
      "0.14457117 0.13999672\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(normal, norm.pdf(normal, np.mean(normal), np.std(normal)), 'o')\n",
    "plt.plot(anormaly, norm.pdf(anormaly, np.mean(anormaly), np.std(anormaly)), 'o')\n",
    "\n",
    "print(np.mean(normal), np.mean(anormaly))\n",
    "print(np.std(normal), np.std(anormaly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 개구리는 안돼요(CIFAR-10)\n",
    "- 지금까지 잘 따라오셨나요?\n",
    "\n",
    "- 이번 프로젝트는 지금까지의 실습과 동일한 방법으로 CIFAR-10 데이터셋에 대해 진행해 보겠습니다. 여러분들이 만들어야 할 모델은 CIFAR-10의 10가지 클래스 중 개구리 라벨을 이상 데이터로 처리하는 모델입니다. 혹시 개구리가 출현할 경우 이를 감지하여 이상감지 경고를 발생시키는 개구리 감지 모델이라고 할 수 있겠습니다.\n",
    "\n",
    "## 다음의 순서을 따라 진행해 주세요.\n",
    "\n",
    "1. 이상감지용 데이터셋 구축 (개구리 데이터를 학습데이터셋에서 제외하여 테스트 데이터셋에 포함)\n",
    "2. Skip-GANomaly 모델의 구현\n",
    "3. 모델의 학습과 검증\n",
    "4. 검증 결과의 시각화 (정상-이상 데이터의 anomaly score 분포 시각화, 적절한 threshold에 따른 이삼감지율 계산, 감지 성공/실패사례 시각화 포함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
