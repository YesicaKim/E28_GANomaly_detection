{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이미지 이상탐지모델(GAN)\n",
    "\n",
    "- 딥러닝 기술이 유용하게 사용될 수 있는 대표적인 사례로 바로 이상감지(Anomaly Detection)를 꼽을 수 있습니다. 혹시라도 위험한 물품이 포함되어 있지 않을까 24시간 사람이 뚫어져라 모니터를 쳐다보고 있어야만 하는 고된 일을 인공지능이 대신 정확하게 수행해 줄 수 있다면 좋겠죠? 이런 과제는 실제로 공항, 항만, 주요 군사시설 등에서 매우 관심을 가지고 진행하고 있답니다. 그런데 이런 이상감지 모델로 GAN 기술이 유용하게 활용되고 있다고 합니다.\n",
    "\n",
    "- 왜 그럴까요? 그냥 쉽게 생각할 수 있는 지도학습(supervised learning)으로는 어려울까요? 네 그렇습니다. 이상감지 태스크의 경우, 정상(Normal) 상황에 대한 데이터는 아주 풍부하게 얻을 수 있지만, 이상(Anomaly) 상황에 대한 데이터는 매우 부족할 뿐 아니라 사실상 모든 케이스의 이상 상황을 정의할 수조차 없기 때문입니다. 이 때 GAN이 사용된다면 이상(Anomaly) 데이터가 충분하지 않은 경우에도 꽤 쓸만한 이상감지 성능을 발휘하는 모델을 만들 수 있다고 합니다.\n",
    "\n",
    "## 학습목표\n",
    "- 이상(Anomaly) 데이터가 부족한 상황에서 GAN을 이용해 이미지 이상감지 모델을 구축하는 논리를 파악한다.\n",
    "- Skip-GANomaly 모델 및 Loss 함수를 구현해 본다.\n",
    "- 간단한 데이터셋을 이용해 Skip-GANomaly 의 이상감지 효과를 파악해 본다.\n",
    "\n",
    "## 학습 목차\n",
    "- Anomaly Detection with GAN\n",
    "- GANomaly\n",
    "- Skip-GANomaly\n",
    "- 데이터셋 구성\n",
    "- 모델과 Loss함수 구성\n",
    "- 모델 학습과 평가\n",
    "\n",
    "## 평가 루브릭\n",
    "\n",
    "- 아래의 기준을 바탕으로 프로젝트를 평가합니다.\n",
    "\n",
    "###  평가문항\t상세기준\n",
    "1. Anomaly Detection 태스크 특성에 맞도록 데이터셋 가공이 진행되었다.: 개구리 클래스가 배제되어 테스트셋에만 반영되는 로직이 구현되었다.\n",
    "2. Skip-GANomaly 모델이 정상적으로 구현되었다.: 모델 학습 및 테스트가 원활하게 진행되었다.\n",
    "3. 이상감지 수행 결과가 체계적으로 시각화되었다. : 정상-이상 데이터의 anomaly score 분포 시각화, 테스트셋 이상감지 정확도(%), 적절한 threshold에 따른 이상 감지율 계산, 이상감지 성공실패사례 제시가 진행되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ★ 프로젝트: 개구리 이상감지 경고 시스템 (CIFAR-10) ★ \n",
    "\n",
    "- 이번 프로젝트는 CIFAR-10 데이터셋에 대해 진행해 보겠습니다. \n",
    "- CIFAR-10의 10가지 클래스 중 개구리 라벨을 이상 데이터로 처리하는 모델입니다. \n",
    "- 혹시 개구리가 출현할 경우 이를 감지하여 이상감지 경고를 발생시키는 개구리 감지 모델이라고 할 수 있겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 구성(Fashion-MNIST)\n",
    "\n",
    "- 지금부터는 Skip-GANomaly 모델을 실제로 구현하는 과정을 진행하려고 합니다. \n",
    "- 이런 이상감지 모델을 학습할 때 쉽게 사용하는 데이터 구성은 MNIST 같이 잘 알려진 데이터셋에서 특정 클래스를 이상 데이터로 활용하는 방식입니다. \n",
    "- 예를 들어 0으로 라벨링된 모든 이미지를 훈련 데이터에서 제거하고 1~9로만 학습을 시킨 후 테스트 데이터를 0을 포함한(이상 데이터를 포함한) 형태로 구성하여 이상감지 성능을 평가하는 방식입니다.\n",
    "\n",
    "- CIFAR-10에서 6번 라벨이 개구리(Frog)입니다. 개구리를 제거하고 나머지 클래스로 구성된 CIFAR-10 데이터를 정상 데이터로 삼아볼 생각입니다.\n",
    "\n",
    "- 말하자면, 우리의 모델은 개구리는 보도 듣도 못하였으며, 개구리가 나타나면 매우 특이한 상황이라고 가정하는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix, precision_recall_curve, average_precision_score\n",
    "from scipy.interpolate import interp1d\n",
    "from inspect import signature\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from scipy.stats import norm\n",
    "\n",
    "from IPython import display\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이상감지용 데이터셋 구축\n",
    "\n",
    "## 개구리 데이터를 학습데이터셋에서 제외하여 테스트 데이터셋에 포함\n",
    "\n",
    "- 위에서 언급한 대로 학습/테스트 데이터셋을 구축해 보겠습니다.\n",
    "\n",
    "- CIFAR-10 이미지를 사용하도록 하겠습니다. \n",
    "\n",
    "    - 3채널 color 데이터셋입니다. Convolution 연산의 reshape 과정이 필요없습니다. \n",
    "    - 데이터 이미지 사이즈는 32 X 32이므로 바로 Unet 구조 활용이 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습에 사용할 train_x의 이미지를 [-1, 1]로 정규화합니다.\n",
    "- 로드한 CIFAR-10 학습 데이터를 시각화를 통해 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i in range(10):\n",
    "    rn = np.random.randint(len(train_data))\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(train_data[rn])\n",
    "    plt.title(f'index: {rn}')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지를 [-1, 1]로 정규화합니다\n",
    "\n",
    "train_data = (train_data - 127.5) / 127.5\n",
    "test_data = (test_data - 127.5) / 127.5\n",
    "\n",
    "# float32 데이터 타입 변환\n",
    "train_data = train_data.reshape(train_data.shape[0], 32, 32, 3).astype('float32')\n",
    "test_data = test_data.reshape(test_data.shape[0], 32, 32, 3).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위에서 설명한 대로 이상 데이터로 선정된 6번 라벨(Frog) 데이터를 제외하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_labels(labels):\n",
    "    new_t_labels = []\n",
    "    for old_label in labels:\n",
    "        if old_label == 6:   # Frog: 6\n",
    "            new_t_labels.append([0])  # Frog를 이상치로 처리\n",
    "        else:\n",
    "            new_t_labels.append([1])  # 그 외의 경우는 정상치\n",
    "             \n",
    "    return np.array(new_t_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bol_train_labels = set_labels(train_labels)\n",
    "bol_test_labels = set_labels(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = []\n",
    "normal_labels = []\n",
    "anomaly_data = []\n",
    "anomaly_labels = []\n",
    "for data, label in zip(train_data, bol_train_labels):\n",
    "    if label == 0:\n",
    "        anomaly_data.append(data)\n",
    "        anomaly_labels.append(label)\n",
    "    else:\n",
    "        normal_data.append(data)\n",
    "        normal_labels.append(label)\n",
    "        \n",
    "normal_data = np.array(normal_data)\n",
    "normal_labels = np.array(normal_labels)\n",
    "anomaly_data = np.array(anomaly_data)\n",
    "anomaly_labels = np.array(anomaly_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32, 3) (45000, 1)\n",
      "(5000, 32, 32, 3) (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(normal_data.shape, normal_labels.shape)\n",
    "print(anomaly_data.shape, anomaly_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = normal_data\n",
    "bol_train_labels = normal_labels\n",
    "test_data = tf.concat([test_data, anomaly_data], 0)\n",
    "bol_test_labels = tf.concat([bol_test_labels, anomaly_labels], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 32, 32, 3)\n",
      "(15000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 1)\n",
      "(15000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(bol_train_labels.shape)\n",
    "print(bol_test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 어떻습니까? 50000건의 훈련데이터 중 5000건이 제외되어 10000건의 테스트 데이터에 추가되었습니다. 계산이 맞아떨어지나요?\n",
    "\n",
    "- 데이터셋이 정확하게 구성되었는지 좀더 검증해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "for label in bol_train_labels:\n",
    "    if label == 0:\n",
    "        print(label)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제 데이터셋을 구성하고 label을 검증해 보겠습니다.\n",
    "\n",
    "- 훈련 데이터셋에는 라벨이 1인 데이터만 존재하고, 테스트 데이터에는 0과 1이 섞여 있어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, bol_train_labels))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, bol_test_labels))\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]], shape=(8, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for data, label in train_dataset.take(1):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]], shape=(8, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for data, label in test_dataset.take(1):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-GANomaly 모델의 구현\n",
    "\n",
    "\n",
    "![title](Skip-GANomaly1.png)\n",
    "\n",
    "\n",
    "![title](Skip-GANomaly2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델과 Loss함수 구성\n",
    "\n",
    "## Generator\n",
    "- 이제 본격적으로 모델을 구성해 보겠습니다. Generator는 그동안 자주 다루었을 UNet 구조를 따릅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_block, self).__init__()\n",
    "        self.conv_layer = tf.keras.Sequential([\n",
    "            layers.Conv2D(num_filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                          kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        outputs = self.conv_layer(inputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_T_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_T_block, self).__init__()\n",
    "        self.conv_T_layer = tf.keras.Sequential([\n",
    "            layers.Conv2DTranspose(num_filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                                   kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, concat, training=False):\n",
    "        upsample = self.conv_T_layer(inputs)\n",
    "        outputs = tf.concat([upsample, concat], -1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, num_output_channel=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder_1 = Conv_block(64) # 16\n",
    "        self.encoder_2 = Conv_block(128) # 8\n",
    "        self.encoder_3 = Conv_block(256) # 4\n",
    "        self.encoder_4 = Conv_block(512) # 2\n",
    "        \n",
    "        self.center = Conv_block(512) # 1\n",
    "        \n",
    "        self.decoder_4 = Conv_T_block(512) # 2\n",
    "        self.decoder_3 = Conv_T_block(256) # 4\n",
    "        self.decoder_2 = Conv_T_block(128) # 8\n",
    "        self.decoder_1 = Conv_T_block(64) # 16\n",
    "        \n",
    "        self.output_layer = layers.Conv2DTranspose(num_output_channel, 1, strides=2, padding='same', use_bias=False, # 32\n",
    "                                                   kernel_initializer=tf.random_normal_initializer(0., 0.02))\n",
    "                \n",
    "    def call(self, inputs, training=False):\n",
    "        en_1 = self.encoder_1(inputs) # gen\n",
    "        en_2 = self.encoder_2(en_1)\n",
    "        en_3 = self.encoder_3(en_2)\n",
    "        en_4 = self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        de_4 = self.decoder_4(center, en_4)\n",
    "        de_3 = self.decoder_3(de_4, en_3)\n",
    "        de_2 = self.decoder_2(de_3, en_2)\n",
    "        de_1 = self.decoder_1(de_2, en_1)\n",
    "        \n",
    "        outputs = self.output_layer(de_1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator\n",
    "- Discriminator도 Generator처럼 Conv_block을 활용하며, 최종적으로 sigmoid를 거쳐 0~1 사이의 숫자를 리턴합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.encoder_1 = Conv_block(64) # 16\n",
    "        self.encoder_2 = Conv_block(128) # 8\n",
    "        self.encoder_3 = Conv_block(256) # 4\n",
    "        self.encoder_4 = Conv_block(512) # 2\n",
    "        \n",
    "        self.center = Conv_block(100) # 1\n",
    "        \n",
    "        self.outputs = layers.Conv2D(1, 3, strides=1, padding='same',\n",
    "                                          use_bias=False, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        en_1 = self.encoder_1(inputs) # dis\n",
    "        en_2 = self.encoder_2(en_1)\n",
    "        en_3 = self.encoder_3(en_2)\n",
    "        en_4 = self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        outputs = self.outputs(center)\n",
    "        \n",
    "        return outputs, center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전체 모델 구성\n",
    "- Generator와 Discriminator을 합쳐 전체 모델을 구성해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(num_output_channel=3)  # Generator가 32X32X3 짜리 이미지를 생성해야 합니다. \n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss 함수\n",
    "- GAN 모델의 핵심은 Loss 함수의 구성방법에 달려 있다고 해도 과언이 아닙니다. Skip-GANomaly는 이전 모델들과 달리 일반적인 GAN의 학습 절차와 같은 형태의 Loss 구성이 진행됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_loss = tf.keras.losses.MeanSquaredError()\n",
    "l1_loss = tf.keras.losses.MeanAbsoluteError()\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(pred_real, pred_fake):\n",
    "    real_loss = cross_entropy(tf.ones_like(pred_real), pred_real)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(pred_fake), pred_fake)\n",
    "    \n",
    "    total_dis_loss = (real_loss + fake_loss) * 0.5\n",
    "    \n",
    "    return total_dis_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아래 Generator Loss에는 이전 스텝에서 설명했던 Skip-GANomaly의 주요 loss 함수들이 포함되어 있음을 주목해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(real_output, fake_output, input_data, gen_data, latent_first, latent_sec):\n",
    "    w_adv = 1.\n",
    "    w_context = 40.\n",
    "    w_encoder = 1.\n",
    "    \n",
    "    adv_loss = cross_entropy(real_output, fake_output)\n",
    "    context_loss = l1_loss(input_data, gen_data)\n",
    "    encoder_loss = l2_loss(latent_first, latent_sec)\n",
    "    \n",
    "    total_gen_loss = w_adv * adv_loss + \\\n",
    "                     w_context * context_loss + \\\n",
    "                     w_encoder * encoder_loss\n",
    "    \n",
    "    return total_gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer 설정\n",
    "generator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델의 학습과 검증\n",
    "\n",
    "# 모델 학습과 평가\n",
    "## Model Train\n",
    "- 이제 본격적으로 모델을 학습해 보겠습니다. 총 25Epoch 대략 1시간 정도 소요될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(images, training=True)\n",
    "        \n",
    "        pred_real, feat_real = discriminator(images, training=True)\n",
    "        pred_fake, feat_fake = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(pred_real, pred_fake,\n",
    "                                  images, generated_images,\n",
    "                                  feat_real, feat_fake)\n",
    "\n",
    "        disc_loss = discriminator_loss(pred_real, pred_fake)        \n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(os.getenv('HOME'),'aiffel/GANomaly/ganomaly_skip_no_norm/ckpt1')\n",
    "\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 총 25Epoch 를 수행하는데 1시간 이상 소요될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 100, \t Total Gen Loss : 14.313623428344727, \t Total Dis Loss : 0.7018235921859741\n",
      "Steps : 200, \t Total Gen Loss : 14.977291107177734, \t Total Dis Loss : 0.6398770809173584\n",
      "Steps : 300, \t Total Gen Loss : 12.386713981628418, \t Total Dis Loss : 0.535588800907135\n",
      "Steps : 400, \t Total Gen Loss : 17.506315231323242, \t Total Dis Loss : 0.3149919807910919\n",
      "Steps : 500, \t Total Gen Loss : 16.563995361328125, \t Total Dis Loss : 0.17589375376701355\n",
      "Steps : 600, \t Total Gen Loss : 20.432497024536133, \t Total Dis Loss : 0.2681710124015808\n",
      "Steps : 700, \t Total Gen Loss : 16.349384307861328, \t Total Dis Loss : 0.0801103338599205\n",
      "Steps : 800, \t Total Gen Loss : 18.181903839111328, \t Total Dis Loss : 0.056031644344329834\n",
      "Steps : 900, \t Total Gen Loss : 17.390636444091797, \t Total Dis Loss : 0.13699772953987122\n",
      "Steps : 1000, \t Total Gen Loss : 18.663225173950195, \t Total Dis Loss : 0.29011446237564087\n",
      "Steps : 1100, \t Total Gen Loss : 20.830421447753906, \t Total Dis Loss : 0.0272382702678442\n",
      "Steps : 1200, \t Total Gen Loss : 17.94619369506836, \t Total Dis Loss : 0.02551468461751938\n",
      "Steps : 1300, \t Total Gen Loss : 16.359573364257812, \t Total Dis Loss : 0.6740323305130005\n",
      "Steps : 1400, \t Total Gen Loss : 21.195114135742188, \t Total Dis Loss : 0.021366748958826065\n",
      "Steps : 1500, \t Total Gen Loss : 18.367996215820312, \t Total Dis Loss : 0.2565392255783081\n",
      "Steps : 1600, \t Total Gen Loss : 19.462984085083008, \t Total Dis Loss : 0.013390392065048218\n",
      "Steps : 1700, \t Total Gen Loss : 20.19717788696289, \t Total Dis Loss : 0.01876598410308361\n",
      "Steps : 1800, \t Total Gen Loss : 19.8334903717041, \t Total Dis Loss : 0.061187587678432465\n",
      "Steps : 1900, \t Total Gen Loss : 21.85918426513672, \t Total Dis Loss : 0.005796799436211586\n",
      "Steps : 2000, \t Total Gen Loss : 23.315914154052734, \t Total Dis Loss : 0.03393232822418213\n",
      "Steps : 2100, \t Total Gen Loss : 18.898221969604492, \t Total Dis Loss : 0.11760744452476501\n",
      "Steps : 2200, \t Total Gen Loss : 23.85309600830078, \t Total Dis Loss : 0.07465905696153641\n",
      "Steps : 2300, \t Total Gen Loss : 20.65083122253418, \t Total Dis Loss : 0.009783929213881493\n",
      "Steps : 2400, \t Total Gen Loss : 20.63471221923828, \t Total Dis Loss : 0.0072674741968512535\n",
      "Steps : 2500, \t Total Gen Loss : 19.092050552368164, \t Total Dis Loss : 0.012648447416722775\n",
      "Steps : 2600, \t Total Gen Loss : 19.04990005493164, \t Total Dis Loss : 0.02491576038300991\n",
      "Steps : 2700, \t Total Gen Loss : 19.23978042602539, \t Total Dis Loss : 0.009452266618609428\n",
      "Steps : 2800, \t Total Gen Loss : 20.125303268432617, \t Total Dis Loss : 0.01347068976610899\n",
      "Steps : 2900, \t Total Gen Loss : 21.83026123046875, \t Total Dis Loss : 0.007115032058209181\n",
      "Steps : 3000, \t Total Gen Loss : 19.6839656829834, \t Total Dis Loss : 0.036128539592027664\n",
      "Steps : 3100, \t Total Gen Loss : 20.71145248413086, \t Total Dis Loss : 0.00373100396245718\n",
      "Steps : 3200, \t Total Gen Loss : 18.186431884765625, \t Total Dis Loss : 0.01964329555630684\n",
      "Steps : 3300, \t Total Gen Loss : 24.937232971191406, \t Total Dis Loss : 0.004199179355055094\n",
      "Steps : 3400, \t Total Gen Loss : 21.838058471679688, \t Total Dis Loss : 0.007567013613879681\n",
      "Steps : 3500, \t Total Gen Loss : 22.352876663208008, \t Total Dis Loss : 0.002408471191301942\n",
      "Steps : 3600, \t Total Gen Loss : 22.358516693115234, \t Total Dis Loss : 0.06428447365760803\n",
      "Steps : 3700, \t Total Gen Loss : 23.60984992980957, \t Total Dis Loss : 0.025359999388456345\n",
      "Steps : 3800, \t Total Gen Loss : 20.820890426635742, \t Total Dis Loss : 0.019947020336985588\n",
      "Steps : 3900, \t Total Gen Loss : 21.220191955566406, \t Total Dis Loss : 0.024670319631695747\n",
      "Steps : 4000, \t Total Gen Loss : 20.107704162597656, \t Total Dis Loss : 0.003340297145769\n",
      "Steps : 4100, \t Total Gen Loss : 21.557132720947266, \t Total Dis Loss : 0.024969853460788727\n",
      "Steps : 4200, \t Total Gen Loss : 20.972505569458008, \t Total Dis Loss : 0.004335900768637657\n",
      "Steps : 4300, \t Total Gen Loss : 20.723491668701172, \t Total Dis Loss : 0.17409494519233704\n",
      "Steps : 4400, \t Total Gen Loss : 22.578697204589844, \t Total Dis Loss : 0.003893285058438778\n",
      "Steps : 4500, \t Total Gen Loss : 21.65970802307129, \t Total Dis Loss : 0.006246624514460564\n",
      "Steps : 4600, \t Total Gen Loss : 20.053281784057617, \t Total Dis Loss : 0.007428186945617199\n",
      "Steps : 4700, \t Total Gen Loss : 22.059005737304688, \t Total Dis Loss : 0.0023029018193483353\n",
      "Steps : 4800, \t Total Gen Loss : 18.703887939453125, \t Total Dis Loss : 0.010486960411071777\n",
      "Steps : 4900, \t Total Gen Loss : 20.845050811767578, \t Total Dis Loss : 0.0036192818079143763\n",
      "Steps : 5000, \t Total Gen Loss : 21.025453567504883, \t Total Dis Loss : 0.002072771079838276\n",
      "Steps : 5100, \t Total Gen Loss : 21.539764404296875, \t Total Dis Loss : 0.015446104109287262\n",
      "Steps : 5200, \t Total Gen Loss : 23.2924747467041, \t Total Dis Loss : 0.002583538182079792\n",
      "Steps : 5300, \t Total Gen Loss : 18.425798416137695, \t Total Dis Loss : 1.7179137468338013\n",
      "Steps : 5400, \t Total Gen Loss : 23.27301788330078, \t Total Dis Loss : 0.014771435409784317\n",
      "Steps : 5500, \t Total Gen Loss : 20.89051628112793, \t Total Dis Loss : 0.003923606593161821\n",
      "Steps : 5600, \t Total Gen Loss : 23.019611358642578, \t Total Dis Loss : 0.00677471561357379\n",
      "Time for epoch 1 is 270.5181894302368 sec\n",
      "Steps : 5700, \t Total Gen Loss : 22.450359344482422, \t Total Dis Loss : 0.0010893659200519323\n",
      "Steps : 5800, \t Total Gen Loss : 20.078407287597656, \t Total Dis Loss : 0.01243787445127964\n",
      "Steps : 5900, \t Total Gen Loss : 24.476675033569336, \t Total Dis Loss : 0.008537239395081997\n",
      "Steps : 6000, \t Total Gen Loss : 21.186391830444336, \t Total Dis Loss : 0.017037861049175262\n",
      "Steps : 6100, \t Total Gen Loss : 20.30591583251953, \t Total Dis Loss : 0.02009897120296955\n",
      "Steps : 6200, \t Total Gen Loss : 24.00821876525879, \t Total Dis Loss : 0.024150436744093895\n",
      "Steps : 6300, \t Total Gen Loss : 21.464311599731445, \t Total Dis Loss : 0.2952224612236023\n",
      "Steps : 6400, \t Total Gen Loss : 18.739295959472656, \t Total Dis Loss : 0.015536200255155563\n",
      "Steps : 6500, \t Total Gen Loss : 18.38194465637207, \t Total Dis Loss : 0.013593802228569984\n",
      "Steps : 6600, \t Total Gen Loss : 20.918041229248047, \t Total Dis Loss : 0.01010961178690195\n",
      "Steps : 6700, \t Total Gen Loss : 19.386207580566406, \t Total Dis Loss : 0.005149233154952526\n",
      "Steps : 6800, \t Total Gen Loss : 21.675228118896484, \t Total Dis Loss : 0.004259944893419743\n",
      "Steps : 6900, \t Total Gen Loss : 22.57464599609375, \t Total Dis Loss : 0.0026791314594447613\n",
      "Steps : 7000, \t Total Gen Loss : 19.27777099609375, \t Total Dis Loss : 0.013986696489155293\n",
      "Steps : 7100, \t Total Gen Loss : 23.37389373779297, \t Total Dis Loss : 0.0028631705790758133\n",
      "Steps : 7200, \t Total Gen Loss : 23.317096710205078, \t Total Dis Loss : 0.00685914745554328\n",
      "Steps : 7300, \t Total Gen Loss : 22.186471939086914, \t Total Dis Loss : 0.003104281146079302\n",
      "Steps : 7400, \t Total Gen Loss : 22.234821319580078, \t Total Dis Loss : 0.017108457162976265\n",
      "Steps : 7500, \t Total Gen Loss : 22.265731811523438, \t Total Dis Loss : 0.025064736604690552\n",
      "Steps : 7600, \t Total Gen Loss : 21.800983428955078, \t Total Dis Loss : 0.0030635471921414137\n",
      "Steps : 7700, \t Total Gen Loss : 17.837400436401367, \t Total Dis Loss : 0.04686392843723297\n",
      "Steps : 7800, \t Total Gen Loss : 20.990982055664062, \t Total Dis Loss : 0.017490094527602196\n",
      "Steps : 7900, \t Total Gen Loss : 19.366680145263672, \t Total Dis Loss : 0.09013005346059799\n",
      "Steps : 8000, \t Total Gen Loss : 20.958690643310547, \t Total Dis Loss : 0.0026396496687084436\n",
      "Steps : 8100, \t Total Gen Loss : 20.46895980834961, \t Total Dis Loss : 0.00244891457259655\n",
      "Steps : 8200, \t Total Gen Loss : 22.633926391601562, \t Total Dis Loss : 0.00181626807898283\n",
      "Steps : 8300, \t Total Gen Loss : 23.821205139160156, \t Total Dis Loss : 0.012347792275249958\n",
      "Steps : 8400, \t Total Gen Loss : 23.106143951416016, \t Total Dis Loss : 0.018751202151179314\n",
      "Steps : 8500, \t Total Gen Loss : 21.248750686645508, \t Total Dis Loss : 0.017152735963463783\n",
      "Steps : 8600, \t Total Gen Loss : 21.121368408203125, \t Total Dis Loss : 0.001367224846035242\n",
      "Steps : 8700, \t Total Gen Loss : 26.198753356933594, \t Total Dis Loss : 0.0011472949991002679\n",
      "Steps : 8800, \t Total Gen Loss : 22.2774600982666, \t Total Dis Loss : 0.003507003653794527\n",
      "Steps : 8900, \t Total Gen Loss : 23.550607681274414, \t Total Dis Loss : 0.002859107917174697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 9000, \t Total Gen Loss : 25.380373001098633, \t Total Dis Loss : 0.0011436908971518278\n",
      "Steps : 9100, \t Total Gen Loss : 21.424484252929688, \t Total Dis Loss : 0.002478247042745352\n",
      "Steps : 9200, \t Total Gen Loss : 20.34758186340332, \t Total Dis Loss : 0.005473420023918152\n",
      "Steps : 9300, \t Total Gen Loss : 19.937694549560547, \t Total Dis Loss : 0.003217320889234543\n",
      "Steps : 9400, \t Total Gen Loss : 19.21841049194336, \t Total Dis Loss : 0.009066691622138023\n",
      "Steps : 9500, \t Total Gen Loss : 23.933176040649414, \t Total Dis Loss : 0.0015853820368647575\n",
      "Steps : 9600, \t Total Gen Loss : 21.520015716552734, \t Total Dis Loss : 0.0014084369176998734\n",
      "Steps : 9700, \t Total Gen Loss : 23.588977813720703, \t Total Dis Loss : 0.0008955857483670115\n",
      "Steps : 9800, \t Total Gen Loss : 24.506906509399414, \t Total Dis Loss : 0.0009641604265198112\n",
      "Steps : 9900, \t Total Gen Loss : 23.3546199798584, \t Total Dis Loss : 0.0011049448512494564\n",
      "Steps : 10000, \t Total Gen Loss : 21.726804733276367, \t Total Dis Loss : 0.006206861697137356\n",
      "Steps : 10100, \t Total Gen Loss : 20.255979537963867, \t Total Dis Loss : 0.0015015405369922519\n",
      "Steps : 10200, \t Total Gen Loss : 20.78754234313965, \t Total Dis Loss : 0.001298481714911759\n",
      "Steps : 10300, \t Total Gen Loss : 20.065460205078125, \t Total Dis Loss : 0.003407388459891081\n",
      "Steps : 10400, \t Total Gen Loss : 24.70398712158203, \t Total Dis Loss : 0.0010770120425149798\n",
      "Steps : 10500, \t Total Gen Loss : 22.00389862060547, \t Total Dis Loss : 0.0019299087580293417\n",
      "Steps : 10600, \t Total Gen Loss : 21.723875045776367, \t Total Dis Loss : 0.004510132130235434\n",
      "Steps : 10700, \t Total Gen Loss : 22.817304611206055, \t Total Dis Loss : 0.0007059995550662279\n",
      "Steps : 10800, \t Total Gen Loss : 21.938732147216797, \t Total Dis Loss : 0.001987919444218278\n",
      "Steps : 10900, \t Total Gen Loss : 21.284563064575195, \t Total Dis Loss : 0.002177686197683215\n",
      "Steps : 11000, \t Total Gen Loss : 23.393274307250977, \t Total Dis Loss : 0.001177880447357893\n",
      "Steps : 11100, \t Total Gen Loss : 23.314485549926758, \t Total Dis Loss : 0.015405540354549885\n",
      "Steps : 11200, \t Total Gen Loss : 25.08946418762207, \t Total Dis Loss : 0.004558268003165722\n",
      "Time for epoch 2 is 257.17559576034546 sec\n",
      "Steps : 11300, \t Total Gen Loss : 21.055692672729492, \t Total Dis Loss : 0.0017441337695345283\n",
      "Steps : 11400, \t Total Gen Loss : 24.522960662841797, \t Total Dis Loss : 0.0010385944042354822\n",
      "Steps : 11500, \t Total Gen Loss : 25.5905704498291, \t Total Dis Loss : 0.0027521001175045967\n",
      "Steps : 11600, \t Total Gen Loss : 21.856691360473633, \t Total Dis Loss : 0.0021730568259954453\n",
      "Steps : 11700, \t Total Gen Loss : 22.34622573852539, \t Total Dis Loss : 0.0012019327841699123\n",
      "Steps : 11800, \t Total Gen Loss : 25.27219581604004, \t Total Dis Loss : 0.002992740599438548\n",
      "Steps : 11900, \t Total Gen Loss : 23.429712295532227, \t Total Dis Loss : 0.0012702716048806906\n",
      "Steps : 12000, \t Total Gen Loss : 20.595653533935547, \t Total Dis Loss : 0.0008600903674960136\n",
      "Steps : 12100, \t Total Gen Loss : 24.8337345123291, \t Total Dis Loss : 0.001372668193653226\n",
      "Steps : 12200, \t Total Gen Loss : 23.616653442382812, \t Total Dis Loss : 0.00110002257861197\n",
      "Steps : 12300, \t Total Gen Loss : 23.33860969543457, \t Total Dis Loss : 0.0007768311770632863\n",
      "Steps : 12400, \t Total Gen Loss : 23.843563079833984, \t Total Dis Loss : 0.0013062257785350084\n",
      "Steps : 12500, \t Total Gen Loss : 25.748706817626953, \t Total Dis Loss : 0.0033306321129202843\n",
      "Steps : 12600, \t Total Gen Loss : 22.12398910522461, \t Total Dis Loss : 0.002413091016933322\n",
      "Steps : 12700, \t Total Gen Loss : 24.200042724609375, \t Total Dis Loss : 0.0012398195685818791\n",
      "Steps : 12800, \t Total Gen Loss : 23.58673667907715, \t Total Dis Loss : 0.0018536957213655114\n",
      "Steps : 12900, \t Total Gen Loss : 25.56097984313965, \t Total Dis Loss : 0.0017579933628439903\n",
      "Steps : 13000, \t Total Gen Loss : 24.750316619873047, \t Total Dis Loss : 0.00196830159984529\n",
      "Steps : 13100, \t Total Gen Loss : 25.097715377807617, \t Total Dis Loss : 0.002054099924862385\n",
      "Steps : 13200, \t Total Gen Loss : 27.627113342285156, \t Total Dis Loss : 0.0026251378003507853\n",
      "Steps : 13300, \t Total Gen Loss : 25.01097297668457, \t Total Dis Loss : 0.0014735127333551645\n",
      "Steps : 13400, \t Total Gen Loss : 26.444419860839844, \t Total Dis Loss : 0.0007171737961471081\n",
      "Steps : 13500, \t Total Gen Loss : 24.472583770751953, \t Total Dis Loss : 0.004285077564418316\n",
      "Steps : 13600, \t Total Gen Loss : 21.170726776123047, \t Total Dis Loss : 0.0005506742163561285\n",
      "Steps : 13700, \t Total Gen Loss : 25.058122634887695, \t Total Dis Loss : 0.0007557013304904103\n",
      "Steps : 13800, \t Total Gen Loss : 24.346181869506836, \t Total Dis Loss : 0.0012298249639570713\n",
      "Steps : 13900, \t Total Gen Loss : 26.037939071655273, \t Total Dis Loss : 0.0017287579830735922\n",
      "Steps : 14000, \t Total Gen Loss : 24.61066246032715, \t Total Dis Loss : 0.001738674589432776\n",
      "Steps : 14100, \t Total Gen Loss : 28.11661148071289, \t Total Dis Loss : 0.0013579619117081165\n",
      "Steps : 14200, \t Total Gen Loss : 25.85888671875, \t Total Dis Loss : 0.0012477310374379158\n",
      "Steps : 14300, \t Total Gen Loss : 25.69949722290039, \t Total Dis Loss : 0.0002400301891611889\n",
      "Steps : 14400, \t Total Gen Loss : 25.684093475341797, \t Total Dis Loss : 0.0007299331482499838\n",
      "Steps : 14500, \t Total Gen Loss : 22.72922706604004, \t Total Dis Loss : 0.0037947730161249638\n",
      "Steps : 14600, \t Total Gen Loss : 22.59160041809082, \t Total Dis Loss : 0.004972022958099842\n",
      "Steps : 14700, \t Total Gen Loss : 22.99725341796875, \t Total Dis Loss : 0.0004662310821004212\n",
      "Steps : 14800, \t Total Gen Loss : 22.525001525878906, \t Total Dis Loss : 0.0016034733271226287\n",
      "Steps : 14900, \t Total Gen Loss : 23.905467987060547, \t Total Dis Loss : 0.003313442226499319\n",
      "Steps : 15000, \t Total Gen Loss : 26.110610961914062, \t Total Dis Loss : 0.0006752432091161609\n",
      "Steps : 15100, \t Total Gen Loss : 25.882946014404297, \t Total Dis Loss : 0.004713049158453941\n",
      "Steps : 15200, \t Total Gen Loss : 24.883852005004883, \t Total Dis Loss : 0.001657381420955062\n",
      "Steps : 15300, \t Total Gen Loss : 22.853755950927734, \t Total Dis Loss : 0.003367227967828512\n",
      "Steps : 15400, \t Total Gen Loss : 27.712642669677734, \t Total Dis Loss : 0.010097174905240536\n",
      "Steps : 15500, \t Total Gen Loss : 25.053115844726562, \t Total Dis Loss : 0.0012025346513837576\n",
      "Steps : 15600, \t Total Gen Loss : 22.880138397216797, \t Total Dis Loss : 0.00310164294205606\n",
      "Steps : 15700, \t Total Gen Loss : 22.837467193603516, \t Total Dis Loss : 0.0016074453014880419\n",
      "Steps : 15800, \t Total Gen Loss : 25.880443572998047, \t Total Dis Loss : 0.0013058965560048819\n",
      "Steps : 15900, \t Total Gen Loss : 23.231157302856445, \t Total Dis Loss : 0.0012524980120360851\n",
      "Steps : 16000, \t Total Gen Loss : 25.631694793701172, \t Total Dis Loss : 0.0017814366146922112\n",
      "Steps : 16100, \t Total Gen Loss : 25.897449493408203, \t Total Dis Loss : 0.00044059084029868245\n",
      "Steps : 16200, \t Total Gen Loss : 24.09343910217285, \t Total Dis Loss : 0.003884704317897558\n",
      "Steps : 16300, \t Total Gen Loss : 25.667194366455078, \t Total Dis Loss : 0.001140115549787879\n",
      "Steps : 16400, \t Total Gen Loss : 24.493528366088867, \t Total Dis Loss : 0.0017265374772250652\n",
      "Steps : 16500, \t Total Gen Loss : 24.004409790039062, \t Total Dis Loss : 0.00041222848813049495\n",
      "Steps : 16600, \t Total Gen Loss : 21.319786071777344, \t Total Dis Loss : 0.0013314886018633842\n",
      "Steps : 16700, \t Total Gen Loss : 25.022186279296875, \t Total Dis Loss : 0.0029639513231813908\n",
      "Steps : 16800, \t Total Gen Loss : 24.373760223388672, \t Total Dis Loss : 0.024810872972011566\n",
      "Time for epoch 3 is 255.47721219062805 sec\n",
      "Steps : 16900, \t Total Gen Loss : 25.302907943725586, \t Total Dis Loss : 0.0065627433359622955\n",
      "Steps : 17000, \t Total Gen Loss : 23.928319931030273, \t Total Dis Loss : 0.0023431142326444387\n",
      "Steps : 17100, \t Total Gen Loss : 22.117612838745117, \t Total Dis Loss : 0.0018090923549607396\n",
      "Steps : 17200, \t Total Gen Loss : 24.76314353942871, \t Total Dis Loss : 0.00022727763280272484\n",
      "Steps : 17300, \t Total Gen Loss : 25.770105361938477, \t Total Dis Loss : 0.000653291936032474\n",
      "Steps : 17400, \t Total Gen Loss : 24.021827697753906, \t Total Dis Loss : 0.0011747028911486268\n",
      "Steps : 17500, \t Total Gen Loss : 28.669971466064453, \t Total Dis Loss : 0.0003483171749394387\n",
      "Steps : 17600, \t Total Gen Loss : 23.589096069335938, \t Total Dis Loss : 0.001144683687016368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 17700, \t Total Gen Loss : 25.724586486816406, \t Total Dis Loss : 0.0013633323833346367\n",
      "Steps : 17800, \t Total Gen Loss : 26.798978805541992, \t Total Dis Loss : 0.0003705396084114909\n",
      "Steps : 17900, \t Total Gen Loss : 22.588897705078125, \t Total Dis Loss : 0.00183590454980731\n",
      "Steps : 18000, \t Total Gen Loss : 22.49734878540039, \t Total Dis Loss : 0.0005611561937257648\n",
      "Steps : 18100, \t Total Gen Loss : 26.4140625, \t Total Dis Loss : 0.0004425010411068797\n",
      "Steps : 18200, \t Total Gen Loss : 24.706207275390625, \t Total Dis Loss : 0.0005780404317192733\n",
      "Steps : 18300, \t Total Gen Loss : 24.74075698852539, \t Total Dis Loss : 0.0012782306876033545\n",
      "Steps : 18400, \t Total Gen Loss : 24.150753021240234, \t Total Dis Loss : 0.0013764696195721626\n",
      "Steps : 18500, \t Total Gen Loss : 24.544845581054688, \t Total Dis Loss : 0.0004909060662612319\n",
      "Steps : 18600, \t Total Gen Loss : 26.108715057373047, \t Total Dis Loss : 0.00019644918211270124\n",
      "Steps : 18700, \t Total Gen Loss : 27.134265899658203, \t Total Dis Loss : 0.001193813281133771\n",
      "Steps : 18800, \t Total Gen Loss : 24.571348190307617, \t Total Dis Loss : 0.000662538455799222\n",
      "Steps : 18900, \t Total Gen Loss : 25.704898834228516, \t Total Dis Loss : 0.00018026307225227356\n",
      "Steps : 19000, \t Total Gen Loss : 26.55010223388672, \t Total Dis Loss : 0.0024811248295009136\n",
      "Steps : 19100, \t Total Gen Loss : 23.005443572998047, \t Total Dis Loss : 0.0007698716945014894\n",
      "Steps : 19200, \t Total Gen Loss : 27.399797439575195, \t Total Dis Loss : 0.0005173484678380191\n",
      "Steps : 19300, \t Total Gen Loss : 25.554889678955078, \t Total Dis Loss : 0.0036920816637575626\n",
      "Steps : 19400, \t Total Gen Loss : 27.519874572753906, \t Total Dis Loss : 0.00013933790614828467\n",
      "Steps : 19500, \t Total Gen Loss : 24.72980308532715, \t Total Dis Loss : 0.000695499824360013\n",
      "Steps : 19600, \t Total Gen Loss : 24.46685028076172, \t Total Dis Loss : 0.0005754366284236312\n",
      "Steps : 19700, \t Total Gen Loss : 32.334354400634766, \t Total Dis Loss : 0.006048551760613918\n",
      "Steps : 19800, \t Total Gen Loss : 27.191211700439453, \t Total Dis Loss : 0.0020730544347316027\n",
      "Steps : 19900, \t Total Gen Loss : 29.277557373046875, \t Total Dis Loss : 0.00038699942524544895\n",
      "Steps : 20000, \t Total Gen Loss : 23.171510696411133, \t Total Dis Loss : 0.027601614594459534\n",
      "Steps : 20100, \t Total Gen Loss : 24.49103546142578, \t Total Dis Loss : 0.00763130746781826\n",
      "Steps : 20200, \t Total Gen Loss : 24.196653366088867, \t Total Dis Loss : 0.0030241094063967466\n",
      "Steps : 20300, \t Total Gen Loss : 24.779903411865234, \t Total Dis Loss : 0.0012557527516037226\n",
      "Steps : 20400, \t Total Gen Loss : 24.473888397216797, \t Total Dis Loss : 0.001830780296586454\n",
      "Steps : 20500, \t Total Gen Loss : 26.189241409301758, \t Total Dis Loss : 0.00047969690058380365\n",
      "Steps : 20600, \t Total Gen Loss : 30.162246704101562, \t Total Dis Loss : 0.0011819436913356185\n",
      "Steps : 20700, \t Total Gen Loss : 28.247846603393555, \t Total Dis Loss : 0.003645113203674555\n",
      "Steps : 20800, \t Total Gen Loss : 27.10140609741211, \t Total Dis Loss : 0.0023531015031039715\n",
      "Steps : 20900, \t Total Gen Loss : 28.3400821685791, \t Total Dis Loss : 0.0013773541431874037\n",
      "Steps : 21000, \t Total Gen Loss : 25.16679573059082, \t Total Dis Loss : 0.0009263003594242036\n",
      "Steps : 21100, \t Total Gen Loss : 27.188106536865234, \t Total Dis Loss : 0.000724851677659899\n",
      "Steps : 21200, \t Total Gen Loss : 25.4067440032959, \t Total Dis Loss : 0.004456237889826298\n",
      "Steps : 21300, \t Total Gen Loss : 22.31191635131836, \t Total Dis Loss : 0.001039316994138062\n",
      "Steps : 21400, \t Total Gen Loss : 25.56300163269043, \t Total Dis Loss : 0.0012176493182778358\n",
      "Steps : 21500, \t Total Gen Loss : 23.580636978149414, \t Total Dis Loss : 0.0062225884757936\n",
      "Steps : 21600, \t Total Gen Loss : 25.14612579345703, \t Total Dis Loss : 0.004820108413696289\n",
      "Steps : 21700, \t Total Gen Loss : 29.145126342773438, \t Total Dis Loss : 0.0009773813653737307\n",
      "Steps : 21800, \t Total Gen Loss : 23.747833251953125, \t Total Dis Loss : 0.03258746489882469\n",
      "Steps : 21900, \t Total Gen Loss : 28.352264404296875, \t Total Dis Loss : 0.0011745166266337037\n",
      "Steps : 22000, \t Total Gen Loss : 24.31903076171875, \t Total Dis Loss : 0.00037926773075014353\n",
      "Steps : 22100, \t Total Gen Loss : 23.479856491088867, \t Total Dis Loss : 0.0008934664074331522\n",
      "Steps : 22200, \t Total Gen Loss : 24.837398529052734, \t Total Dis Loss : 0.0002149106585420668\n",
      "Steps : 22300, \t Total Gen Loss : 25.972726821899414, \t Total Dis Loss : 0.0033028714824467897\n",
      "Steps : 22400, \t Total Gen Loss : 24.049482345581055, \t Total Dis Loss : 0.0010138319339603186\n",
      "Steps : 22500, \t Total Gen Loss : 24.193777084350586, \t Total Dis Loss : 0.0010335610713809729\n",
      "Time for epoch 4 is 250.91477394104004 sec\n",
      "Steps : 22600, \t Total Gen Loss : 23.510576248168945, \t Total Dis Loss : 0.002722225384786725\n",
      "Steps : 22700, \t Total Gen Loss : 24.72803497314453, \t Total Dis Loss : 0.0008003331022337079\n",
      "Steps : 22800, \t Total Gen Loss : 24.427766799926758, \t Total Dis Loss : 0.004027545917779207\n",
      "Steps : 22900, \t Total Gen Loss : 23.271638870239258, \t Total Dis Loss : 0.001086068106815219\n",
      "Steps : 23000, \t Total Gen Loss : 26.03420066833496, \t Total Dis Loss : 0.0008527449681423604\n",
      "Steps : 23100, \t Total Gen Loss : 26.08690071105957, \t Total Dis Loss : 0.0003521714243106544\n",
      "Steps : 23200, \t Total Gen Loss : 26.370407104492188, \t Total Dis Loss : 0.0008152518421411514\n",
      "Steps : 23300, \t Total Gen Loss : 20.67469596862793, \t Total Dis Loss : 0.0115438187494874\n",
      "Steps : 23400, \t Total Gen Loss : 28.55808448791504, \t Total Dis Loss : 0.00040550570702180266\n",
      "Steps : 23500, \t Total Gen Loss : 25.175521850585938, \t Total Dis Loss : 0.04423276707530022\n",
      "Steps : 23600, \t Total Gen Loss : 23.671737670898438, \t Total Dis Loss : 0.005096593406051397\n",
      "Steps : 23700, \t Total Gen Loss : 24.430397033691406, \t Total Dis Loss : 0.0012026544427499175\n",
      "Steps : 23800, \t Total Gen Loss : 28.65174674987793, \t Total Dis Loss : 0.0006170718697831035\n",
      "Steps : 23900, \t Total Gen Loss : 26.932109832763672, \t Total Dis Loss : 0.0009074156987480819\n",
      "Steps : 24000, \t Total Gen Loss : 29.0117130279541, \t Total Dis Loss : 0.000537704792805016\n",
      "Steps : 24100, \t Total Gen Loss : 21.95525360107422, \t Total Dis Loss : 0.004123326390981674\n",
      "Steps : 24200, \t Total Gen Loss : 27.889354705810547, \t Total Dis Loss : 0.002649288857355714\n",
      "Steps : 24300, \t Total Gen Loss : 25.04555892944336, \t Total Dis Loss : 0.006007293704897165\n",
      "Steps : 24400, \t Total Gen Loss : 23.656396865844727, \t Total Dis Loss : 0.005055495537817478\n",
      "Steps : 24500, \t Total Gen Loss : 25.706636428833008, \t Total Dis Loss : 0.0008175170514732599\n",
      "Steps : 24600, \t Total Gen Loss : 25.472511291503906, \t Total Dis Loss : 0.0009261142695322633\n",
      "Steps : 24700, \t Total Gen Loss : 25.269792556762695, \t Total Dis Loss : 0.0011594216339290142\n",
      "Steps : 24800, \t Total Gen Loss : 24.40416145324707, \t Total Dis Loss : 0.0014741732738912106\n",
      "Steps : 24900, \t Total Gen Loss : 25.59495735168457, \t Total Dis Loss : 0.001868384308181703\n",
      "Steps : 25000, \t Total Gen Loss : 28.035980224609375, \t Total Dis Loss : 0.0005995862884446979\n",
      "Steps : 25100, \t Total Gen Loss : 26.323518753051758, \t Total Dis Loss : 0.0013413720298558474\n",
      "Steps : 25200, \t Total Gen Loss : 27.34054946899414, \t Total Dis Loss : 0.0004504908574745059\n",
      "Steps : 25300, \t Total Gen Loss : 28.59811782836914, \t Total Dis Loss : 0.002215137705206871\n",
      "Steps : 25400, \t Total Gen Loss : 26.373912811279297, \t Total Dis Loss : 0.0009586467640474439\n",
      "Steps : 25500, \t Total Gen Loss : 24.180164337158203, \t Total Dis Loss : 0.0007499319617636502\n",
      "Steps : 25600, \t Total Gen Loss : 27.12531089782715, \t Total Dis Loss : 0.0006374187651090324\n",
      "Steps : 25700, \t Total Gen Loss : 26.77814292907715, \t Total Dis Loss : 0.0005637024296447635\n",
      "Steps : 25800, \t Total Gen Loss : 27.573781967163086, \t Total Dis Loss : 0.00040397272096015513\n",
      "Steps : 25900, \t Total Gen Loss : 26.017263412475586, \t Total Dis Loss : 0.001254554372280836\n",
      "Steps : 26000, \t Total Gen Loss : 24.665035247802734, \t Total Dis Loss : 0.0018588376697152853\n",
      "Steps : 26100, \t Total Gen Loss : 27.75227928161621, \t Total Dis Loss : 0.0008368638227693737\n",
      "Steps : 26200, \t Total Gen Loss : 28.467796325683594, \t Total Dis Loss : 0.0006850052159279585\n",
      "Steps : 26300, \t Total Gen Loss : 31.238656997680664, \t Total Dis Loss : 0.0008354116580449045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 26400, \t Total Gen Loss : 26.100521087646484, \t Total Dis Loss : 0.0018775511998683214\n",
      "Steps : 26500, \t Total Gen Loss : 30.35918617248535, \t Total Dis Loss : 0.03422648459672928\n",
      "Steps : 26600, \t Total Gen Loss : 26.602954864501953, \t Total Dis Loss : 0.00043855200055986643\n",
      "Steps : 26700, \t Total Gen Loss : 24.8847599029541, \t Total Dis Loss : 0.0329446941614151\n",
      "Steps : 26800, \t Total Gen Loss : 27.695165634155273, \t Total Dis Loss : 0.0009725734125822783\n",
      "Steps : 26900, \t Total Gen Loss : 28.988088607788086, \t Total Dis Loss : 0.002723292214795947\n",
      "Steps : 27000, \t Total Gen Loss : 25.635316848754883, \t Total Dis Loss : 0.0017817754996940494\n",
      "Steps : 27100, \t Total Gen Loss : 25.116823196411133, \t Total Dis Loss : 0.0054518794640898705\n",
      "Steps : 27200, \t Total Gen Loss : 24.685710906982422, \t Total Dis Loss : 0.002139149932190776\n",
      "Steps : 27300, \t Total Gen Loss : 30.226160049438477, \t Total Dis Loss : 0.0008011008030734956\n",
      "Steps : 27400, \t Total Gen Loss : 32.42326736450195, \t Total Dis Loss : 0.00039893866050988436\n",
      "Steps : 27500, \t Total Gen Loss : 28.00556182861328, \t Total Dis Loss : 0.002001351909711957\n",
      "Steps : 27600, \t Total Gen Loss : 22.624282836914062, \t Total Dis Loss : 0.0013863880885764956\n",
      "Steps : 27700, \t Total Gen Loss : 23.148670196533203, \t Total Dis Loss : 0.0011411774903535843\n",
      "Steps : 27800, \t Total Gen Loss : 24.00372886657715, \t Total Dis Loss : 0.0009557094890624285\n",
      "Steps : 27900, \t Total Gen Loss : 26.579296112060547, \t Total Dis Loss : 0.06203671172261238\n",
      "Steps : 28000, \t Total Gen Loss : 24.746965408325195, \t Total Dis Loss : 0.05248710513114929\n",
      "Steps : 28100, \t Total Gen Loss : 23.830780029296875, \t Total Dis Loss : 0.001889814273454249\n",
      "Time for epoch 5 is 251.7461438179016 sec\n",
      "Steps : 28200, \t Total Gen Loss : 26.10100555419922, \t Total Dis Loss : 0.0005570492940023541\n",
      "Steps : 28300, \t Total Gen Loss : 23.10147476196289, \t Total Dis Loss : 0.0010128030553460121\n",
      "Steps : 28400, \t Total Gen Loss : 24.506061553955078, \t Total Dis Loss : 0.0020967917516827583\n",
      "Steps : 28500, \t Total Gen Loss : 24.813669204711914, \t Total Dis Loss : 0.0005918590468354523\n",
      "Steps : 28600, \t Total Gen Loss : 25.97414207458496, \t Total Dis Loss : 0.000624721113126725\n",
      "Steps : 28700, \t Total Gen Loss : 24.214736938476562, \t Total Dis Loss : 0.0022197291254997253\n",
      "Steps : 28800, \t Total Gen Loss : 25.544527053833008, \t Total Dis Loss : 0.002638306003063917\n",
      "Steps : 28900, \t Total Gen Loss : 26.467105865478516, \t Total Dis Loss : 0.0037320691626518965\n",
      "Steps : 29000, \t Total Gen Loss : 28.554214477539062, \t Total Dis Loss : 0.0050719683058559895\n",
      "Steps : 29100, \t Total Gen Loss : 26.629629135131836, \t Total Dis Loss : 0.0010191495530307293\n",
      "Steps : 29200, \t Total Gen Loss : 25.287429809570312, \t Total Dis Loss : 0.00125811155885458\n",
      "Steps : 29300, \t Total Gen Loss : 26.53392219543457, \t Total Dis Loss : 0.001790939480997622\n",
      "Steps : 29400, \t Total Gen Loss : 25.04644012451172, \t Total Dis Loss : 0.0005863129626959562\n",
      "Steps : 29500, \t Total Gen Loss : 24.903825759887695, \t Total Dis Loss : 0.0004694626550190151\n",
      "Steps : 29600, \t Total Gen Loss : 24.55234718322754, \t Total Dis Loss : 0.12512606382369995\n",
      "Steps : 29700, \t Total Gen Loss : 26.5732421875, \t Total Dis Loss : 0.0009967135265469551\n",
      "Steps : 29800, \t Total Gen Loss : 27.9609375, \t Total Dis Loss : 0.003892267355695367\n",
      "Steps : 29900, \t Total Gen Loss : 25.371339797973633, \t Total Dis Loss : 0.0010684879962354898\n",
      "Steps : 30000, \t Total Gen Loss : 29.48480796813965, \t Total Dis Loss : 0.00019885075744241476\n",
      "Steps : 30100, \t Total Gen Loss : 24.93916893005371, \t Total Dis Loss : 0.0011750349076464772\n",
      "Steps : 30200, \t Total Gen Loss : 23.09059715270996, \t Total Dis Loss : 0.000719878647942096\n",
      "Steps : 30300, \t Total Gen Loss : 26.146038055419922, \t Total Dis Loss : 0.0018986542709171772\n",
      "Steps : 30400, \t Total Gen Loss : 25.831336975097656, \t Total Dis Loss : 0.003815045580267906\n",
      "Steps : 30500, \t Total Gen Loss : 25.550939559936523, \t Total Dis Loss : 0.0008762507350184023\n",
      "Steps : 30600, \t Total Gen Loss : 24.673646926879883, \t Total Dis Loss : 0.0020888596773147583\n",
      "Steps : 30700, \t Total Gen Loss : 23.2479305267334, \t Total Dis Loss : 0.00109403720125556\n",
      "Steps : 30800, \t Total Gen Loss : 25.80889129638672, \t Total Dis Loss : 0.00028027629014104605\n",
      "Steps : 30900, \t Total Gen Loss : 24.522947311401367, \t Total Dis Loss : 0.0012463752645999193\n",
      "Steps : 31000, \t Total Gen Loss : 23.08030128479004, \t Total Dis Loss : 0.0018983874469995499\n",
      "Steps : 31100, \t Total Gen Loss : 25.904130935668945, \t Total Dis Loss : 0.00038311444222927094\n",
      "Steps : 31200, \t Total Gen Loss : 24.474517822265625, \t Total Dis Loss : 0.00018826290033757687\n",
      "Steps : 31300, \t Total Gen Loss : 29.687421798706055, \t Total Dis Loss : 0.0009060974698513746\n",
      "Steps : 31400, \t Total Gen Loss : 28.84942054748535, \t Total Dis Loss : 0.003161435481160879\n",
      "Steps : 31500, \t Total Gen Loss : 25.784656524658203, \t Total Dis Loss : 0.003272460075095296\n",
      "Steps : 31600, \t Total Gen Loss : 28.03646469116211, \t Total Dis Loss : 9.083530312636867e-05\n",
      "Steps : 31700, \t Total Gen Loss : 26.207656860351562, \t Total Dis Loss : 0.007379796355962753\n",
      "Steps : 31800, \t Total Gen Loss : 26.50576400756836, \t Total Dis Loss : 0.0009819779079407454\n",
      "Steps : 31900, \t Total Gen Loss : 26.630416870117188, \t Total Dis Loss : 0.000576562830246985\n",
      "Steps : 32000, \t Total Gen Loss : 27.290544509887695, \t Total Dis Loss : 0.0007507127011194825\n",
      "Steps : 32100, \t Total Gen Loss : 25.93364906311035, \t Total Dis Loss : 0.0009532928233966231\n",
      "Steps : 32200, \t Total Gen Loss : 26.44239044189453, \t Total Dis Loss : 0.0007411710103042424\n",
      "Steps : 32300, \t Total Gen Loss : 25.095518112182617, \t Total Dis Loss : 0.0006436379044316709\n",
      "Steps : 32400, \t Total Gen Loss : 27.676025390625, \t Total Dis Loss : 0.00044387936941348016\n",
      "Steps : 32500, \t Total Gen Loss : 27.164249420166016, \t Total Dis Loss : 0.0010719024576246738\n",
      "Steps : 32600, \t Total Gen Loss : 29.817968368530273, \t Total Dis Loss : 0.00030069614876993\n",
      "Steps : 32700, \t Total Gen Loss : 25.278059005737305, \t Total Dis Loss : 0.0016778638819232583\n",
      "Steps : 32800, \t Total Gen Loss : 24.60582160949707, \t Total Dis Loss : 0.0007296472322195768\n",
      "Steps : 32900, \t Total Gen Loss : 22.546302795410156, \t Total Dis Loss : 0.003417222760617733\n",
      "Steps : 33000, \t Total Gen Loss : 26.146839141845703, \t Total Dis Loss : 0.0005167317576706409\n",
      "Steps : 33100, \t Total Gen Loss : 20.649391174316406, \t Total Dis Loss : 0.001427455572411418\n",
      "Steps : 33200, \t Total Gen Loss : 25.45067024230957, \t Total Dis Loss : 0.0018875112291425467\n",
      "Steps : 33300, \t Total Gen Loss : 24.464202880859375, \t Total Dis Loss : 0.000723083212506026\n",
      "Steps : 33400, \t Total Gen Loss : 24.486173629760742, \t Total Dis Loss : 0.001628081314265728\n",
      "Steps : 33500, \t Total Gen Loss : 22.65342903137207, \t Total Dis Loss : 0.0010873032733798027\n",
      "Steps : 33600, \t Total Gen Loss : 25.905345916748047, \t Total Dis Loss : 0.000582114327698946\n",
      "Steps : 33700, \t Total Gen Loss : 23.330106735229492, \t Total Dis Loss : 0.0004849152755923569\n",
      "Time for epoch 6 is 256.521372795105 sec\n",
      "Steps : 33800, \t Total Gen Loss : 29.123424530029297, \t Total Dis Loss : 0.009632558561861515\n",
      "Steps : 33900, \t Total Gen Loss : 25.896886825561523, \t Total Dis Loss : 0.0008353424491360784\n",
      "Steps : 34000, \t Total Gen Loss : 26.823917388916016, \t Total Dis Loss : 0.2501858174800873\n",
      "Steps : 34100, \t Total Gen Loss : 25.535594940185547, \t Total Dis Loss : 0.013243026100099087\n",
      "Steps : 34200, \t Total Gen Loss : 26.950918197631836, \t Total Dis Loss : 0.000542570254765451\n",
      "Steps : 34300, \t Total Gen Loss : 27.577884674072266, \t Total Dis Loss : 0.0014372104778885841\n",
      "Steps : 34400, \t Total Gen Loss : 26.281835556030273, \t Total Dis Loss : 0.0005754074081778526\n",
      "Steps : 34500, \t Total Gen Loss : 25.77935791015625, \t Total Dis Loss : 0.0011175511172041297\n",
      "Steps : 34600, \t Total Gen Loss : 26.766494750976562, \t Total Dis Loss : 0.0008623782778158784\n",
      "Steps : 34700, \t Total Gen Loss : 25.158180236816406, \t Total Dis Loss : 0.003535341238602996\n",
      "Steps : 34800, \t Total Gen Loss : 27.88917350769043, \t Total Dis Loss : 0.010078308172523975\n",
      "Steps : 34900, \t Total Gen Loss : 24.098276138305664, \t Total Dis Loss : 0.0015364092541858554\n",
      "Steps : 35000, \t Total Gen Loss : 28.652910232543945, \t Total Dis Loss : 0.00048756253090687096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 35100, \t Total Gen Loss : 26.305387496948242, \t Total Dis Loss : 0.00022861844627186656\n",
      "Steps : 35200, \t Total Gen Loss : 22.52717399597168, \t Total Dis Loss : 0.0003082270850427449\n",
      "Steps : 35300, \t Total Gen Loss : 28.591697692871094, \t Total Dis Loss : 0.0001731092343106866\n",
      "Steps : 35400, \t Total Gen Loss : 26.94941520690918, \t Total Dis Loss : 0.0005431465106084943\n",
      "Steps : 35500, \t Total Gen Loss : 26.011672973632812, \t Total Dis Loss : 0.0007620514370501041\n",
      "Steps : 35600, \t Total Gen Loss : 25.15285873413086, \t Total Dis Loss : 0.00018829107284545898\n",
      "Steps : 35700, \t Total Gen Loss : 26.564132690429688, \t Total Dis Loss : 0.0011070292675867677\n",
      "Steps : 35800, \t Total Gen Loss : 27.739219665527344, \t Total Dis Loss : 0.0005703965434804559\n",
      "Steps : 35900, \t Total Gen Loss : 28.958759307861328, \t Total Dis Loss : 0.0007732524536550045\n",
      "Steps : 36000, \t Total Gen Loss : 27.4603271484375, \t Total Dis Loss : 0.0054631540551781654\n",
      "Steps : 36100, \t Total Gen Loss : 24.055461883544922, \t Total Dis Loss : 0.002714628353714943\n",
      "Steps : 36200, \t Total Gen Loss : 27.69676399230957, \t Total Dis Loss : 0.002531857695430517\n",
      "Steps : 36300, \t Total Gen Loss : 20.140308380126953, \t Total Dis Loss : 0.00602497486397624\n",
      "Steps : 36400, \t Total Gen Loss : 28.596086502075195, \t Total Dis Loss : 0.00029467465355992317\n",
      "Steps : 36500, \t Total Gen Loss : 24.60747528076172, \t Total Dis Loss : 0.0009974773274734616\n",
      "Steps : 36600, \t Total Gen Loss : 21.05354881286621, \t Total Dis Loss : 0.001640806789509952\n",
      "Steps : 36700, \t Total Gen Loss : 25.472749710083008, \t Total Dis Loss : 0.0004695304378401488\n",
      "Steps : 36800, \t Total Gen Loss : 24.99142837524414, \t Total Dis Loss : 0.0005642505711875856\n",
      "Steps : 36900, \t Total Gen Loss : 26.66147804260254, \t Total Dis Loss : 0.0010369541123509407\n",
      "Steps : 37000, \t Total Gen Loss : 23.09150505065918, \t Total Dis Loss : 0.0009837455581873655\n",
      "Steps : 37100, \t Total Gen Loss : 23.84234046936035, \t Total Dis Loss : 0.0003378349938429892\n",
      "Steps : 37200, \t Total Gen Loss : 25.663192749023438, \t Total Dis Loss : 0.0005533293588086963\n",
      "Steps : 37300, \t Total Gen Loss : 23.74534797668457, \t Total Dis Loss : 0.0005397921777330339\n",
      "Steps : 37400, \t Total Gen Loss : 22.908679962158203, \t Total Dis Loss : 0.000489163096062839\n",
      "Steps : 37500, \t Total Gen Loss : 26.972009658813477, \t Total Dis Loss : 0.0006683574756607413\n",
      "Steps : 37600, \t Total Gen Loss : 23.891130447387695, \t Total Dis Loss : 0.0005017667426727712\n",
      "Steps : 37700, \t Total Gen Loss : 24.036352157592773, \t Total Dis Loss : 0.0024423308204859495\n",
      "Steps : 37800, \t Total Gen Loss : 28.441099166870117, \t Total Dis Loss : 0.0003987719537690282\n",
      "Steps : 37900, \t Total Gen Loss : 26.341930389404297, \t Total Dis Loss : 0.00011529534822329879\n",
      "Steps : 38000, \t Total Gen Loss : 23.730741500854492, \t Total Dis Loss : 0.001381345558911562\n",
      "Steps : 38100, \t Total Gen Loss : 26.433391571044922, \t Total Dis Loss : 0.00021846831077709794\n",
      "Steps : 38200, \t Total Gen Loss : 26.50059700012207, \t Total Dis Loss : 0.0006922468892298639\n",
      "Steps : 38300, \t Total Gen Loss : 25.92466926574707, \t Total Dis Loss : 0.00024316586495842785\n",
      "Steps : 38400, \t Total Gen Loss : 23.877622604370117, \t Total Dis Loss : 0.00030714942840859294\n",
      "Steps : 38500, \t Total Gen Loss : 26.037330627441406, \t Total Dis Loss : 0.00034506776137277484\n",
      "Steps : 38600, \t Total Gen Loss : 24.430923461914062, \t Total Dis Loss : 0.0029457223135977983\n",
      "Steps : 38700, \t Total Gen Loss : 24.919279098510742, \t Total Dis Loss : 0.0010639221873134375\n",
      "Steps : 38800, \t Total Gen Loss : 24.06195831298828, \t Total Dis Loss : 0.0004951734445057809\n",
      "Steps : 38900, \t Total Gen Loss : 29.02634620666504, \t Total Dis Loss : 4.900130443274975e-05\n",
      "Steps : 39000, \t Total Gen Loss : 27.47926139831543, \t Total Dis Loss : 0.006132713984698057\n",
      "Steps : 39100, \t Total Gen Loss : 24.02804183959961, \t Total Dis Loss : 0.003251004032790661\n",
      "Steps : 39200, \t Total Gen Loss : 26.53185272216797, \t Total Dis Loss : 0.0034237366635352373\n",
      "Steps : 39300, \t Total Gen Loss : 27.61598014831543, \t Total Dis Loss : 0.0007892420399002731\n",
      "Time for epoch 7 is 254.85976719856262 sec\n",
      "Steps : 39400, \t Total Gen Loss : 30.123197555541992, \t Total Dis Loss : 0.27401211857795715\n",
      "Steps : 39500, \t Total Gen Loss : 29.990625381469727, \t Total Dis Loss : 0.000143976736580953\n",
      "Steps : 39600, \t Total Gen Loss : 30.466882705688477, \t Total Dis Loss : 0.00033002562122419477\n",
      "Steps : 39700, \t Total Gen Loss : 27.596540451049805, \t Total Dis Loss : 0.0008315541781485081\n",
      "Steps : 39800, \t Total Gen Loss : 26.05615234375, \t Total Dis Loss : 0.00039178741280920804\n",
      "Steps : 39900, \t Total Gen Loss : 25.631799697875977, \t Total Dis Loss : 0.00032822691719047725\n",
      "Steps : 40000, \t Total Gen Loss : 23.21578598022461, \t Total Dis Loss : 0.0014675239799544215\n",
      "Steps : 40100, \t Total Gen Loss : 26.316177368164062, \t Total Dis Loss : 0.0003675610350910574\n",
      "Steps : 40200, \t Total Gen Loss : 26.2132511138916, \t Total Dis Loss : 0.0011480676475912333\n",
      "Steps : 40300, \t Total Gen Loss : 24.263004302978516, \t Total Dis Loss : 0.00023255872656591237\n",
      "Steps : 40400, \t Total Gen Loss : 23.180400848388672, \t Total Dis Loss : 0.0005106262979097664\n",
      "Steps : 40500, \t Total Gen Loss : 24.10323143005371, \t Total Dis Loss : 0.001076033804565668\n",
      "Steps : 40600, \t Total Gen Loss : 25.606510162353516, \t Total Dis Loss : 0.04798474162817001\n",
      "Steps : 40700, \t Total Gen Loss : 25.381620407104492, \t Total Dis Loss : 0.0005321287317201495\n",
      "Steps : 40800, \t Total Gen Loss : 26.392032623291016, \t Total Dis Loss : 0.0016455757431685925\n",
      "Steps : 40900, \t Total Gen Loss : 26.466413497924805, \t Total Dis Loss : 0.000293504330329597\n",
      "Steps : 41000, \t Total Gen Loss : 24.409767150878906, \t Total Dis Loss : 0.0004169139720033854\n",
      "Steps : 41100, \t Total Gen Loss : 25.203222274780273, \t Total Dis Loss : 0.09792076796293259\n",
      "Steps : 41200, \t Total Gen Loss : 28.694412231445312, \t Total Dis Loss : 0.0006522807525470853\n",
      "Steps : 41300, \t Total Gen Loss : 25.05218505859375, \t Total Dis Loss : 0.0068709468469023705\n",
      "Steps : 41400, \t Total Gen Loss : 29.89708137512207, \t Total Dis Loss : 0.00011926650768145919\n",
      "Steps : 41500, \t Total Gen Loss : 30.237606048583984, \t Total Dis Loss : 0.0006866154726594687\n",
      "Steps : 41600, \t Total Gen Loss : 30.107933044433594, \t Total Dis Loss : 4.469823034014553e-05\n",
      "Steps : 41700, \t Total Gen Loss : 27.519704818725586, \t Total Dis Loss : 0.00610190536826849\n",
      "Steps : 41800, \t Total Gen Loss : 24.045745849609375, \t Total Dis Loss : 0.0010537442285567522\n",
      "Steps : 41900, \t Total Gen Loss : 27.547897338867188, \t Total Dis Loss : 0.003118599532172084\n",
      "Steps : 42000, \t Total Gen Loss : 29.57086181640625, \t Total Dis Loss : 0.0036959517747163773\n",
      "Steps : 42100, \t Total Gen Loss : 28.38913917541504, \t Total Dis Loss : 0.0005935955559834838\n",
      "Steps : 42200, \t Total Gen Loss : 32.585594177246094, \t Total Dis Loss : 0.002989684697240591\n",
      "Steps : 42300, \t Total Gen Loss : 28.432432174682617, \t Total Dis Loss : 0.011394607834517956\n",
      "Steps : 42400, \t Total Gen Loss : 26.477914810180664, \t Total Dis Loss : 0.00047670898493379354\n",
      "Steps : 42500, \t Total Gen Loss : 29.94629669189453, \t Total Dis Loss : 0.018063684925436974\n",
      "Steps : 42600, \t Total Gen Loss : 28.502696990966797, \t Total Dis Loss : 0.0005138078704476357\n",
      "Steps : 42700, \t Total Gen Loss : 28.665149688720703, \t Total Dis Loss : 0.002182631753385067\n",
      "Steps : 42800, \t Total Gen Loss : 28.30524253845215, \t Total Dis Loss : 0.0031495485454797745\n",
      "Steps : 42900, \t Total Gen Loss : 26.79848861694336, \t Total Dis Loss : 0.00988129060715437\n",
      "Steps : 43000, \t Total Gen Loss : 28.47918701171875, \t Total Dis Loss : 0.008042335510253906\n",
      "Steps : 43100, \t Total Gen Loss : 26.87429428100586, \t Total Dis Loss : 0.00394099997356534\n",
      "Steps : 43200, \t Total Gen Loss : 28.099027633666992, \t Total Dis Loss : 0.0005371677689254284\n",
      "Steps : 43300, \t Total Gen Loss : 27.538156509399414, \t Total Dis Loss : 0.0004412437556311488\n",
      "Steps : 43400, \t Total Gen Loss : 26.55651092529297, \t Total Dis Loss : 0.0009282796527259052\n",
      "Steps : 43500, \t Total Gen Loss : 25.3364315032959, \t Total Dis Loss : 0.057606011629104614\n",
      "Steps : 43600, \t Total Gen Loss : 24.431245803833008, \t Total Dis Loss : 0.09194126725196838\n",
      "Steps : 43700, \t Total Gen Loss : 26.632293701171875, \t Total Dis Loss : 0.0013471459969878197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 43800, \t Total Gen Loss : 26.33578872680664, \t Total Dis Loss : 0.0005664181080646813\n",
      "Steps : 43900, \t Total Gen Loss : 28.7326717376709, \t Total Dis Loss : 0.00019209226593375206\n",
      "Steps : 44000, \t Total Gen Loss : 29.695560455322266, \t Total Dis Loss : 0.0002855970524251461\n",
      "Steps : 44100, \t Total Gen Loss : 29.16278648376465, \t Total Dis Loss : 0.00034167018020525575\n",
      "Steps : 44200, \t Total Gen Loss : 27.22183609008789, \t Total Dis Loss : 0.001742927823215723\n",
      "Steps : 44300, \t Total Gen Loss : 26.077993392944336, \t Total Dis Loss : 0.0005101317074149847\n",
      "Steps : 44400, \t Total Gen Loss : 25.10308265686035, \t Total Dis Loss : 0.0026698021683841944\n",
      "Steps : 44500, \t Total Gen Loss : 26.81296157836914, \t Total Dis Loss : 0.0002499778347555548\n",
      "Steps : 44600, \t Total Gen Loss : 27.945276260375977, \t Total Dis Loss : 0.00029068876756355166\n",
      "Steps : 44700, \t Total Gen Loss : 26.727954864501953, \t Total Dis Loss : 0.0008640224696137011\n",
      "Steps : 44800, \t Total Gen Loss : 25.532899856567383, \t Total Dis Loss : 0.00018299427756574005\n",
      "Steps : 44900, \t Total Gen Loss : 26.35042381286621, \t Total Dis Loss : 0.0005701193585991859\n",
      "Steps : 45000, \t Total Gen Loss : 26.701074600219727, \t Total Dis Loss : 0.0006223423406481743\n",
      "Time for epoch 8 is 254.99215292930603 sec\n",
      "Steps : 45100, \t Total Gen Loss : 26.443634033203125, \t Total Dis Loss : 0.00014660158194601536\n",
      "Steps : 45200, \t Total Gen Loss : 26.208938598632812, \t Total Dis Loss : 0.0005590417422354221\n",
      "Steps : 45300, \t Total Gen Loss : 28.084260940551758, \t Total Dis Loss : 0.0005471028853207827\n",
      "Steps : 45400, \t Total Gen Loss : 26.385494232177734, \t Total Dis Loss : 0.00013943409430794418\n",
      "Steps : 45500, \t Total Gen Loss : 26.7750244140625, \t Total Dis Loss : 0.00046975124860182405\n",
      "Steps : 45600, \t Total Gen Loss : 25.65077018737793, \t Total Dis Loss : 0.004829805810004473\n",
      "Steps : 45700, \t Total Gen Loss : 27.258092880249023, \t Total Dis Loss : 0.0029667015187442303\n",
      "Steps : 45800, \t Total Gen Loss : 23.192655563354492, \t Total Dis Loss : 0.0024803022388368845\n",
      "Steps : 45900, \t Total Gen Loss : 24.103595733642578, \t Total Dis Loss : 0.0006145940278656781\n",
      "Steps : 46000, \t Total Gen Loss : 26.077138900756836, \t Total Dis Loss : 0.0003775141085498035\n",
      "Steps : 46100, \t Total Gen Loss : 23.80541229248047, \t Total Dis Loss : 0.00015921668091323227\n",
      "Steps : 46200, \t Total Gen Loss : 26.56468391418457, \t Total Dis Loss : 0.0003224306274205446\n",
      "Steps : 46300, \t Total Gen Loss : 25.419221878051758, \t Total Dis Loss : 0.00036767570418305695\n",
      "Steps : 46400, \t Total Gen Loss : 22.99077606201172, \t Total Dis Loss : 0.00039048335747793317\n",
      "Steps : 46500, \t Total Gen Loss : 23.796537399291992, \t Total Dis Loss : 0.0001454519951948896\n",
      "Steps : 46600, \t Total Gen Loss : 27.267841339111328, \t Total Dis Loss : 0.0014351025456562638\n",
      "Steps : 46700, \t Total Gen Loss : 27.315303802490234, \t Total Dis Loss : 0.00019341187726240605\n",
      "Steps : 46800, \t Total Gen Loss : 25.76399040222168, \t Total Dis Loss : 5.574449460254982e-05\n",
      "Steps : 46900, \t Total Gen Loss : 25.58335304260254, \t Total Dis Loss : 0.0008493734640069306\n",
      "Steps : 47000, \t Total Gen Loss : 26.373476028442383, \t Total Dis Loss : 0.00037169799907132983\n",
      "Steps : 47100, \t Total Gen Loss : 27.855979919433594, \t Total Dis Loss : 0.00031715800287202\n",
      "Steps : 47200, \t Total Gen Loss : 28.557971954345703, \t Total Dis Loss : 0.005039341282099485\n",
      "Steps : 47300, \t Total Gen Loss : 25.121257781982422, \t Total Dis Loss : 0.00023314887948799878\n",
      "Steps : 47400, \t Total Gen Loss : 23.038570404052734, \t Total Dis Loss : 0.012146313674747944\n",
      "Steps : 47500, \t Total Gen Loss : 22.791858673095703, \t Total Dis Loss : 0.002189852762967348\n",
      "Steps : 47600, \t Total Gen Loss : 25.383590698242188, \t Total Dis Loss : 4.491143045015633e-05\n",
      "Steps : 47700, \t Total Gen Loss : 24.21102523803711, \t Total Dis Loss : 0.0002861686807591468\n",
      "Steps : 47800, \t Total Gen Loss : 26.74415397644043, \t Total Dis Loss : 0.0005512358038686216\n",
      "Steps : 47900, \t Total Gen Loss : 23.30388641357422, \t Total Dis Loss : 0.00023517204681411386\n",
      "Steps : 48000, \t Total Gen Loss : 23.94803810119629, \t Total Dis Loss : 0.0009818361140787601\n",
      "Steps : 48100, \t Total Gen Loss : 23.680557250976562, \t Total Dis Loss : 0.00036434593494050205\n",
      "Steps : 48200, \t Total Gen Loss : 24.342397689819336, \t Total Dis Loss : 0.003291165689006448\n",
      "Steps : 48300, \t Total Gen Loss : 22.26819610595703, \t Total Dis Loss : 0.0011650064261630177\n",
      "Steps : 48400, \t Total Gen Loss : 24.957698822021484, \t Total Dis Loss : 0.00039233046118170023\n",
      "Steps : 48500, \t Total Gen Loss : 23.445100784301758, \t Total Dis Loss : 0.0007981267990544438\n",
      "Steps : 48600, \t Total Gen Loss : 25.638042449951172, \t Total Dis Loss : 0.0023293739650398493\n",
      "Steps : 48700, \t Total Gen Loss : 22.142732620239258, \t Total Dis Loss : 0.0005539277917705476\n",
      "Steps : 48800, \t Total Gen Loss : 23.36956024169922, \t Total Dis Loss : 0.0002838598738890141\n",
      "Steps : 48900, \t Total Gen Loss : 25.008638381958008, \t Total Dis Loss : 0.00011307428212603554\n",
      "Steps : 49000, \t Total Gen Loss : 22.315650939941406, \t Total Dis Loss : 0.0001410484837833792\n",
      "Steps : 49100, \t Total Gen Loss : 22.23651123046875, \t Total Dis Loss : 0.08387769013643265\n",
      "Steps : 49200, \t Total Gen Loss : 27.827505111694336, \t Total Dis Loss : 0.0002719619369599968\n",
      "Steps : 49300, \t Total Gen Loss : 33.9224853515625, \t Total Dis Loss : 0.03398459777235985\n",
      "Steps : 49400, \t Total Gen Loss : 28.25523567199707, \t Total Dis Loss : 0.0006540829199366271\n",
      "Steps : 49500, \t Total Gen Loss : 26.747364044189453, \t Total Dis Loss : 0.0014800782082602382\n",
      "Steps : 49600, \t Total Gen Loss : 25.969648361206055, \t Total Dis Loss : 0.03442754969000816\n",
      "Steps : 49700, \t Total Gen Loss : 29.681304931640625, \t Total Dis Loss : 0.00019337149569764733\n",
      "Steps : 49800, \t Total Gen Loss : 30.428146362304688, \t Total Dis Loss : 0.0006550320540554821\n",
      "Steps : 49900, \t Total Gen Loss : 24.883480072021484, \t Total Dis Loss : 0.00038705868064425886\n",
      "Steps : 50000, \t Total Gen Loss : 28.078638076782227, \t Total Dis Loss : 0.0002540260902605951\n",
      "Steps : 50100, \t Total Gen Loss : 27.429359436035156, \t Total Dis Loss : 0.002025741385295987\n",
      "Steps : 50200, \t Total Gen Loss : 26.218263626098633, \t Total Dis Loss : 0.00030594199779443443\n",
      "Steps : 50300, \t Total Gen Loss : 26.89634895324707, \t Total Dis Loss : 0.00035406797542236745\n",
      "Steps : 50400, \t Total Gen Loss : 24.731958389282227, \t Total Dis Loss : 0.00041504716500639915\n",
      "Steps : 50500, \t Total Gen Loss : 26.788066864013672, \t Total Dis Loss : 0.0015519220614805818\n",
      "Steps : 50600, \t Total Gen Loss : 23.597991943359375, \t Total Dis Loss : 0.0012868455378338695\n",
      "Time for epoch 9 is 255.91239094734192 sec\n",
      "Steps : 50700, \t Total Gen Loss : 27.042821884155273, \t Total Dis Loss : 0.00032344533246941864\n",
      "Steps : 50800, \t Total Gen Loss : 26.944433212280273, \t Total Dis Loss : 0.0006533433333970606\n",
      "Steps : 50900, \t Total Gen Loss : 25.522600173950195, \t Total Dis Loss : 0.0003857904812321067\n",
      "Steps : 51000, \t Total Gen Loss : 25.93642807006836, \t Total Dis Loss : 0.00015425866877194494\n",
      "Steps : 51100, \t Total Gen Loss : 28.102819442749023, \t Total Dis Loss : 0.0003689681470859796\n",
      "Steps : 51200, \t Total Gen Loss : 26.161306381225586, \t Total Dis Loss : 0.0017464798875153065\n",
      "Steps : 51300, \t Total Gen Loss : 27.38857650756836, \t Total Dis Loss : 0.0003142576024401933\n",
      "Steps : 51400, \t Total Gen Loss : 25.736230850219727, \t Total Dis Loss : 0.00011918960080947727\n",
      "Steps : 51500, \t Total Gen Loss : 23.376813888549805, \t Total Dis Loss : 0.0037856586277484894\n",
      "Steps : 51600, \t Total Gen Loss : 25.089134216308594, \t Total Dis Loss : 0.001256507937796414\n",
      "Steps : 51700, \t Total Gen Loss : 26.151193618774414, \t Total Dis Loss : 0.00027237270842306316\n",
      "Steps : 51800, \t Total Gen Loss : 28.493234634399414, \t Total Dis Loss : 0.0004821766051463783\n",
      "Steps : 51900, \t Total Gen Loss : 24.183988571166992, \t Total Dis Loss : 0.0005097034154459834\n",
      "Steps : 52000, \t Total Gen Loss : 23.649356842041016, \t Total Dis Loss : 0.0006714657065458596\n",
      "Steps : 52100, \t Total Gen Loss : 28.403383255004883, \t Total Dis Loss : 0.00017819192726165056\n",
      "Steps : 52200, \t Total Gen Loss : 26.39983558654785, \t Total Dis Loss : 0.0008083176799118519\n",
      "Steps : 52300, \t Total Gen Loss : 27.096294403076172, \t Total Dis Loss : 0.29173094034194946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 52400, \t Total Gen Loss : 27.311729431152344, \t Total Dis Loss : 0.0005883705453015864\n",
      "Steps : 52500, \t Total Gen Loss : 25.06689453125, \t Total Dis Loss : 0.0002714730508159846\n",
      "Steps : 52600, \t Total Gen Loss : 29.135051727294922, \t Total Dis Loss : 0.00021089005167596042\n",
      "Steps : 52700, \t Total Gen Loss : 26.151962280273438, \t Total Dis Loss : 0.013136306777596474\n",
      "Steps : 52800, \t Total Gen Loss : 26.037080764770508, \t Total Dis Loss : 0.00010655514779500663\n",
      "Steps : 52900, \t Total Gen Loss : 23.92413330078125, \t Total Dis Loss : 0.00047175417421385646\n",
      "Steps : 53000, \t Total Gen Loss : 29.362899780273438, \t Total Dis Loss : 0.0009130330290645361\n",
      "Steps : 53100, \t Total Gen Loss : 28.695213317871094, \t Total Dis Loss : 0.01080428808927536\n",
      "Steps : 53200, \t Total Gen Loss : 26.033363342285156, \t Total Dis Loss : 0.008837820030748844\n",
      "Steps : 53300, \t Total Gen Loss : 23.57972526550293, \t Total Dis Loss : 0.0008505221921950579\n",
      "Steps : 53400, \t Total Gen Loss : 30.081981658935547, \t Total Dis Loss : 0.0001359747548121959\n",
      "Steps : 53500, \t Total Gen Loss : 28.532485961914062, \t Total Dis Loss : 0.0003158280742354691\n",
      "Steps : 53600, \t Total Gen Loss : 26.296981811523438, \t Total Dis Loss : 0.0009644153760746121\n",
      "Steps : 53700, \t Total Gen Loss : 27.723575592041016, \t Total Dis Loss : 0.0009201921056956053\n",
      "Steps : 53800, \t Total Gen Loss : 27.41264533996582, \t Total Dis Loss : 0.0010817426955327392\n",
      "Steps : 53900, \t Total Gen Loss : 26.694293975830078, \t Total Dis Loss : 0.0004581885877996683\n",
      "Steps : 54000, \t Total Gen Loss : 25.960012435913086, \t Total Dis Loss : 0.00021291867597028613\n",
      "Steps : 54100, \t Total Gen Loss : 22.96685028076172, \t Total Dis Loss : 0.9268603920936584\n",
      "Steps : 54200, \t Total Gen Loss : 24.47003936767578, \t Total Dis Loss : 0.0009174885344691575\n",
      "Steps : 54300, \t Total Gen Loss : 27.439626693725586, \t Total Dis Loss : 0.0009059059084393084\n",
      "Steps : 54400, \t Total Gen Loss : 24.895824432373047, \t Total Dis Loss : 0.0010853587882593274\n",
      "Steps : 54500, \t Total Gen Loss : 23.50988006591797, \t Total Dis Loss : 0.0007889883709140122\n",
      "Steps : 54600, \t Total Gen Loss : 25.31251335144043, \t Total Dis Loss : 0.0007409051759168506\n",
      "Steps : 54700, \t Total Gen Loss : 24.66990852355957, \t Total Dis Loss : 0.00041810411494225264\n",
      "Steps : 54800, \t Total Gen Loss : 25.887731552124023, \t Total Dis Loss : 0.0001943430834216997\n",
      "Steps : 54900, \t Total Gen Loss : 26.344911575317383, \t Total Dis Loss : 0.00020972019410692155\n",
      "Steps : 55000, \t Total Gen Loss : 28.328563690185547, \t Total Dis Loss : 0.00017261187895201147\n",
      "Steps : 55100, \t Total Gen Loss : 23.03257179260254, \t Total Dis Loss : 0.0005329467821866274\n",
      "Steps : 55200, \t Total Gen Loss : 28.182655334472656, \t Total Dis Loss : 0.00024351399042643607\n",
      "Steps : 55300, \t Total Gen Loss : 25.028400421142578, \t Total Dis Loss : 0.0008102463325485587\n",
      "Steps : 55400, \t Total Gen Loss : 27.14106559753418, \t Total Dis Loss : 0.0002873567573260516\n",
      "Steps : 55500, \t Total Gen Loss : 28.04383659362793, \t Total Dis Loss : 0.00011301337508484721\n",
      "Steps : 55600, \t Total Gen Loss : 25.45561408996582, \t Total Dis Loss : 0.0006167449173517525\n",
      "Steps : 55700, \t Total Gen Loss : 24.555097579956055, \t Total Dis Loss : 0.00028678023954853415\n",
      "Steps : 55800, \t Total Gen Loss : 26.099515914916992, \t Total Dis Loss : 0.00021930245566181839\n",
      "Steps : 55900, \t Total Gen Loss : 29.55376625061035, \t Total Dis Loss : 0.00014070280303712934\n",
      "Steps : 56000, \t Total Gen Loss : 25.135906219482422, \t Total Dis Loss : 0.0005638654110953212\n",
      "Steps : 56100, \t Total Gen Loss : 24.266355514526367, \t Total Dis Loss : 0.00030555660487152636\n",
      "Steps : 56200, \t Total Gen Loss : 26.130033493041992, \t Total Dis Loss : 0.00013334228424355388\n",
      "Time for epoch 10 is 259.75484919548035 sec\n",
      "Steps : 56300, \t Total Gen Loss : 24.858877182006836, \t Total Dis Loss : 0.0001156693761004135\n",
      "Steps : 56400, \t Total Gen Loss : 26.71748924255371, \t Total Dis Loss : 0.00010268681944580749\n",
      "Steps : 56500, \t Total Gen Loss : 26.779979705810547, \t Total Dis Loss : 0.00025047690724022686\n",
      "Steps : 56600, \t Total Gen Loss : 24.738059997558594, \t Total Dis Loss : 0.00022946058015804738\n",
      "Steps : 56700, \t Total Gen Loss : 28.41707992553711, \t Total Dis Loss : 0.00015949932276271284\n",
      "Steps : 56800, \t Total Gen Loss : 28.25351333618164, \t Total Dis Loss : 0.0002529306511860341\n",
      "Steps : 56900, \t Total Gen Loss : 32.03226089477539, \t Total Dis Loss : 4.897967301076278e-05\n",
      "Steps : 57000, \t Total Gen Loss : 26.802696228027344, \t Total Dis Loss : 0.0001423342910129577\n",
      "Steps : 57100, \t Total Gen Loss : 28.74176597595215, \t Total Dis Loss : 3.557421587174758e-05\n",
      "Steps : 57200, \t Total Gen Loss : 27.42070198059082, \t Total Dis Loss : 9.958432201528922e-05\n",
      "Steps : 57300, \t Total Gen Loss : 26.9362735748291, \t Total Dis Loss : 8.450557652395219e-05\n",
      "Steps : 57400, \t Total Gen Loss : 26.290082931518555, \t Total Dis Loss : 0.0004534221370704472\n",
      "Steps : 57500, \t Total Gen Loss : 25.777835845947266, \t Total Dis Loss : 0.00019049402908422053\n",
      "Steps : 57600, \t Total Gen Loss : 25.127824783325195, \t Total Dis Loss : 0.00015859745326451957\n",
      "Steps : 57700, \t Total Gen Loss : 24.98033905029297, \t Total Dis Loss : 0.0001214992007589899\n",
      "Steps : 57800, \t Total Gen Loss : 25.042625427246094, \t Total Dis Loss : 0.00013995968038216233\n",
      "Steps : 57900, \t Total Gen Loss : 27.065757751464844, \t Total Dis Loss : 0.0009691411978565156\n",
      "Steps : 58000, \t Total Gen Loss : 26.23831558227539, \t Total Dis Loss : 0.00011145615280838683\n",
      "Steps : 58100, \t Total Gen Loss : 27.525352478027344, \t Total Dis Loss : 0.00023111500195227563\n",
      "Steps : 58200, \t Total Gen Loss : 25.48680305480957, \t Total Dis Loss : 0.00012998135935049504\n",
      "Steps : 58300, \t Total Gen Loss : 27.497575759887695, \t Total Dis Loss : 6.257969653233886e-05\n",
      "Steps : 58400, \t Total Gen Loss : 25.823837280273438, \t Total Dis Loss : 0.0003047320060431957\n",
      "Steps : 58500, \t Total Gen Loss : 25.551891326904297, \t Total Dis Loss : 0.00023751487606205046\n",
      "Steps : 58600, \t Total Gen Loss : 27.599218368530273, \t Total Dis Loss : 8.53415040182881e-05\n",
      "Steps : 58700, \t Total Gen Loss : 27.3181095123291, \t Total Dis Loss : 0.003746617119759321\n",
      "Steps : 58800, \t Total Gen Loss : 27.29315185546875, \t Total Dis Loss : 0.0005498091923072934\n",
      "Steps : 58900, \t Total Gen Loss : 26.280757904052734, \t Total Dis Loss : 0.0014588582562282681\n",
      "Steps : 59000, \t Total Gen Loss : 25.864307403564453, \t Total Dis Loss : 0.0002453781198710203\n",
      "Steps : 59100, \t Total Gen Loss : 26.088848114013672, \t Total Dis Loss : 0.00020552378555294126\n",
      "Steps : 59200, \t Total Gen Loss : 22.880756378173828, \t Total Dis Loss : 0.0004613222845364362\n",
      "Steps : 59300, \t Total Gen Loss : 23.228694915771484, \t Total Dis Loss : 0.0006491228123195469\n",
      "Steps : 59400, \t Total Gen Loss : 24.510950088500977, \t Total Dis Loss : 0.00017798211774788797\n",
      "Steps : 59500, \t Total Gen Loss : 25.035266876220703, \t Total Dis Loss : 0.00035624232259579003\n",
      "Steps : 59600, \t Total Gen Loss : 28.1966495513916, \t Total Dis Loss : 0.0004891625139862299\n",
      "Steps : 59700, \t Total Gen Loss : 27.47232437133789, \t Total Dis Loss : 0.00026554593932814896\n",
      "Steps : 59800, \t Total Gen Loss : 27.694408416748047, \t Total Dis Loss : 0.0002448445011395961\n",
      "Steps : 59900, \t Total Gen Loss : 24.71119499206543, \t Total Dis Loss : 0.0004716432886198163\n",
      "Steps : 60000, \t Total Gen Loss : 25.62862205505371, \t Total Dis Loss : 0.0002875019854400307\n",
      "Steps : 60100, \t Total Gen Loss : 25.997303009033203, \t Total Dis Loss : 0.0002248778473585844\n",
      "Steps : 60200, \t Total Gen Loss : 25.89301300048828, \t Total Dis Loss : 0.0003831234062090516\n",
      "Steps : 60300, \t Total Gen Loss : 26.52528190612793, \t Total Dis Loss : 0.00032266549533233047\n",
      "Steps : 60400, \t Total Gen Loss : 26.266422271728516, \t Total Dis Loss : 0.0003149909316562116\n",
      "Steps : 60500, \t Total Gen Loss : 29.365951538085938, \t Total Dis Loss : 0.0001696942636044696\n",
      "Steps : 60600, \t Total Gen Loss : 26.212167739868164, \t Total Dis Loss : 0.00015979984891600907\n",
      "Steps : 60700, \t Total Gen Loss : 27.75339126586914, \t Total Dis Loss : 0.00016161937674041837\n",
      "Steps : 60800, \t Total Gen Loss : 24.713886260986328, \t Total Dis Loss : 0.00011857885692734271\n",
      "Steps : 60900, \t Total Gen Loss : 23.12404441833496, \t Total Dis Loss : 0.00012170633272035047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 61000, \t Total Gen Loss : 25.586883544921875, \t Total Dis Loss : 0.00028075225418433547\n",
      "Steps : 61100, \t Total Gen Loss : 24.199491500854492, \t Total Dis Loss : 0.0027201874181628227\n",
      "Steps : 61200, \t Total Gen Loss : 24.645612716674805, \t Total Dis Loss : 0.0008392552845180035\n",
      "Steps : 61300, \t Total Gen Loss : 26.696029663085938, \t Total Dis Loss : 0.0005333180888555944\n",
      "Steps : 61400, \t Total Gen Loss : 27.13003921508789, \t Total Dis Loss : 0.00020146665337961167\n",
      "Steps : 61500, \t Total Gen Loss : 26.1509952545166, \t Total Dis Loss : 0.00020135937666054815\n",
      "Steps : 61600, \t Total Gen Loss : 25.23679542541504, \t Total Dis Loss : 0.0001212015631608665\n",
      "Steps : 61700, \t Total Gen Loss : 27.19515037536621, \t Total Dis Loss : 0.00021306784765329212\n",
      "Steps : 61800, \t Total Gen Loss : 26.319923400878906, \t Total Dis Loss : 0.0002284024958498776\n",
      "Time for epoch 11 is 255.23148393630981 sec\n",
      "Steps : 61900, \t Total Gen Loss : 27.55307960510254, \t Total Dis Loss : 7.735177496215329e-05\n",
      "Steps : 62000, \t Total Gen Loss : 27.690134048461914, \t Total Dis Loss : 0.00029373326105996966\n",
      "Steps : 62100, \t Total Gen Loss : 23.85003662109375, \t Total Dis Loss : 0.0014760131016373634\n",
      "Steps : 62200, \t Total Gen Loss : 25.15572738647461, \t Total Dis Loss : 0.00028933960129506886\n",
      "Steps : 62300, \t Total Gen Loss : 27.191726684570312, \t Total Dis Loss : 0.0002548785414546728\n",
      "Steps : 62400, \t Total Gen Loss : 25.42107391357422, \t Total Dis Loss : 0.0005440436070784926\n",
      "Steps : 62500, \t Total Gen Loss : 28.152692794799805, \t Total Dis Loss : 0.00019512353173922747\n",
      "Steps : 62600, \t Total Gen Loss : 26.803783416748047, \t Total Dis Loss : 0.00024190894328057766\n",
      "Steps : 62700, \t Total Gen Loss : 25.02581024169922, \t Total Dis Loss : 0.0003513467381708324\n",
      "Steps : 62800, \t Total Gen Loss : 28.307390213012695, \t Total Dis Loss : 0.00018617721798364073\n",
      "Steps : 62900, \t Total Gen Loss : 26.082447052001953, \t Total Dis Loss : 0.00020140073320362717\n",
      "Steps : 63000, \t Total Gen Loss : 30.621000289916992, \t Total Dis Loss : 0.00011744067160179839\n",
      "Steps : 63100, \t Total Gen Loss : 25.982187271118164, \t Total Dis Loss : 0.00024117054999805987\n",
      "Steps : 63200, \t Total Gen Loss : 32.312416076660156, \t Total Dis Loss : 0.00036164632183499634\n",
      "Steps : 63300, \t Total Gen Loss : 25.24282455444336, \t Total Dis Loss : 0.0004591337637975812\n",
      "Steps : 63400, \t Total Gen Loss : 24.60455894470215, \t Total Dis Loss : 0.000909578287974\n",
      "Steps : 63500, \t Total Gen Loss : 27.004663467407227, \t Total Dis Loss : 0.012411413714289665\n",
      "Steps : 63600, \t Total Gen Loss : 26.663837432861328, \t Total Dis Loss : 0.0003315793874207884\n",
      "Steps : 63700, \t Total Gen Loss : 30.430173873901367, \t Total Dis Loss : 9.788171155378222e-05\n",
      "Steps : 63800, \t Total Gen Loss : 27.112991333007812, \t Total Dis Loss : 0.00017007248243317008\n",
      "Steps : 63900, \t Total Gen Loss : 28.455636978149414, \t Total Dis Loss : 0.00013382022734731436\n",
      "Steps : 64000, \t Total Gen Loss : 28.163124084472656, \t Total Dis Loss : 0.0002297005121363327\n",
      "Steps : 64100, \t Total Gen Loss : 26.92974853515625, \t Total Dis Loss : 4.412391717778519e-05\n",
      "Steps : 64200, \t Total Gen Loss : 23.558496475219727, \t Total Dis Loss : 0.000801769201643765\n",
      "Steps : 64300, \t Total Gen Loss : 26.446205139160156, \t Total Dis Loss : 0.00014149564958643168\n",
      "Steps : 64400, \t Total Gen Loss : 21.587156295776367, \t Total Dis Loss : 0.07636462152004242\n",
      "Steps : 64500, \t Total Gen Loss : 28.825077056884766, \t Total Dis Loss : 0.00014679779997095466\n",
      "Steps : 64600, \t Total Gen Loss : 22.33444595336914, \t Total Dis Loss : 0.0040547591634094715\n",
      "Steps : 64700, \t Total Gen Loss : 27.79007339477539, \t Total Dis Loss : 7.300102151930332e-05\n",
      "Steps : 64800, \t Total Gen Loss : 25.855478286743164, \t Total Dis Loss : 7.943357923068106e-05\n",
      "Steps : 64900, \t Total Gen Loss : 21.919239044189453, \t Total Dis Loss : 0.0005257707089185715\n",
      "Steps : 65000, \t Total Gen Loss : 23.56618881225586, \t Total Dis Loss : 0.0004401242476888001\n",
      "Steps : 65100, \t Total Gen Loss : 30.027862548828125, \t Total Dis Loss : 0.00017463433323428035\n",
      "Steps : 65200, \t Total Gen Loss : 27.315168380737305, \t Total Dis Loss : 5.698155291611329e-05\n",
      "Steps : 65300, \t Total Gen Loss : 25.41492462158203, \t Total Dis Loss : 0.0009081478347070515\n",
      "Steps : 65400, \t Total Gen Loss : 22.883119583129883, \t Total Dis Loss : 0.00031165083055384457\n",
      "Steps : 65500, \t Total Gen Loss : 25.583202362060547, \t Total Dis Loss : 0.00013327467604540288\n",
      "Steps : 65600, \t Total Gen Loss : 30.51199722290039, \t Total Dis Loss : 0.000106951774796471\n",
      "Steps : 65700, \t Total Gen Loss : 24.756364822387695, \t Total Dis Loss : 5.8509831433184445e-05\n",
      "Steps : 65800, \t Total Gen Loss : 26.952190399169922, \t Total Dis Loss : 0.00032919307705014944\n",
      "Steps : 65900, \t Total Gen Loss : 25.07766342163086, \t Total Dis Loss : 6.892722012707964e-05\n",
      "Steps : 66000, \t Total Gen Loss : 29.521896362304688, \t Total Dis Loss : 3.982149428338744e-05\n",
      "Steps : 66100, \t Total Gen Loss : 21.99465560913086, \t Total Dis Loss : 0.0012961988104507327\n",
      "Steps : 66200, \t Total Gen Loss : 26.650177001953125, \t Total Dis Loss : 0.0006954747950658202\n",
      "Steps : 66300, \t Total Gen Loss : 22.213619232177734, \t Total Dis Loss : 0.0005700980545952916\n",
      "Steps : 66400, \t Total Gen Loss : 23.03130340576172, \t Total Dis Loss : 0.0002668422821443528\n",
      "Steps : 66500, \t Total Gen Loss : 23.501867294311523, \t Total Dis Loss : 0.0001239054254256189\n",
      "Steps : 66600, \t Total Gen Loss : 26.18075180053711, \t Total Dis Loss : 0.00022966590768191963\n",
      "Steps : 66700, \t Total Gen Loss : 27.137388229370117, \t Total Dis Loss : 0.0006025951006449759\n",
      "Steps : 66800, \t Total Gen Loss : 29.9785099029541, \t Total Dis Loss : 0.00011940403783228248\n",
      "Steps : 66900, \t Total Gen Loss : 28.441055297851562, \t Total Dis Loss : 0.0021437460090965033\n",
      "Steps : 67000, \t Total Gen Loss : 27.38105010986328, \t Total Dis Loss : 0.002389416564255953\n",
      "Steps : 67100, \t Total Gen Loss : 30.976436614990234, \t Total Dis Loss : 7.826917862985283e-05\n",
      "Steps : 67200, \t Total Gen Loss : 31.099077224731445, \t Total Dis Loss : 4.0134858863893896e-05\n",
      "Steps : 67300, \t Total Gen Loss : 30.205730438232422, \t Total Dis Loss : 9.601835336070508e-05\n",
      "Steps : 67400, \t Total Gen Loss : 27.905988693237305, \t Total Dis Loss : 0.0002803998067975044\n",
      "Steps : 67500, \t Total Gen Loss : 33.8848991394043, \t Total Dis Loss : 0.00037536025047302246\n",
      "Time for epoch 12 is 255.62275624275208 sec\n",
      "Steps : 67600, \t Total Gen Loss : 29.730539321899414, \t Total Dis Loss : 0.0026430345606058836\n",
      "Steps : 67700, \t Total Gen Loss : 30.398672103881836, \t Total Dis Loss : 0.00023340254847425967\n",
      "Steps : 67800, \t Total Gen Loss : 32.78740692138672, \t Total Dis Loss : 3.966545045841485e-05\n",
      "Steps : 67900, \t Total Gen Loss : 30.565757751464844, \t Total Dis Loss : 9.669793507782742e-05\n",
      "Steps : 68000, \t Total Gen Loss : 30.575593948364258, \t Total Dis Loss : 0.0012980349129065871\n",
      "Steps : 68100, \t Total Gen Loss : 32.053646087646484, \t Total Dis Loss : 5.4059677495388314e-05\n",
      "Steps : 68200, \t Total Gen Loss : 31.423994064331055, \t Total Dis Loss : 0.00040005959453992546\n",
      "Steps : 68300, \t Total Gen Loss : 29.417633056640625, \t Total Dis Loss : 7.148889562813565e-05\n",
      "Steps : 68400, \t Total Gen Loss : 29.458484649658203, \t Total Dis Loss : 2.4293711248901673e-05\n",
      "Steps : 68500, \t Total Gen Loss : 27.24315643310547, \t Total Dis Loss : 6.640957144554704e-05\n",
      "Steps : 68600, \t Total Gen Loss : 27.612384796142578, \t Total Dis Loss : 0.0015296676428988576\n",
      "Steps : 68700, \t Total Gen Loss : 26.183134078979492, \t Total Dis Loss : 0.00018216943135485053\n",
      "Steps : 68800, \t Total Gen Loss : 29.32514190673828, \t Total Dis Loss : 0.00019247856107540429\n",
      "Steps : 68900, \t Total Gen Loss : 30.516836166381836, \t Total Dis Loss : 3.649544669315219e-05\n",
      "Steps : 69000, \t Total Gen Loss : 28.697261810302734, \t Total Dis Loss : 0.000100627017673105\n",
      "Steps : 69100, \t Total Gen Loss : 26.601287841796875, \t Total Dis Loss : 0.00018087997159454972\n",
      "Steps : 69200, \t Total Gen Loss : 28.984397888183594, \t Total Dis Loss : 3.917749199899845e-05\n",
      "Steps : 69300, \t Total Gen Loss : 29.403059005737305, \t Total Dis Loss : 0.0003223214880563319\n",
      "Steps : 69400, \t Total Gen Loss : 26.837276458740234, \t Total Dis Loss : 3.9152066165115684e-05\n",
      "Steps : 69500, \t Total Gen Loss : 28.034448623657227, \t Total Dis Loss : 3.5336692235432565e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 69600, \t Total Gen Loss : 29.630643844604492, \t Total Dis Loss : 6.712670437991619e-05\n",
      "Steps : 69700, \t Total Gen Loss : 29.092758178710938, \t Total Dis Loss : 0.0004301125300116837\n",
      "Steps : 69800, \t Total Gen Loss : 24.939088821411133, \t Total Dis Loss : 0.0013903421349823475\n",
      "Steps : 69900, \t Total Gen Loss : 28.845155715942383, \t Total Dis Loss : 0.00031367060728371143\n",
      "Steps : 70000, \t Total Gen Loss : 29.195171356201172, \t Total Dis Loss : 0.004985653795301914\n",
      "Steps : 70100, \t Total Gen Loss : 31.378971099853516, \t Total Dis Loss : 0.0001674116065260023\n",
      "Steps : 70200, \t Total Gen Loss : 30.98784065246582, \t Total Dis Loss : 0.0007925101090222597\n",
      "Steps : 70300, \t Total Gen Loss : 29.659984588623047, \t Total Dis Loss : 0.00020081170077901334\n",
      "Steps : 70400, \t Total Gen Loss : 25.07428741455078, \t Total Dis Loss : 0.001442466862499714\n",
      "Steps : 70500, \t Total Gen Loss : 29.14729881286621, \t Total Dis Loss : 0.00021813492639921606\n",
      "Steps : 70600, \t Total Gen Loss : 26.48661231994629, \t Total Dis Loss : 0.0009900308214128017\n",
      "Steps : 70700, \t Total Gen Loss : 25.505451202392578, \t Total Dis Loss : 8.499782416038215e-05\n",
      "Steps : 70800, \t Total Gen Loss : 25.78014373779297, \t Total Dis Loss : 0.00019060634076595306\n",
      "Steps : 70900, \t Total Gen Loss : 28.050373077392578, \t Total Dis Loss : 0.00011898558295797557\n",
      "Steps : 71000, \t Total Gen Loss : 24.923725128173828, \t Total Dis Loss : 0.0003887788625434041\n",
      "Steps : 71100, \t Total Gen Loss : 21.327051162719727, \t Total Dis Loss : 0.000493368657771498\n",
      "Steps : 71200, \t Total Gen Loss : 25.80542755126953, \t Total Dis Loss : 0.00033374648774042726\n",
      "Steps : 71300, \t Total Gen Loss : 26.583927154541016, \t Total Dis Loss : 8.291412086691707e-05\n",
      "Steps : 71400, \t Total Gen Loss : 26.162334442138672, \t Total Dis Loss : 0.00012345713912509382\n",
      "Steps : 71500, \t Total Gen Loss : 31.454559326171875, \t Total Dis Loss : 0.003827515756711364\n",
      "Steps : 71600, \t Total Gen Loss : 28.27742576599121, \t Total Dis Loss : 0.0001132976176450029\n",
      "Steps : 71700, \t Total Gen Loss : 28.55902099609375, \t Total Dis Loss : 4.532443927018903e-05\n",
      "Steps : 71800, \t Total Gen Loss : 29.35216522216797, \t Total Dis Loss : 0.00015323239495046437\n",
      "Steps : 71900, \t Total Gen Loss : 27.664804458618164, \t Total Dis Loss : 0.0004093124298378825\n",
      "Steps : 72000, \t Total Gen Loss : 34.36674118041992, \t Total Dis Loss : 0.0006640455685555935\n",
      "Steps : 72100, \t Total Gen Loss : 30.493335723876953, \t Total Dis Loss : 0.0009589440305717289\n",
      "Steps : 72200, \t Total Gen Loss : 28.488922119140625, \t Total Dis Loss : 0.0005284258513711393\n",
      "Steps : 72300, \t Total Gen Loss : 28.83763885498047, \t Total Dis Loss : 0.0005311670247465372\n",
      "Steps : 72400, \t Total Gen Loss : 28.60936737060547, \t Total Dis Loss : 0.0034028596710413694\n",
      "Steps : 72500, \t Total Gen Loss : 26.176633834838867, \t Total Dis Loss : 7.465607632184401e-05\n",
      "Steps : 72600, \t Total Gen Loss : 25.40178871154785, \t Total Dis Loss : 0.0012317351065576077\n",
      "Steps : 72700, \t Total Gen Loss : 25.771255493164062, \t Total Dis Loss : 0.00289837084710598\n",
      "Steps : 72800, \t Total Gen Loss : 27.389970779418945, \t Total Dis Loss : 0.00038812420098111033\n",
      "Steps : 72900, \t Total Gen Loss : 26.933853149414062, \t Total Dis Loss : 0.0008662223117426038\n",
      "Steps : 73000, \t Total Gen Loss : 29.77431869506836, \t Total Dis Loss : 0.00017356594617012888\n",
      "Steps : 73100, \t Total Gen Loss : 30.030170440673828, \t Total Dis Loss : 0.00024049360945355147\n",
      "Time for epoch 13 is 254.07620525360107 sec\n",
      "Steps : 73200, \t Total Gen Loss : 24.201725006103516, \t Total Dis Loss : 0.004738535266369581\n",
      "Steps : 73300, \t Total Gen Loss : 26.869359970092773, \t Total Dis Loss : 0.0015033080708235502\n",
      "Steps : 73400, \t Total Gen Loss : 30.530006408691406, \t Total Dis Loss : 0.002158136572688818\n",
      "Steps : 73500, \t Total Gen Loss : 27.181486129760742, \t Total Dis Loss : 0.003639972535893321\n",
      "Steps : 73600, \t Total Gen Loss : 28.903316497802734, \t Total Dis Loss : 0.00010271982318954542\n",
      "Steps : 73700, \t Total Gen Loss : 26.58226776123047, \t Total Dis Loss : 0.003558393567800522\n",
      "Steps : 73800, \t Total Gen Loss : 27.25780487060547, \t Total Dis Loss : 0.0001682546571828425\n",
      "Steps : 73900, \t Total Gen Loss : 29.111082077026367, \t Total Dis Loss : 0.0004098010540474206\n",
      "Steps : 74000, \t Total Gen Loss : 26.758230209350586, \t Total Dis Loss : 0.0012266936246305704\n",
      "Steps : 74100, \t Total Gen Loss : 27.48558807373047, \t Total Dis Loss : 0.0007509514689445496\n",
      "Steps : 74200, \t Total Gen Loss : 33.87042999267578, \t Total Dis Loss : 9.830133785726503e-05\n",
      "Steps : 74300, \t Total Gen Loss : 30.76852035522461, \t Total Dis Loss : 0.0014079135144129395\n",
      "Steps : 74400, \t Total Gen Loss : 28.809612274169922, \t Total Dis Loss : 0.00017277689767070115\n",
      "Steps : 74500, \t Total Gen Loss : 26.144603729248047, \t Total Dis Loss : 0.00030478666303679347\n",
      "Steps : 74600, \t Total Gen Loss : 30.19571304321289, \t Total Dis Loss : 0.00031692979973740876\n",
      "Steps : 74700, \t Total Gen Loss : 25.304981231689453, \t Total Dis Loss : 0.00024791245232336223\n",
      "Steps : 74800, \t Total Gen Loss : 28.09610939025879, \t Total Dis Loss : 0.00031421182211488485\n",
      "Steps : 74900, \t Total Gen Loss : 28.101598739624023, \t Total Dis Loss : 0.00032942270627245307\n",
      "Steps : 75000, \t Total Gen Loss : 29.616823196411133, \t Total Dis Loss : 9.834315278567374e-05\n",
      "Steps : 75100, \t Total Gen Loss : 26.886980056762695, \t Total Dis Loss : 0.0001626632729312405\n",
      "Steps : 75200, \t Total Gen Loss : 28.479581832885742, \t Total Dis Loss : 8.383963722735643e-05\n",
      "Steps : 75300, \t Total Gen Loss : 27.63421630859375, \t Total Dis Loss : 0.00015942797472234815\n",
      "Steps : 75400, \t Total Gen Loss : 27.782882690429688, \t Total Dis Loss : 8.983864972833544e-05\n",
      "Steps : 75500, \t Total Gen Loss : 24.117656707763672, \t Total Dis Loss : 0.00015641775098629296\n",
      "Steps : 75600, \t Total Gen Loss : 30.465375900268555, \t Total Dis Loss : 4.7146735596470535e-05\n",
      "Steps : 75700, \t Total Gen Loss : 32.22207260131836, \t Total Dis Loss : 6.571126141352579e-05\n",
      "Steps : 75800, \t Total Gen Loss : 30.292388916015625, \t Total Dis Loss : 0.00020101851259823889\n",
      "Steps : 75900, \t Total Gen Loss : 29.23088836669922, \t Total Dis Loss : 9.64309656410478e-05\n",
      "Steps : 76000, \t Total Gen Loss : 26.843692779541016, \t Total Dis Loss : 0.00015892951341811568\n",
      "Steps : 76100, \t Total Gen Loss : 27.114418029785156, \t Total Dis Loss : 0.00017400842625647783\n",
      "Steps : 76200, \t Total Gen Loss : 29.55409049987793, \t Total Dis Loss : 5.3063584346091375e-05\n",
      "Steps : 76300, \t Total Gen Loss : 27.379642486572266, \t Total Dis Loss : 0.00012700608931481838\n",
      "Steps : 76400, \t Total Gen Loss : 28.492645263671875, \t Total Dis Loss : 0.00010972763993777335\n",
      "Steps : 76500, \t Total Gen Loss : 29.128658294677734, \t Total Dis Loss : 0.0001333600521320477\n",
      "Steps : 76600, \t Total Gen Loss : 28.71955680847168, \t Total Dis Loss : 0.00012899257126264274\n",
      "Steps : 76700, \t Total Gen Loss : 28.1972713470459, \t Total Dis Loss : 7.578835356980562e-05\n",
      "Steps : 76800, \t Total Gen Loss : 27.801441192626953, \t Total Dis Loss : 7.948069833219051e-05\n",
      "Steps : 76900, \t Total Gen Loss : 29.56258773803711, \t Total Dis Loss : 8.735197479836643e-05\n",
      "Steps : 77000, \t Total Gen Loss : 26.887557983398438, \t Total Dis Loss : 0.0003794797230511904\n",
      "Steps : 77100, \t Total Gen Loss : 28.749849319458008, \t Total Dis Loss : 0.000635258387774229\n",
      "Steps : 77200, \t Total Gen Loss : 27.53244972229004, \t Total Dis Loss : 0.00013775916886515915\n",
      "Steps : 77300, \t Total Gen Loss : 33.08505630493164, \t Total Dis Loss : 0.00021438198746182024\n",
      "Steps : 77400, \t Total Gen Loss : 29.103374481201172, \t Total Dis Loss : 0.0007003879873082042\n",
      "Steps : 77500, \t Total Gen Loss : 29.869657516479492, \t Total Dis Loss : 0.00010732718510553241\n",
      "Steps : 77600, \t Total Gen Loss : 27.73479461669922, \t Total Dis Loss : 0.00019755790708586574\n",
      "Steps : 77700, \t Total Gen Loss : 29.335662841796875, \t Total Dis Loss : 0.036268580704927444\n",
      "Steps : 77800, \t Total Gen Loss : 33.05058670043945, \t Total Dis Loss : 0.0015003200387582183\n",
      "Steps : 77900, \t Total Gen Loss : 30.514278411865234, \t Total Dis Loss : 2.950553607661277e-05\n",
      "Steps : 78000, \t Total Gen Loss : 30.794361114501953, \t Total Dis Loss : 5.4188793001230806e-05\n",
      "Steps : 78100, \t Total Gen Loss : 24.52234649658203, \t Total Dis Loss : 0.002062419429421425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 78200, \t Total Gen Loss : 25.922266006469727, \t Total Dis Loss : 0.029185142368078232\n",
      "Steps : 78300, \t Total Gen Loss : 31.615140914916992, \t Total Dis Loss : 0.0004283562011551112\n",
      "Steps : 78400, \t Total Gen Loss : 31.044822692871094, \t Total Dis Loss : 0.0018670412246137857\n",
      "Steps : 78500, \t Total Gen Loss : 28.881603240966797, \t Total Dis Loss : 0.0008751003770157695\n",
      "Steps : 78600, \t Total Gen Loss : 28.528043746948242, \t Total Dis Loss : 0.0004189792671240866\n",
      "Steps : 78700, \t Total Gen Loss : 27.715234756469727, \t Total Dis Loss : 0.00018227528198622167\n",
      "Time for epoch 14 is 250.2766978740692 sec\n",
      "Steps : 78800, \t Total Gen Loss : 27.41529083251953, \t Total Dis Loss : 0.0001264187740162015\n",
      "Steps : 78900, \t Total Gen Loss : 27.55608367919922, \t Total Dis Loss : 0.00010016659507527947\n",
      "Steps : 79000, \t Total Gen Loss : 25.880922317504883, \t Total Dis Loss : 0.00012620538473129272\n",
      "Steps : 79100, \t Total Gen Loss : 27.37603759765625, \t Total Dis Loss : 0.00017622581799514592\n",
      "Steps : 79200, \t Total Gen Loss : 25.47175407409668, \t Total Dis Loss : 0.0002608991926535964\n",
      "Steps : 79300, \t Total Gen Loss : 26.803913116455078, \t Total Dis Loss : 0.00019893713761121035\n",
      "Steps : 79400, \t Total Gen Loss : 29.947935104370117, \t Total Dis Loss : 0.00011935640941374004\n",
      "Steps : 79500, \t Total Gen Loss : 26.964855194091797, \t Total Dis Loss : 0.00047325779451057315\n",
      "Steps : 79600, \t Total Gen Loss : 27.12968635559082, \t Total Dis Loss : 0.00019344937754794955\n",
      "Steps : 79700, \t Total Gen Loss : 31.58193016052246, \t Total Dis Loss : 0.0002824972616508603\n",
      "Steps : 79800, \t Total Gen Loss : 26.93300437927246, \t Total Dis Loss : 0.00018771906616166234\n",
      "Steps : 79900, \t Total Gen Loss : 27.133033752441406, \t Total Dis Loss : 0.00039011077024042606\n",
      "Steps : 80000, \t Total Gen Loss : 23.905656814575195, \t Total Dis Loss : 0.002580637112259865\n",
      "Steps : 80100, \t Total Gen Loss : 26.720991134643555, \t Total Dis Loss : 0.008812885731458664\n",
      "Steps : 80200, \t Total Gen Loss : 21.635292053222656, \t Total Dis Loss : 0.03777686133980751\n",
      "Steps : 80300, \t Total Gen Loss : 31.31109046936035, \t Total Dis Loss : 0.0020991067867726088\n",
      "Steps : 80400, \t Total Gen Loss : 28.383684158325195, \t Total Dis Loss : 0.003207083325833082\n",
      "Steps : 80500, \t Total Gen Loss : 27.906497955322266, \t Total Dis Loss : 0.00010762931196950376\n",
      "Steps : 80600, \t Total Gen Loss : 29.327356338500977, \t Total Dis Loss : 0.00027375511126592755\n",
      "Steps : 80700, \t Total Gen Loss : 25.064857482910156, \t Total Dis Loss : 0.00023166324535850435\n",
      "Steps : 80800, \t Total Gen Loss : 28.536174774169922, \t Total Dis Loss : 7.571043533971533e-05\n",
      "Steps : 80900, \t Total Gen Loss : 25.756824493408203, \t Total Dis Loss : 4.344343324191868e-05\n",
      "Steps : 81000, \t Total Gen Loss : 26.004751205444336, \t Total Dis Loss : 3.9207268855534494e-05\n",
      "Steps : 81100, \t Total Gen Loss : 25.208566665649414, \t Total Dis Loss : 4.403572893352248e-05\n",
      "Steps : 81200, \t Total Gen Loss : 25.75238037109375, \t Total Dis Loss : 0.0001927762059494853\n",
      "Steps : 81300, \t Total Gen Loss : 26.733890533447266, \t Total Dis Loss : 0.00020107539603486657\n",
      "Steps : 81400, \t Total Gen Loss : 25.51812744140625, \t Total Dis Loss : 0.00011111763888038695\n",
      "Steps : 81500, \t Total Gen Loss : 25.059112548828125, \t Total Dis Loss : 0.00031732505885884166\n",
      "Steps : 81600, \t Total Gen Loss : 28.592317581176758, \t Total Dis Loss : 0.00011573413939913735\n",
      "Steps : 81700, \t Total Gen Loss : 27.792184829711914, \t Total Dis Loss : 3.494561315164901e-05\n",
      "Steps : 81800, \t Total Gen Loss : 27.217185974121094, \t Total Dis Loss : 5.6890094128903e-05\n",
      "Steps : 81900, \t Total Gen Loss : 27.755075454711914, \t Total Dis Loss : 3.035481677216012e-05\n",
      "Steps : 82000, \t Total Gen Loss : 26.294069290161133, \t Total Dis Loss : 0.0011847104178741574\n",
      "Steps : 82100, \t Total Gen Loss : 29.472890853881836, \t Total Dis Loss : 0.00012368620082270354\n",
      "Steps : 82200, \t Total Gen Loss : 28.06061363220215, \t Total Dis Loss : 0.00016566510021220893\n",
      "Steps : 82300, \t Total Gen Loss : 28.019439697265625, \t Total Dis Loss : 0.191207155585289\n",
      "Steps : 82400, \t Total Gen Loss : 24.496280670166016, \t Total Dis Loss : 0.0006363190477713943\n",
      "Steps : 82500, \t Total Gen Loss : 31.035999298095703, \t Total Dis Loss : 0.00022841477766633034\n",
      "Steps : 82600, \t Total Gen Loss : 30.06022071838379, \t Total Dis Loss : 0.00011918482778128237\n",
      "Steps : 82700, \t Total Gen Loss : 27.452919006347656, \t Total Dis Loss : 0.0039757839404046535\n",
      "Steps : 82800, \t Total Gen Loss : 31.123292922973633, \t Total Dis Loss : 0.0003641328657977283\n",
      "Steps : 82900, \t Total Gen Loss : 30.405601501464844, \t Total Dis Loss : 0.0051621729508042336\n",
      "Steps : 83000, \t Total Gen Loss : 30.114782333374023, \t Total Dis Loss : 2.829475124599412e-05\n",
      "Steps : 83100, \t Total Gen Loss : 28.44228744506836, \t Total Dis Loss : 0.0006866823532618582\n",
      "Steps : 83200, \t Total Gen Loss : 28.367538452148438, \t Total Dis Loss : 0.0002573390956968069\n",
      "Steps : 83300, \t Total Gen Loss : 28.317916870117188, \t Total Dis Loss : 0.00012444431195035577\n",
      "Steps : 83400, \t Total Gen Loss : 27.11178207397461, \t Total Dis Loss : 0.0005791072617284954\n",
      "Steps : 83500, \t Total Gen Loss : 26.84565544128418, \t Total Dis Loss : 0.00021350941096898168\n",
      "Steps : 83600, \t Total Gen Loss : 28.247547149658203, \t Total Dis Loss : 0.00015498197171837091\n",
      "Steps : 83700, \t Total Gen Loss : 29.400062561035156, \t Total Dis Loss : 0.00017853445024229586\n",
      "Steps : 83800, \t Total Gen Loss : 26.105634689331055, \t Total Dis Loss : 7.784814079059288e-05\n",
      "Steps : 83900, \t Total Gen Loss : 25.293869018554688, \t Total Dis Loss : 0.00023332747514359653\n",
      "Steps : 84000, \t Total Gen Loss : 27.311336517333984, \t Total Dis Loss : 0.00020626912009902298\n",
      "Steps : 84100, \t Total Gen Loss : 26.497882843017578, \t Total Dis Loss : 0.00038306263741105795\n",
      "Steps : 84200, \t Total Gen Loss : 29.662572860717773, \t Total Dis Loss : 0.00034253534977324307\n",
      "Steps : 84300, \t Total Gen Loss : 31.61109161376953, \t Total Dis Loss : 1.3265993402455933e-05\n",
      "Time for epoch 15 is 256.14490723609924 sec\n",
      "Steps : 84400, \t Total Gen Loss : 28.75542449951172, \t Total Dis Loss : 0.00031323186703957617\n",
      "Steps : 84500, \t Total Gen Loss : 28.382740020751953, \t Total Dis Loss : 0.0006451569497585297\n",
      "Steps : 84600, \t Total Gen Loss : 29.178783416748047, \t Total Dis Loss : 0.06481041759252548\n",
      "Steps : 84700, \t Total Gen Loss : 28.572721481323242, \t Total Dis Loss : 0.0005979937268421054\n",
      "Steps : 84800, \t Total Gen Loss : 28.80599021911621, \t Total Dis Loss : 0.0010043028742074966\n",
      "Steps : 84900, \t Total Gen Loss : 29.730159759521484, \t Total Dis Loss : 4.5287983084563166e-05\n",
      "Steps : 85000, \t Total Gen Loss : 31.936885833740234, \t Total Dis Loss : 0.0001429783005733043\n",
      "Steps : 85100, \t Total Gen Loss : 28.64724349975586, \t Total Dis Loss : 9.910926746670157e-05\n",
      "Steps : 85200, \t Total Gen Loss : 32.63752365112305, \t Total Dis Loss : 0.0002795900509227067\n",
      "Steps : 85300, \t Total Gen Loss : 31.94169807434082, \t Total Dis Loss : 2.4111492166412063e-05\n",
      "Steps : 85400, \t Total Gen Loss : 29.241506576538086, \t Total Dis Loss : 0.0005267770611681044\n",
      "Steps : 85500, \t Total Gen Loss : 27.384784698486328, \t Total Dis Loss : 0.004235065076500177\n",
      "Steps : 85600, \t Total Gen Loss : 26.818492889404297, \t Total Dis Loss : 0.0003347338642925024\n",
      "Steps : 85700, \t Total Gen Loss : 23.98532485961914, \t Total Dis Loss : 0.0001400430192006752\n",
      "Steps : 85800, \t Total Gen Loss : 23.16912841796875, \t Total Dis Loss : 0.014451847411692142\n",
      "Steps : 85900, \t Total Gen Loss : 23.80008316040039, \t Total Dis Loss : 0.11063984781503677\n",
      "Steps : 86000, \t Total Gen Loss : 25.363754272460938, \t Total Dis Loss : 0.00018180573533754796\n",
      "Steps : 86100, \t Total Gen Loss : 23.38923454284668, \t Total Dis Loss : 0.0002479769173078239\n",
      "Steps : 86200, \t Total Gen Loss : 26.634737014770508, \t Total Dis Loss : 0.0007597485091537237\n",
      "Steps : 86300, \t Total Gen Loss : 26.5020809173584, \t Total Dis Loss : 0.00033987301867455244\n",
      "Steps : 86400, \t Total Gen Loss : 27.549278259277344, \t Total Dis Loss : 0.0001831635890994221\n",
      "Steps : 86500, \t Total Gen Loss : 26.670560836791992, \t Total Dis Loss : 0.00035206088796257973\n",
      "Steps : 86600, \t Total Gen Loss : 31.294851303100586, \t Total Dis Loss : 0.00034062948543578386\n",
      "Steps : 86700, \t Total Gen Loss : 28.937711715698242, \t Total Dis Loss : 0.00011949845793424174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 86800, \t Total Gen Loss : 27.337675094604492, \t Total Dis Loss : 0.00023945719294715673\n",
      "Steps : 86900, \t Total Gen Loss : 29.440837860107422, \t Total Dis Loss : 0.00017509116150904447\n",
      "Steps : 87000, \t Total Gen Loss : 27.310556411743164, \t Total Dis Loss : 0.0007899167248979211\n",
      "Steps : 87100, \t Total Gen Loss : 26.317066192626953, \t Total Dis Loss : 0.0023516223300248384\n",
      "Steps : 87200, \t Total Gen Loss : 26.639862060546875, \t Total Dis Loss : 0.0006476311828009784\n",
      "Steps : 87300, \t Total Gen Loss : 26.130550384521484, \t Total Dis Loss : 0.0008162652957253158\n",
      "Steps : 87400, \t Total Gen Loss : 27.30597496032715, \t Total Dis Loss : 0.00033654586877673864\n",
      "Steps : 87500, \t Total Gen Loss : 26.72187042236328, \t Total Dis Loss : 0.0015417271060869098\n",
      "Steps : 87600, \t Total Gen Loss : 26.45094108581543, \t Total Dis Loss : 0.0004067404952365905\n",
      "Steps : 87700, \t Total Gen Loss : 30.70075225830078, \t Total Dis Loss : 1.0393750926596113e-05\n",
      "Steps : 87800, \t Total Gen Loss : 32.820377349853516, \t Total Dis Loss : 8.253997657448053e-05\n",
      "Steps : 87900, \t Total Gen Loss : 27.314294815063477, \t Total Dis Loss : 4.967787754139863e-05\n",
      "Steps : 88000, \t Total Gen Loss : 29.14032745361328, \t Total Dis Loss : 4.3728472519433126e-05\n",
      "Steps : 88100, \t Total Gen Loss : 27.73654556274414, \t Total Dis Loss : 0.00021468599152285606\n",
      "Steps : 88200, \t Total Gen Loss : 27.36880874633789, \t Total Dis Loss : 0.000246651004999876\n",
      "Steps : 88300, \t Total Gen Loss : 27.727432250976562, \t Total Dis Loss : 0.000290259049506858\n",
      "Steps : 88400, \t Total Gen Loss : 25.188905715942383, \t Total Dis Loss : 0.0001945949043147266\n",
      "Steps : 88500, \t Total Gen Loss : 30.71636390686035, \t Total Dis Loss : 8.160418656188995e-05\n",
      "Steps : 88600, \t Total Gen Loss : 30.93644905090332, \t Total Dis Loss : 0.00016417096776422113\n",
      "Steps : 88700, \t Total Gen Loss : 27.92129135131836, \t Total Dis Loss : 0.00016549996507819742\n",
      "Steps : 88800, \t Total Gen Loss : 25.702957153320312, \t Total Dis Loss : 0.00021228555124253035\n",
      "Steps : 88900, \t Total Gen Loss : 26.807605743408203, \t Total Dis Loss : 0.00014435910270549357\n",
      "Steps : 89000, \t Total Gen Loss : 30.323650360107422, \t Total Dis Loss : 5.4840089433128014e-05\n",
      "Steps : 89100, \t Total Gen Loss : 27.046552658081055, \t Total Dis Loss : 0.0002904273278545588\n",
      "Steps : 89200, \t Total Gen Loss : 28.784177780151367, \t Total Dis Loss : 0.0021297154016792774\n",
      "Steps : 89300, \t Total Gen Loss : 28.749740600585938, \t Total Dis Loss : 0.001019941410049796\n",
      "Steps : 89400, \t Total Gen Loss : 29.57529067993164, \t Total Dis Loss : 0.0002727006794884801\n",
      "Steps : 89500, \t Total Gen Loss : 29.42327117919922, \t Total Dis Loss : 0.0010723231825977564\n",
      "Steps : 89600, \t Total Gen Loss : 23.10995864868164, \t Total Dis Loss : 0.15526898205280304\n",
      "Steps : 89700, \t Total Gen Loss : 28.568483352661133, \t Total Dis Loss : 2.7829240934806876e-05\n",
      "Steps : 89800, \t Total Gen Loss : 28.335634231567383, \t Total Dis Loss : 0.0002077741955872625\n",
      "Steps : 89900, \t Total Gen Loss : 27.289840698242188, \t Total Dis Loss : 0.00014788492990192026\n",
      "Steps : 90000, \t Total Gen Loss : 28.745285034179688, \t Total Dis Loss : 0.0006897463463246822\n",
      "Time for epoch 16 is 253.3848271369934 sec\n",
      "Steps : 90100, \t Total Gen Loss : 31.7666015625, \t Total Dis Loss : 2.1962589016766287e-05\n",
      "Steps : 90200, \t Total Gen Loss : 30.682315826416016, \t Total Dis Loss : 1.5452855222974904e-05\n",
      "Steps : 90300, \t Total Gen Loss : 29.673057556152344, \t Total Dis Loss : 0.0008333910373039544\n",
      "Steps : 90400, \t Total Gen Loss : 31.865127563476562, \t Total Dis Loss : 0.0008037573425099254\n",
      "Steps : 90500, \t Total Gen Loss : 30.094482421875, \t Total Dis Loss : 0.0005130458739586174\n",
      "Steps : 90600, \t Total Gen Loss : 30.78619384765625, \t Total Dis Loss : 7.186875154729933e-05\n",
      "Steps : 90700, \t Total Gen Loss : 28.083232879638672, \t Total Dis Loss : 5.925397636019625e-05\n",
      "Steps : 90800, \t Total Gen Loss : 32.448387145996094, \t Total Dis Loss : 0.0006321578403003514\n",
      "Steps : 90900, \t Total Gen Loss : 25.65446662902832, \t Total Dis Loss : 0.0013238604879006743\n",
      "Steps : 91000, \t Total Gen Loss : 29.76850128173828, \t Total Dis Loss : 0.0002080886042676866\n",
      "Steps : 91100, \t Total Gen Loss : 28.13326644897461, \t Total Dis Loss : 6.290884630288929e-05\n",
      "Steps : 91200, \t Total Gen Loss : 26.866901397705078, \t Total Dis Loss : 0.00016270951891783625\n",
      "Steps : 91300, \t Total Gen Loss : 27.3561954498291, \t Total Dis Loss : 5.590901491814293e-05\n",
      "Steps : 91400, \t Total Gen Loss : 26.03437042236328, \t Total Dis Loss : 0.00011806990369223058\n",
      "Steps : 91500, \t Total Gen Loss : 29.750335693359375, \t Total Dis Loss : 0.0008810961153358221\n",
      "Steps : 91600, \t Total Gen Loss : 28.571998596191406, \t Total Dis Loss : 0.00010114992619492114\n",
      "Steps : 91700, \t Total Gen Loss : 26.22560691833496, \t Total Dis Loss : 0.00010083029337693006\n",
      "Steps : 91800, \t Total Gen Loss : 28.334854125976562, \t Total Dis Loss : 4.735122638521716e-05\n",
      "Steps : 91900, \t Total Gen Loss : 29.115875244140625, \t Total Dis Loss : 4.3848558561876416e-05\n",
      "Steps : 92000, \t Total Gen Loss : 26.944454193115234, \t Total Dis Loss : 9.515546844340861e-05\n",
      "Steps : 92100, \t Total Gen Loss : 28.65243148803711, \t Total Dis Loss : 5.196562415221706e-05\n",
      "Steps : 92200, \t Total Gen Loss : 30.9841365814209, \t Total Dis Loss : 7.63902353355661e-05\n",
      "Steps : 92300, \t Total Gen Loss : 31.706336975097656, \t Total Dis Loss : 0.0003292166511528194\n",
      "Steps : 92400, \t Total Gen Loss : 28.486888885498047, \t Total Dis Loss : 5.878624506294727e-05\n",
      "Steps : 92500, \t Total Gen Loss : 27.83894920349121, \t Total Dis Loss : 0.0033927953336387873\n",
      "Steps : 92600, \t Total Gen Loss : 27.158430099487305, \t Total Dis Loss : 0.0001115635532187298\n",
      "Steps : 92700, \t Total Gen Loss : 29.975650787353516, \t Total Dis Loss : 8.17791442386806e-05\n",
      "Steps : 92800, \t Total Gen Loss : 30.476375579833984, \t Total Dis Loss : 9.817176760407165e-05\n",
      "Steps : 92900, \t Total Gen Loss : 26.794376373291016, \t Total Dis Loss : 0.0005441302200779319\n",
      "Steps : 93000, \t Total Gen Loss : 31.8560848236084, \t Total Dis Loss : 8.576107211410999e-05\n",
      "Steps : 93100, \t Total Gen Loss : 28.00068473815918, \t Total Dis Loss : 0.0001792464463505894\n",
      "Steps : 93200, \t Total Gen Loss : 28.979568481445312, \t Total Dis Loss : 0.0009006785694509745\n",
      "Steps : 93300, \t Total Gen Loss : 30.982990264892578, \t Total Dis Loss : 0.00018847691535484046\n",
      "Steps : 93400, \t Total Gen Loss : 28.192230224609375, \t Total Dis Loss : 0.0001681279973126948\n",
      "Steps : 93500, \t Total Gen Loss : 25.717222213745117, \t Total Dis Loss : 0.0007178167579695582\n",
      "Steps : 93600, \t Total Gen Loss : 26.979219436645508, \t Total Dis Loss : 0.00023459564545191824\n",
      "Steps : 93700, \t Total Gen Loss : 25.880691528320312, \t Total Dis Loss : 0.00012296551722101867\n",
      "Steps : 93800, \t Total Gen Loss : 25.885757446289062, \t Total Dis Loss : 0.0002321327046956867\n",
      "Steps : 93900, \t Total Gen Loss : 30.253705978393555, \t Total Dis Loss : 8.021056419238448e-05\n",
      "Steps : 94000, \t Total Gen Loss : 27.024972915649414, \t Total Dis Loss : 7.661980635020882e-05\n",
      "Steps : 94100, \t Total Gen Loss : 28.503210067749023, \t Total Dis Loss : 8.920189429773018e-05\n",
      "Steps : 94200, \t Total Gen Loss : 33.31990432739258, \t Total Dis Loss : 3.748905874090269e-05\n",
      "Steps : 94300, \t Total Gen Loss : 28.43706703186035, \t Total Dis Loss : 7.392317638732493e-05\n",
      "Steps : 94400, \t Total Gen Loss : 27.949386596679688, \t Total Dis Loss : 4.437398456502706e-05\n",
      "Steps : 94500, \t Total Gen Loss : 30.919403076171875, \t Total Dis Loss : 9.606903768144548e-05\n",
      "Steps : 94600, \t Total Gen Loss : 25.654922485351562, \t Total Dis Loss : 0.00028585843392647803\n",
      "Steps : 94700, \t Total Gen Loss : 29.44252586364746, \t Total Dis Loss : 4.509638893068768e-05\n",
      "Steps : 94800, \t Total Gen Loss : 26.821630477905273, \t Total Dis Loss : 0.0001312405802309513\n",
      "Steps : 94900, \t Total Gen Loss : 30.16654396057129, \t Total Dis Loss : 5.390883961808868e-05\n",
      "Steps : 95000, \t Total Gen Loss : 30.692310333251953, \t Total Dis Loss : 6.95534108672291e-05\n",
      "Steps : 95100, \t Total Gen Loss : 27.621875762939453, \t Total Dis Loss : 0.00011516644735820591\n",
      "Steps : 95200, \t Total Gen Loss : 24.733718872070312, \t Total Dis Loss : 0.0004999035154469311\n",
      "Steps : 95300, \t Total Gen Loss : 28.77290916442871, \t Total Dis Loss : 0.00012701429659500718\n",
      "Steps : 95400, \t Total Gen Loss : 27.3912353515625, \t Total Dis Loss : 4.4991396862315014e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 95500, \t Total Gen Loss : 27.43440055847168, \t Total Dis Loss : 2.108940498146694e-05\n",
      "Steps : 95600, \t Total Gen Loss : 27.371641159057617, \t Total Dis Loss : 5.090571357868612e-05\n",
      "Time for epoch 17 is 256.30618262290955 sec\n",
      "Steps : 95700, \t Total Gen Loss : 32.00618362426758, \t Total Dis Loss : 1.7464695702074096e-05\n",
      "Steps : 95800, \t Total Gen Loss : 31.081558227539062, \t Total Dis Loss : 4.0235456253867596e-05\n",
      "Steps : 95900, \t Total Gen Loss : 27.34368324279785, \t Total Dis Loss : 0.0002889894531108439\n",
      "Steps : 96000, \t Total Gen Loss : 30.23357582092285, \t Total Dis Loss : 0.00012921450252179056\n",
      "Steps : 96100, \t Total Gen Loss : 27.736452102661133, \t Total Dis Loss : 0.0002686640073079616\n",
      "Steps : 96200, \t Total Gen Loss : 26.67113494873047, \t Total Dis Loss : 6.255244079511613e-05\n",
      "Steps : 96300, \t Total Gen Loss : 27.98699951171875, \t Total Dis Loss : 1.3560293155023828e-05\n",
      "Steps : 96400, \t Total Gen Loss : 28.90842056274414, \t Total Dis Loss : 3.050734994758386e-05\n",
      "Steps : 96500, \t Total Gen Loss : 29.80817985534668, \t Total Dis Loss : 3.942649345844984e-05\n",
      "Steps : 96600, \t Total Gen Loss : 28.741392135620117, \t Total Dis Loss : 2.5429491870454513e-05\n",
      "Steps : 96700, \t Total Gen Loss : 28.315689086914062, \t Total Dis Loss : 6.947269139345735e-05\n",
      "Steps : 96800, \t Total Gen Loss : 28.542903900146484, \t Total Dis Loss : 0.001500191749073565\n",
      "Steps : 96900, \t Total Gen Loss : 25.39171600341797, \t Total Dis Loss : 0.001611562678590417\n",
      "Steps : 97000, \t Total Gen Loss : 25.49911880493164, \t Total Dis Loss : 0.00014250171079766005\n",
      "Steps : 97100, \t Total Gen Loss : 26.54514503479004, \t Total Dis Loss : 0.0002906133886426687\n",
      "Steps : 97200, \t Total Gen Loss : 27.416505813598633, \t Total Dis Loss : 0.00019781102309934795\n",
      "Steps : 97300, \t Total Gen Loss : 28.520376205444336, \t Total Dis Loss : 0.00023846150725148618\n",
      "Steps : 97400, \t Total Gen Loss : 28.18124771118164, \t Total Dis Loss : 0.0002056925732176751\n",
      "Steps : 97500, \t Total Gen Loss : 28.744977951049805, \t Total Dis Loss : 0.00011114752851426601\n",
      "Steps : 97600, \t Total Gen Loss : 24.221323013305664, \t Total Dis Loss : 0.0004599203821271658\n",
      "Steps : 97700, \t Total Gen Loss : 28.66832733154297, \t Total Dis Loss : 0.00020936237706337124\n",
      "Steps : 97800, \t Total Gen Loss : 30.799861907958984, \t Total Dis Loss : 1.5318661098717712e-05\n",
      "Steps : 97900, \t Total Gen Loss : 27.989248275756836, \t Total Dis Loss : 0.00027905681054107845\n",
      "Steps : 98000, \t Total Gen Loss : 26.277427673339844, \t Total Dis Loss : 0.00016824786143843085\n",
      "Steps : 98100, \t Total Gen Loss : 26.717018127441406, \t Total Dis Loss : 0.0001392292615491897\n",
      "Steps : 98200, \t Total Gen Loss : 28.22247886657715, \t Total Dis Loss : 8.959460683399811e-05\n",
      "Steps : 98300, \t Total Gen Loss : 27.528268814086914, \t Total Dis Loss : 0.00011563791485968977\n",
      "Steps : 98400, \t Total Gen Loss : 26.029478073120117, \t Total Dis Loss : 8.773421723162755e-05\n",
      "Steps : 98500, \t Total Gen Loss : 27.4421329498291, \t Total Dis Loss : 8.049583993852139e-05\n",
      "Steps : 98600, \t Total Gen Loss : 26.797008514404297, \t Total Dis Loss : 4.1292620153399184e-05\n",
      "Steps : 98700, \t Total Gen Loss : 30.01795768737793, \t Total Dis Loss : 4.078654455952346e-05\n",
      "Steps : 98800, \t Total Gen Loss : 27.661483764648438, \t Total Dis Loss : 3.056402420043014e-05\n",
      "Steps : 98900, \t Total Gen Loss : 28.962142944335938, \t Total Dis Loss : 3.3919932320714e-05\n",
      "Steps : 99000, \t Total Gen Loss : 31.547813415527344, \t Total Dis Loss : 2.7385509383748285e-05\n",
      "Steps : 99100, \t Total Gen Loss : 25.789472579956055, \t Total Dis Loss : 4.1509949369356036e-05\n",
      "Steps : 99200, \t Total Gen Loss : 29.18947982788086, \t Total Dis Loss : 4.203190474072471e-05\n",
      "Steps : 99300, \t Total Gen Loss : 29.3621883392334, \t Total Dis Loss : 2.9941138564026915e-05\n",
      "Steps : 99400, \t Total Gen Loss : 32.3197135925293, \t Total Dis Loss : 3.6877914681099355e-05\n",
      "Steps : 99500, \t Total Gen Loss : 31.338523864746094, \t Total Dis Loss : 3.5130768083035946e-05\n",
      "Steps : 99600, \t Total Gen Loss : 27.62548065185547, \t Total Dis Loss : 2.7132015020470135e-05\n",
      "Steps : 99700, \t Total Gen Loss : 28.569358825683594, \t Total Dis Loss : 9.504723129794002e-05\n",
      "Steps : 99800, \t Total Gen Loss : 28.66390037536621, \t Total Dis Loss : 0.00014379189815372229\n",
      "Steps : 99900, \t Total Gen Loss : 24.941226959228516, \t Total Dis Loss : 0.00017792661674320698\n",
      "Steps : 100000, \t Total Gen Loss : 28.538097381591797, \t Total Dis Loss : 0.00010221265984000638\n",
      "Steps : 100100, \t Total Gen Loss : 26.71834945678711, \t Total Dis Loss : 0.00041181957931257784\n",
      "Steps : 100200, \t Total Gen Loss : 28.094484329223633, \t Total Dis Loss : 6.79419536027126e-05\n",
      "Steps : 100300, \t Total Gen Loss : 29.55507469177246, \t Total Dis Loss : 6.687434506602585e-05\n",
      "Steps : 100400, \t Total Gen Loss : 31.88892364501953, \t Total Dis Loss : 1.7025258784997277e-05\n",
      "Steps : 100500, \t Total Gen Loss : 30.192655563354492, \t Total Dis Loss : 7.525169348809868e-05\n",
      "Steps : 100600, \t Total Gen Loss : 35.64980697631836, \t Total Dis Loss : 0.00019084729137830436\n",
      "Steps : 100700, \t Total Gen Loss : 28.15594482421875, \t Total Dis Loss : 0.00027612163103185594\n",
      "Steps : 100800, \t Total Gen Loss : 30.59422492980957, \t Total Dis Loss : 0.0009160113986581564\n",
      "Steps : 100900, \t Total Gen Loss : 25.878238677978516, \t Total Dis Loss : 0.0013007839443162084\n",
      "Steps : 101000, \t Total Gen Loss : 29.33976936340332, \t Total Dis Loss : 3.598479452193715e-05\n",
      "Steps : 101100, \t Total Gen Loss : 31.278850555419922, \t Total Dis Loss : 0.0013832903932780027\n",
      "Steps : 101200, \t Total Gen Loss : 27.6793270111084, \t Total Dis Loss : 0.00020431129087228328\n",
      "Time for epoch 18 is 253.90801763534546 sec\n",
      "Steps : 101300, \t Total Gen Loss : 24.60038948059082, \t Total Dis Loss : 0.9794167876243591\n",
      "Steps : 101400, \t Total Gen Loss : 27.291330337524414, \t Total Dis Loss : 0.0003555011353455484\n",
      "Steps : 101500, \t Total Gen Loss : 30.076128005981445, \t Total Dis Loss : 3.369369005667977e-05\n",
      "Steps : 101600, \t Total Gen Loss : 27.571792602539062, \t Total Dis Loss : 0.0001689729979261756\n",
      "Steps : 101700, \t Total Gen Loss : 28.653589248657227, \t Total Dis Loss : 0.00032500969246029854\n",
      "Steps : 101800, \t Total Gen Loss : 29.43239402770996, \t Total Dis Loss : 0.00028195243794471025\n",
      "Steps : 101900, \t Total Gen Loss : 28.358306884765625, \t Total Dis Loss : 2.1061978259240277e-05\n",
      "Steps : 102000, \t Total Gen Loss : 28.88524627685547, \t Total Dis Loss : 3.557551826816052e-05\n",
      "Steps : 102100, \t Total Gen Loss : 25.301599502563477, \t Total Dis Loss : 0.00024616403970867395\n",
      "Steps : 102200, \t Total Gen Loss : 30.33548927307129, \t Total Dis Loss : 6.996162937866757e-06\n",
      "Steps : 102300, \t Total Gen Loss : 31.405315399169922, \t Total Dis Loss : 4.189742321614176e-05\n",
      "Steps : 102400, \t Total Gen Loss : 28.556692123413086, \t Total Dis Loss : 6.401829159585759e-05\n",
      "Steps : 102500, \t Total Gen Loss : 29.65285301208496, \t Total Dis Loss : 4.693466325988993e-05\n",
      "Steps : 102600, \t Total Gen Loss : 28.479238510131836, \t Total Dis Loss : 4.645548324333504e-05\n",
      "Steps : 102700, \t Total Gen Loss : 29.323476791381836, \t Total Dis Loss : 7.249022019095719e-05\n",
      "Steps : 102800, \t Total Gen Loss : 32.032127380371094, \t Total Dis Loss : 1.0620939065120183e-05\n",
      "Steps : 102900, \t Total Gen Loss : 32.021915435791016, \t Total Dis Loss : 5.1826431445078924e-05\n",
      "Steps : 103000, \t Total Gen Loss : 28.237045288085938, \t Total Dis Loss : 9.239683276973665e-05\n",
      "Steps : 103100, \t Total Gen Loss : 27.843931198120117, \t Total Dis Loss : 4.3634019675664604e-05\n",
      "Steps : 103200, \t Total Gen Loss : 27.96479034423828, \t Total Dis Loss : 8.009759767446667e-05\n",
      "Steps : 103300, \t Total Gen Loss : 21.84190559387207, \t Total Dis Loss : 0.0010277885012328625\n",
      "Steps : 103400, \t Total Gen Loss : 25.4228458404541, \t Total Dis Loss : 0.000896105426363647\n",
      "Steps : 103500, \t Total Gen Loss : 29.149808883666992, \t Total Dis Loss : 0.00025685576838441193\n",
      "Steps : 103600, \t Total Gen Loss : 27.236553192138672, \t Total Dis Loss : 7.094410830177367e-05\n",
      "Steps : 103700, \t Total Gen Loss : 31.55683135986328, \t Total Dis Loss : 4.706734398496337e-05\n",
      "Steps : 103800, \t Total Gen Loss : 26.45975112915039, \t Total Dis Loss : 0.00011259468010393903\n",
      "Steps : 103900, \t Total Gen Loss : 28.300649642944336, \t Total Dis Loss : 0.0008459111559204757\n",
      "Steps : 104000, \t Total Gen Loss : 26.268579483032227, \t Total Dis Loss : 0.00032136362278833985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 104100, \t Total Gen Loss : 28.983341217041016, \t Total Dis Loss : 3.482188913039863e-05\n",
      "Steps : 104200, \t Total Gen Loss : 28.529356002807617, \t Total Dis Loss : 0.0002912200288847089\n",
      "Steps : 104300, \t Total Gen Loss : 26.768712997436523, \t Total Dis Loss : 0.00016669105389155447\n",
      "Steps : 104400, \t Total Gen Loss : 27.266292572021484, \t Total Dis Loss : 0.00019960901408921927\n",
      "Steps : 104500, \t Total Gen Loss : 31.794506072998047, \t Total Dis Loss : 0.00015770453319419175\n",
      "Steps : 104600, \t Total Gen Loss : 30.163373947143555, \t Total Dis Loss : 0.000305276014842093\n",
      "Steps : 104700, \t Total Gen Loss : 31.99509048461914, \t Total Dis Loss : 0.00010157726501347497\n",
      "Steps : 104800, \t Total Gen Loss : 29.204843521118164, \t Total Dis Loss : 0.0010400126921012998\n",
      "Steps : 104900, \t Total Gen Loss : 30.435094833374023, \t Total Dis Loss : 0.00027497546398080885\n",
      "Steps : 105000, \t Total Gen Loss : 28.41936492919922, \t Total Dis Loss : 0.0008792936569079757\n",
      "Steps : 105100, \t Total Gen Loss : 30.92208480834961, \t Total Dis Loss : 0.00014166333130560815\n",
      "Steps : 105200, \t Total Gen Loss : 26.654712677001953, \t Total Dis Loss : 0.00012965365021955222\n",
      "Steps : 105300, \t Total Gen Loss : 28.041589736938477, \t Total Dis Loss : 5.594541289610788e-05\n",
      "Steps : 105400, \t Total Gen Loss : 27.84845542907715, \t Total Dis Loss : 0.007478426210582256\n",
      "Steps : 105500, \t Total Gen Loss : 29.26519775390625, \t Total Dis Loss : 0.0004134228511247784\n",
      "Steps : 105600, \t Total Gen Loss : 32.26725769042969, \t Total Dis Loss : 0.0004951102891936898\n",
      "Steps : 105700, \t Total Gen Loss : 31.6057186126709, \t Total Dis Loss : 0.0003427198971621692\n",
      "Steps : 105800, \t Total Gen Loss : 28.51399803161621, \t Total Dis Loss : 0.00011753317812690511\n",
      "Steps : 105900, \t Total Gen Loss : 31.968717575073242, \t Total Dis Loss : 5.38809435965959e-05\n",
      "Steps : 106000, \t Total Gen Loss : 32.943138122558594, \t Total Dis Loss : 0.0001502119703218341\n",
      "Steps : 106100, \t Total Gen Loss : 31.367137908935547, \t Total Dis Loss : 5.747898103436455e-05\n",
      "Steps : 106200, \t Total Gen Loss : 27.40044403076172, \t Total Dis Loss : 3.148356336168945e-05\n",
      "Steps : 106300, \t Total Gen Loss : 29.79254722595215, \t Total Dis Loss : 0.0002664131752680987\n",
      "Steps : 106400, \t Total Gen Loss : 28.10356903076172, \t Total Dis Loss : 8.117493416648358e-05\n",
      "Steps : 106500, \t Total Gen Loss : 32.42184066772461, \t Total Dis Loss : 0.00013314273383002728\n",
      "Steps : 106600, \t Total Gen Loss : 27.788888931274414, \t Total Dis Loss : 0.0005617470014840364\n",
      "Steps : 106700, \t Total Gen Loss : 27.840946197509766, \t Total Dis Loss : 9.838580444920808e-05\n",
      "Steps : 106800, \t Total Gen Loss : 29.971105575561523, \t Total Dis Loss : 3.441559601924382e-05\n",
      "Time for epoch 19 is 252.6467480659485 sec\n",
      "Steps : 106900, \t Total Gen Loss : 34.028541564941406, \t Total Dis Loss : 7.685938180657104e-05\n",
      "Steps : 107000, \t Total Gen Loss : 32.58412551879883, \t Total Dis Loss : 4.3138272303622216e-05\n",
      "Steps : 107100, \t Total Gen Loss : 31.940793991088867, \t Total Dis Loss : 0.00030652937130071223\n",
      "Steps : 107200, \t Total Gen Loss : 31.654705047607422, \t Total Dis Loss : 0.00011793527664849535\n",
      "Steps : 107300, \t Total Gen Loss : 31.4146671295166, \t Total Dis Loss : 2.5962714062188752e-05\n",
      "Steps : 107400, \t Total Gen Loss : 30.43279266357422, \t Total Dis Loss : 0.00011654574336716905\n",
      "Steps : 107500, \t Total Gen Loss : 32.209625244140625, \t Total Dis Loss : 0.01522955484688282\n",
      "Steps : 107600, \t Total Gen Loss : 31.429672241210938, \t Total Dis Loss : 0.00015313492622226477\n",
      "Steps : 107700, \t Total Gen Loss : 34.1904296875, \t Total Dis Loss : 0.0027560642920434475\n",
      "Steps : 107800, \t Total Gen Loss : 28.447601318359375, \t Total Dis Loss : 0.00030835618963465095\n",
      "Steps : 107900, \t Total Gen Loss : 31.265037536621094, \t Total Dis Loss : 0.00043851928785443306\n",
      "Steps : 108000, \t Total Gen Loss : 27.385570526123047, \t Total Dis Loss : 0.0003951053658965975\n",
      "Steps : 108100, \t Total Gen Loss : 29.164199829101562, \t Total Dis Loss : 0.0004904711386188865\n",
      "Steps : 108200, \t Total Gen Loss : 31.741456985473633, \t Total Dis Loss : 0.0001952944730874151\n",
      "Steps : 108300, \t Total Gen Loss : 37.3588752746582, \t Total Dis Loss : 0.0018439614214003086\n",
      "Steps : 108400, \t Total Gen Loss : 32.29164123535156, \t Total Dis Loss : 0.0001708717318251729\n",
      "Steps : 108500, \t Total Gen Loss : 33.187408447265625, \t Total Dis Loss : 0.00011990727216470987\n",
      "Steps : 108600, \t Total Gen Loss : 31.28958511352539, \t Total Dis Loss : 7.766079215798527e-05\n",
      "Steps : 108700, \t Total Gen Loss : 32.79169845581055, \t Total Dis Loss : 0.00052180967759341\n",
      "Steps : 108800, \t Total Gen Loss : 33.60329818725586, \t Total Dis Loss : 0.00027609654353000224\n",
      "Steps : 108900, \t Total Gen Loss : 30.209186553955078, \t Total Dis Loss : 0.00010398632002761588\n",
      "Steps : 109000, \t Total Gen Loss : 27.182720184326172, \t Total Dis Loss : 0.000299229403026402\n",
      "Steps : 109100, \t Total Gen Loss : 31.585962295532227, \t Total Dis Loss : 7.857284799683839e-05\n",
      "Steps : 109200, \t Total Gen Loss : 28.763652801513672, \t Total Dis Loss : 0.00010517019836697727\n",
      "Steps : 109300, \t Total Gen Loss : 28.588539123535156, \t Total Dis Loss : 0.00022452960547525436\n",
      "Steps : 109400, \t Total Gen Loss : 32.24980163574219, \t Total Dis Loss : 0.00012458990386221558\n",
      "Steps : 109500, \t Total Gen Loss : 34.2222900390625, \t Total Dis Loss : 0.00048669325769878924\n",
      "Steps : 109600, \t Total Gen Loss : 27.477676391601562, \t Total Dis Loss : 0.00045966904144734144\n",
      "Steps : 109700, \t Total Gen Loss : 34.31711959838867, \t Total Dis Loss : 0.0006750549655407667\n",
      "Steps : 109800, \t Total Gen Loss : 29.22454071044922, \t Total Dis Loss : 0.0006788159953430295\n",
      "Steps : 109900, \t Total Gen Loss : 31.677268981933594, \t Total Dis Loss : 4.231611092109233e-05\n",
      "Steps : 110000, \t Total Gen Loss : 34.100948333740234, \t Total Dis Loss : 1.3891853086533956e-05\n",
      "Steps : 110100, \t Total Gen Loss : 28.243911743164062, \t Total Dis Loss : 0.006824243348091841\n",
      "Steps : 110200, \t Total Gen Loss : 33.58746337890625, \t Total Dis Loss : 0.000378802273189649\n",
      "Steps : 110300, \t Total Gen Loss : 31.304044723510742, \t Total Dis Loss : 2.9855909815523773e-05\n",
      "Steps : 110400, \t Total Gen Loss : 26.729496002197266, \t Total Dis Loss : 0.0011010899906978011\n",
      "Steps : 110500, \t Total Gen Loss : 27.56875991821289, \t Total Dis Loss : 0.010836114175617695\n",
      "Steps : 110600, \t Total Gen Loss : 31.87406349182129, \t Total Dis Loss : 0.0003126968804281205\n",
      "Steps : 110700, \t Total Gen Loss : 34.14814376831055, \t Total Dis Loss : 0.00023225323820952326\n",
      "Steps : 110800, \t Total Gen Loss : 29.37580680847168, \t Total Dis Loss : 0.0001974855113076046\n",
      "Steps : 110900, \t Total Gen Loss : 31.76929473876953, \t Total Dis Loss : 5.378816422307864e-05\n",
      "Steps : 111000, \t Total Gen Loss : 31.926515579223633, \t Total Dis Loss : 4.758259456139058e-05\n",
      "Steps : 111100, \t Total Gen Loss : 33.13365936279297, \t Total Dis Loss : 0.0005729261902160943\n",
      "Steps : 111200, \t Total Gen Loss : 31.893903732299805, \t Total Dis Loss : 0.0019940922502428293\n",
      "Steps : 111300, \t Total Gen Loss : 32.53667449951172, \t Total Dis Loss : 0.0003662786621134728\n",
      "Steps : 111400, \t Total Gen Loss : 32.98627471923828, \t Total Dis Loss : 5.562875958275981e-05\n",
      "Steps : 111500, \t Total Gen Loss : 29.55936050415039, \t Total Dis Loss : 0.00021521872258745134\n",
      "Steps : 111600, \t Total Gen Loss : 31.669147491455078, \t Total Dis Loss : 0.0007765312329865992\n",
      "Steps : 111700, \t Total Gen Loss : 28.240707397460938, \t Total Dis Loss : 0.00047135367640294135\n",
      "Steps : 111800, \t Total Gen Loss : 26.541854858398438, \t Total Dis Loss : 0.006020106840878725\n",
      "Steps : 111900, \t Total Gen Loss : 28.59995460510254, \t Total Dis Loss : 0.00030141568277031183\n",
      "Steps : 112000, \t Total Gen Loss : 28.118507385253906, \t Total Dis Loss : 0.0003409348428249359\n",
      "Steps : 112100, \t Total Gen Loss : 29.35169219970703, \t Total Dis Loss : 0.0004248914192430675\n",
      "Steps : 112200, \t Total Gen Loss : 29.045047760009766, \t Total Dis Loss : 0.0002623668115120381\n",
      "Steps : 112300, \t Total Gen Loss : 29.18259620666504, \t Total Dis Loss : 0.00035006413236260414\n",
      "Steps : 112400, \t Total Gen Loss : 25.929744720458984, \t Total Dis Loss : 0.006723187863826752\n",
      "Steps : 112500, \t Total Gen Loss : 28.927738189697266, \t Total Dis Loss : 0.00019815081031993032\n",
      "Time for epoch 20 is 254.25287175178528 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 112600, \t Total Gen Loss : 31.683826446533203, \t Total Dis Loss : 0.0005353831220418215\n",
      "Steps : 112700, \t Total Gen Loss : 30.534774780273438, \t Total Dis Loss : 0.0008878294611349702\n",
      "Steps : 112800, \t Total Gen Loss : 32.59077835083008, \t Total Dis Loss : 0.0019425852224230766\n",
      "Steps : 112900, \t Total Gen Loss : 30.447132110595703, \t Total Dis Loss : 0.00043131483835168183\n",
      "Steps : 113000, \t Total Gen Loss : 26.81187629699707, \t Total Dis Loss : 0.009572089649736881\n",
      "Steps : 113100, \t Total Gen Loss : 31.579118728637695, \t Total Dis Loss : 0.0002536463434807956\n",
      "Steps : 113200, \t Total Gen Loss : 34.06884765625, \t Total Dis Loss : 0.0002594527031760663\n",
      "Steps : 113300, \t Total Gen Loss : 26.16154670715332, \t Total Dis Loss : 0.0004662573919631541\n",
      "Steps : 113400, \t Total Gen Loss : 27.968154907226562, \t Total Dis Loss : 0.0007260688580572605\n",
      "Steps : 113500, \t Total Gen Loss : 31.987171173095703, \t Total Dis Loss : 0.00022438677842728794\n",
      "Steps : 113600, \t Total Gen Loss : 27.70012855529785, \t Total Dis Loss : 0.00240265647880733\n",
      "Steps : 113700, \t Total Gen Loss : 30.472515106201172, \t Total Dis Loss : 0.0008853834588080645\n",
      "Steps : 113800, \t Total Gen Loss : 29.138629913330078, \t Total Dis Loss : 0.0008566858014091849\n",
      "Steps : 113900, \t Total Gen Loss : 23.535688400268555, \t Total Dis Loss : 0.0015994221903383732\n",
      "Steps : 114000, \t Total Gen Loss : 30.18094825744629, \t Total Dis Loss : 0.0003582135250326246\n",
      "Steps : 114100, \t Total Gen Loss : 27.866561889648438, \t Total Dis Loss : 0.0008720298064872622\n",
      "Steps : 114200, \t Total Gen Loss : 32.55768966674805, \t Total Dis Loss : 0.0005072444910183549\n",
      "Steps : 114300, \t Total Gen Loss : 26.50212860107422, \t Total Dis Loss : 0.0018205655505880713\n",
      "Steps : 114400, \t Total Gen Loss : 24.41132926940918, \t Total Dis Loss : 0.0009529418894089758\n",
      "Steps : 114500, \t Total Gen Loss : 29.35538673400879, \t Total Dis Loss : 0.0016683602007105947\n",
      "Steps : 114600, \t Total Gen Loss : 26.919227600097656, \t Total Dis Loss : 0.0004933570744469762\n",
      "Steps : 114700, \t Total Gen Loss : 28.65160369873047, \t Total Dis Loss : 0.0010700294515118003\n",
      "Steps : 114800, \t Total Gen Loss : 28.99517059326172, \t Total Dis Loss : 0.45516258478164673\n",
      "Steps : 114900, \t Total Gen Loss : 31.09832763671875, \t Total Dis Loss : 0.000831915414892137\n",
      "Steps : 115000, \t Total Gen Loss : 28.56607437133789, \t Total Dis Loss : 0.0025466810911893845\n",
      "Steps : 115100, \t Total Gen Loss : 29.783584594726562, \t Total Dis Loss : 0.00018258921045344323\n",
      "Steps : 115200, \t Total Gen Loss : 31.217376708984375, \t Total Dis Loss : 0.007132451981306076\n",
      "Steps : 115300, \t Total Gen Loss : 29.32179069519043, \t Total Dis Loss : 0.0036299771163612604\n",
      "Steps : 115400, \t Total Gen Loss : 30.095792770385742, \t Total Dis Loss : 0.0008307311800308526\n",
      "Steps : 115500, \t Total Gen Loss : 28.177913665771484, \t Total Dis Loss : 0.0020612054504454136\n",
      "Steps : 115600, \t Total Gen Loss : 27.715322494506836, \t Total Dis Loss : 0.0012166287051513791\n",
      "Steps : 115700, \t Total Gen Loss : 27.552581787109375, \t Total Dis Loss : 0.0010699920821934938\n",
      "Steps : 115800, \t Total Gen Loss : 27.81390380859375, \t Total Dis Loss : 0.0008711628033779562\n",
      "Steps : 115900, \t Total Gen Loss : 30.67060089111328, \t Total Dis Loss : 0.0005424990085884929\n",
      "Steps : 116000, \t Total Gen Loss : 29.585153579711914, \t Total Dis Loss : 0.0005362828378565609\n",
      "Steps : 116100, \t Total Gen Loss : 28.458707809448242, \t Total Dis Loss : 0.0012996809091418982\n",
      "Steps : 116200, \t Total Gen Loss : 27.422025680541992, \t Total Dis Loss : 0.0002397226489847526\n",
      "Steps : 116300, \t Total Gen Loss : 28.24292755126953, \t Total Dis Loss : 0.016841206699609756\n",
      "Steps : 116400, \t Total Gen Loss : 25.756759643554688, \t Total Dis Loss : 0.0007728483178652823\n",
      "Steps : 116500, \t Total Gen Loss : 28.252666473388672, \t Total Dis Loss : 0.0005634954432025552\n",
      "Steps : 116600, \t Total Gen Loss : 28.02853012084961, \t Total Dis Loss : 0.000737079419195652\n",
      "Steps : 116700, \t Total Gen Loss : 31.50977325439453, \t Total Dis Loss : 0.00017519481480121613\n",
      "Steps : 116800, \t Total Gen Loss : 30.782703399658203, \t Total Dis Loss : 0.0010162096004933119\n",
      "Steps : 116900, \t Total Gen Loss : 27.5341854095459, \t Total Dis Loss : 0.0012923118192702532\n",
      "Steps : 117000, \t Total Gen Loss : 28.178871154785156, \t Total Dis Loss : 0.0012744772247970104\n",
      "Steps : 117100, \t Total Gen Loss : 25.850812911987305, \t Total Dis Loss : 0.000604957458563149\n",
      "Steps : 117200, \t Total Gen Loss : 32.370765686035156, \t Total Dis Loss : 0.004361387807875872\n",
      "Steps : 117300, \t Total Gen Loss : 28.556488037109375, \t Total Dis Loss : 0.0015515533741563559\n",
      "Steps : 117400, \t Total Gen Loss : 24.747098922729492, \t Total Dis Loss : 0.0007204469293355942\n",
      "Steps : 117500, \t Total Gen Loss : 30.247472763061523, \t Total Dis Loss : 0.0012177721364423633\n",
      "Steps : 117600, \t Total Gen Loss : 25.44449234008789, \t Total Dis Loss : 0.4181327819824219\n",
      "Steps : 117700, \t Total Gen Loss : 29.626338958740234, \t Total Dis Loss : 0.0006578477332368493\n",
      "Steps : 117800, \t Total Gen Loss : 29.766380310058594, \t Total Dis Loss : 0.0009087944636121392\n",
      "Steps : 117900, \t Total Gen Loss : 27.389663696289062, \t Total Dis Loss : 0.00045940306154079735\n",
      "Steps : 118000, \t Total Gen Loss : 28.203168869018555, \t Total Dis Loss : 0.0010617889929562807\n",
      "Steps : 118100, \t Total Gen Loss : 28.981565475463867, \t Total Dis Loss : 0.0007827844237908721\n",
      "Time for epoch 21 is 255.14993524551392 sec\n",
      "Steps : 118200, \t Total Gen Loss : 28.312335968017578, \t Total Dis Loss : 0.00016777197015471756\n",
      "Steps : 118300, \t Total Gen Loss : 30.898706436157227, \t Total Dis Loss : 0.0007850890979170799\n",
      "Steps : 118400, \t Total Gen Loss : 29.81458854675293, \t Total Dis Loss : 0.0005844819825142622\n",
      "Steps : 118500, \t Total Gen Loss : 30.89942741394043, \t Total Dis Loss : 0.0005087522440589964\n",
      "Steps : 118600, \t Total Gen Loss : 30.79537010192871, \t Total Dis Loss : 0.0010683174477890134\n",
      "Steps : 118700, \t Total Gen Loss : 26.40678596496582, \t Total Dis Loss : 0.00038149493047967553\n",
      "Steps : 118800, \t Total Gen Loss : 29.802562713623047, \t Total Dis Loss : 9.794335346668959e-05\n",
      "Steps : 118900, \t Total Gen Loss : 28.374208450317383, \t Total Dis Loss : 0.00028005908825434744\n",
      "Steps : 119000, \t Total Gen Loss : 25.56064796447754, \t Total Dis Loss : 0.002576567232608795\n",
      "Steps : 119100, \t Total Gen Loss : 26.72948455810547, \t Total Dis Loss : 0.0003066944482270628\n",
      "Steps : 119200, \t Total Gen Loss : 28.481124877929688, \t Total Dis Loss : 0.00036271451972424984\n",
      "Steps : 119300, \t Total Gen Loss : 31.827585220336914, \t Total Dis Loss : 0.00015615079610142857\n",
      "Steps : 119400, \t Total Gen Loss : 26.9778995513916, \t Total Dis Loss : 0.009319022297859192\n",
      "Steps : 119500, \t Total Gen Loss : 31.216272354125977, \t Total Dis Loss : 0.0003299997770227492\n",
      "Steps : 119600, \t Total Gen Loss : 28.922080993652344, \t Total Dis Loss : 0.00021321093663573265\n",
      "Steps : 119700, \t Total Gen Loss : 28.251110076904297, \t Total Dis Loss : 0.0002832203754223883\n",
      "Steps : 119800, \t Total Gen Loss : 30.002239227294922, \t Total Dis Loss : 0.0003857331466861069\n",
      "Steps : 119900, \t Total Gen Loss : 28.802955627441406, \t Total Dis Loss : 0.013957918621599674\n",
      "Steps : 120000, \t Total Gen Loss : 30.41354751586914, \t Total Dis Loss : 0.0007617396186105907\n",
      "Steps : 120100, \t Total Gen Loss : 29.15117645263672, \t Total Dis Loss : 0.0006326924776658416\n",
      "Steps : 120200, \t Total Gen Loss : 27.264888763427734, \t Total Dis Loss : 0.0002602645836304873\n",
      "Steps : 120300, \t Total Gen Loss : 31.294841766357422, \t Total Dis Loss : 0.0003084668715018779\n",
      "Steps : 120400, \t Total Gen Loss : 30.600643157958984, \t Total Dis Loss : 0.0003312042390462011\n",
      "Steps : 120500, \t Total Gen Loss : 25.37181282043457, \t Total Dis Loss : 0.001899095019325614\n",
      "Steps : 120600, \t Total Gen Loss : 24.785390853881836, \t Total Dis Loss : 0.0017147362232208252\n",
      "Steps : 120700, \t Total Gen Loss : 26.064868927001953, \t Total Dis Loss : 0.0012875153915956616\n",
      "Steps : 120800, \t Total Gen Loss : 25.724449157714844, \t Total Dis Loss : 0.0005312327994033694\n",
      "Steps : 120900, \t Total Gen Loss : 27.741085052490234, \t Total Dis Loss : 0.000988880987279117\n",
      "Steps : 121000, \t Total Gen Loss : 27.48676109313965, \t Total Dis Loss : 0.00044470964348874986\n",
      "Steps : 121100, \t Total Gen Loss : 27.020103454589844, \t Total Dis Loss : 0.00024762042448855937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 121200, \t Total Gen Loss : 26.91653060913086, \t Total Dis Loss : 0.0003686718118842691\n",
      "Steps : 121300, \t Total Gen Loss : 33.37714385986328, \t Total Dis Loss : 0.0008105030283331871\n",
      "Steps : 121400, \t Total Gen Loss : 26.25804328918457, \t Total Dis Loss : 0.0014413780299946666\n",
      "Steps : 121500, \t Total Gen Loss : 28.80424690246582, \t Total Dis Loss : 0.00028413638938218355\n",
      "Steps : 121600, \t Total Gen Loss : 25.606863021850586, \t Total Dis Loss : 0.0011264235945418477\n",
      "Steps : 121700, \t Total Gen Loss : 28.962629318237305, \t Total Dis Loss : 0.0010022621136158705\n",
      "Steps : 121800, \t Total Gen Loss : 31.509241104125977, \t Total Dis Loss : 0.0001488233101554215\n",
      "Steps : 121900, \t Total Gen Loss : 31.030643463134766, \t Total Dis Loss : 0.00017353903967887163\n",
      "Steps : 122000, \t Total Gen Loss : 26.24771499633789, \t Total Dis Loss : 0.0014122597640380263\n",
      "Steps : 122100, \t Total Gen Loss : 26.318492889404297, \t Total Dis Loss : 0.0005838106153532863\n",
      "Steps : 122200, \t Total Gen Loss : 29.13935089111328, \t Total Dis Loss : 0.0003652773448266089\n",
      "Steps : 122300, \t Total Gen Loss : 32.27336883544922, \t Total Dis Loss : 0.00021291442681103945\n",
      "Steps : 122400, \t Total Gen Loss : 27.834978103637695, \t Total Dis Loss : 0.0004193673375993967\n",
      "Steps : 122500, \t Total Gen Loss : 28.031675338745117, \t Total Dis Loss : 0.00020450596639420837\n",
      "Steps : 122600, \t Total Gen Loss : 30.243701934814453, \t Total Dis Loss : 0.00017423230747226626\n",
      "Steps : 122700, \t Total Gen Loss : 27.38701629638672, \t Total Dis Loss : 0.00014250144886318594\n",
      "Steps : 122800, \t Total Gen Loss : 27.935178756713867, \t Total Dis Loss : 0.0006363872089423239\n",
      "Steps : 122900, \t Total Gen Loss : 29.657968521118164, \t Total Dis Loss : 0.00032770427060313523\n",
      "Steps : 123000, \t Total Gen Loss : 31.338178634643555, \t Total Dis Loss : 5.143698217580095e-05\n",
      "Steps : 123100, \t Total Gen Loss : 31.917831420898438, \t Total Dis Loss : 0.0008865053532645106\n",
      "Steps : 123200, \t Total Gen Loss : 27.009029388427734, \t Total Dis Loss : 0.0002887614245992154\n",
      "Steps : 123300, \t Total Gen Loss : 28.57400131225586, \t Total Dis Loss : 0.0007181772380135953\n",
      "Steps : 123400, \t Total Gen Loss : 28.40604591369629, \t Total Dis Loss : 0.005233914125710726\n",
      "Steps : 123500, \t Total Gen Loss : 29.210186004638672, \t Total Dis Loss : 0.0009143662173300982\n",
      "Steps : 123600, \t Total Gen Loss : 25.65811538696289, \t Total Dis Loss : 0.0005211621173657477\n",
      "Steps : 123700, \t Total Gen Loss : 26.90492820739746, \t Total Dis Loss : 0.0005048941820859909\n",
      "Time for epoch 22 is 256.3717665672302 sec\n",
      "Steps : 123800, \t Total Gen Loss : 35.7446174621582, \t Total Dis Loss : 0.00016228297317866236\n",
      "Steps : 123900, \t Total Gen Loss : 28.95205307006836, \t Total Dis Loss : 0.000601248990278691\n",
      "Steps : 124000, \t Total Gen Loss : 26.816482543945312, \t Total Dis Loss : 0.003442588960751891\n",
      "Steps : 124100, \t Total Gen Loss : 26.303924560546875, \t Total Dis Loss : 0.0005671868566423655\n",
      "Steps : 124200, \t Total Gen Loss : 29.489334106445312, \t Total Dis Loss : 0.0007861161720938981\n",
      "Steps : 124300, \t Total Gen Loss : 28.53290367126465, \t Total Dis Loss : 0.0005407631397247314\n",
      "Steps : 124400, \t Total Gen Loss : 29.82394027709961, \t Total Dis Loss : 0.0005046249134466052\n",
      "Steps : 124500, \t Total Gen Loss : 28.105159759521484, \t Total Dis Loss : 0.0003742226690519601\n",
      "Steps : 124600, \t Total Gen Loss : 29.23975944519043, \t Total Dis Loss : 0.0002320503699593246\n",
      "Steps : 124700, \t Total Gen Loss : 31.36907958984375, \t Total Dis Loss : 0.0001101707894122228\n",
      "Steps : 124800, \t Total Gen Loss : 31.504262924194336, \t Total Dis Loss : 9.15187265491113e-05\n",
      "Steps : 124900, \t Total Gen Loss : 26.543537139892578, \t Total Dis Loss : 0.0005115637322887778\n",
      "Steps : 125000, \t Total Gen Loss : 28.947444915771484, \t Total Dis Loss : 0.0005824455874972045\n",
      "Steps : 125100, \t Total Gen Loss : 29.37123680114746, \t Total Dis Loss : 3.169568662997335e-05\n",
      "Steps : 125200, \t Total Gen Loss : 29.435527801513672, \t Total Dis Loss : 0.0005060540279373527\n",
      "Steps : 125300, \t Total Gen Loss : 30.329280853271484, \t Total Dis Loss : 0.004381672944873571\n",
      "Steps : 125400, \t Total Gen Loss : 32.13679885864258, \t Total Dis Loss : 0.0009494443656876683\n",
      "Steps : 125500, \t Total Gen Loss : 31.20526885986328, \t Total Dis Loss : 0.0007626059232279658\n",
      "Steps : 125600, \t Total Gen Loss : 27.48168182373047, \t Total Dis Loss : 0.0004441975906956941\n",
      "Steps : 125700, \t Total Gen Loss : 28.236530303955078, \t Total Dis Loss : 0.0002637219149619341\n",
      "Steps : 125800, \t Total Gen Loss : 28.46624183654785, \t Total Dis Loss : 0.00017209780344273895\n",
      "Steps : 125900, \t Total Gen Loss : 33.12294006347656, \t Total Dis Loss : 7.411182741634548e-05\n",
      "Steps : 126000, \t Total Gen Loss : 31.57575798034668, \t Total Dis Loss : 0.0002166556951124221\n",
      "Steps : 126100, \t Total Gen Loss : 26.8071346282959, \t Total Dis Loss : 0.0002653016126714647\n",
      "Steps : 126200, \t Total Gen Loss : 27.84650993347168, \t Total Dis Loss : 0.0006726522697135806\n",
      "Steps : 126300, \t Total Gen Loss : 34.84789276123047, \t Total Dis Loss : 0.0004992823232896626\n",
      "Steps : 126400, \t Total Gen Loss : 27.585613250732422, \t Total Dis Loss : 0.0006091154646128416\n",
      "Steps : 126500, \t Total Gen Loss : 24.815532684326172, \t Total Dis Loss : 0.009569243527948856\n",
      "Steps : 126600, \t Total Gen Loss : 29.42576789855957, \t Total Dis Loss : 0.00014110787014942616\n",
      "Steps : 126700, \t Total Gen Loss : 25.35325813293457, \t Total Dis Loss : 0.00031929026590660214\n",
      "Steps : 126800, \t Total Gen Loss : 26.474157333374023, \t Total Dis Loss : 0.0003479456063359976\n",
      "Steps : 126900, \t Total Gen Loss : 25.36606788635254, \t Total Dis Loss : 0.002252367325127125\n",
      "Steps : 127000, \t Total Gen Loss : 24.828001022338867, \t Total Dis Loss : 0.0006548948585987091\n",
      "Steps : 127100, \t Total Gen Loss : 32.43883514404297, \t Total Dis Loss : 0.0003341403207741678\n",
      "Steps : 127200, \t Total Gen Loss : 31.268142700195312, \t Total Dis Loss : 0.00032023515086621046\n",
      "Steps : 127300, \t Total Gen Loss : 30.702560424804688, \t Total Dis Loss : 0.00023458735086023808\n",
      "Steps : 127400, \t Total Gen Loss : 28.292816162109375, \t Total Dis Loss : 0.0001679008564678952\n",
      "Steps : 127500, \t Total Gen Loss : 27.69158172607422, \t Total Dis Loss : 0.0006664799875579774\n",
      "Steps : 127600, \t Total Gen Loss : 26.25470733642578, \t Total Dis Loss : 0.03870654106140137\n",
      "Steps : 127700, \t Total Gen Loss : 30.61771011352539, \t Total Dis Loss : 0.00037743145367130637\n",
      "Steps : 127800, \t Total Gen Loss : 29.855392456054688, \t Total Dis Loss : 0.0003424269671086222\n",
      "Steps : 127900, \t Total Gen Loss : 30.266845703125, \t Total Dis Loss : 0.00017062581900972873\n",
      "Steps : 128000, \t Total Gen Loss : 27.485031127929688, \t Total Dis Loss : 0.0010200492106378078\n",
      "Steps : 128100, \t Total Gen Loss : 30.07070541381836, \t Total Dis Loss : 0.0006434355163946748\n",
      "Steps : 128200, \t Total Gen Loss : 29.542078018188477, \t Total Dis Loss : 0.000464142911368981\n",
      "Steps : 128300, \t Total Gen Loss : 28.226499557495117, \t Total Dis Loss : 0.0004040354979224503\n",
      "Steps : 128400, \t Total Gen Loss : 30.418354034423828, \t Total Dis Loss : 0.0002420488017378375\n",
      "Steps : 128500, \t Total Gen Loss : 30.279638290405273, \t Total Dis Loss : 0.0005264015053398907\n",
      "Steps : 128600, \t Total Gen Loss : 32.033164978027344, \t Total Dis Loss : 0.00022536510368809104\n",
      "Steps : 128700, \t Total Gen Loss : 32.76370620727539, \t Total Dis Loss : 0.00018482099403627217\n",
      "Steps : 128800, \t Total Gen Loss : 30.617450714111328, \t Total Dis Loss : 0.0006669472204521298\n",
      "Steps : 128900, \t Total Gen Loss : 27.253204345703125, \t Total Dis Loss : 0.0003288316656835377\n",
      "Steps : 129000, \t Total Gen Loss : 30.918392181396484, \t Total Dis Loss : 0.0003935097483918071\n",
      "Steps : 129100, \t Total Gen Loss : 27.470916748046875, \t Total Dis Loss : 0.0004382459737826139\n",
      "Steps : 129200, \t Total Gen Loss : 29.410547256469727, \t Total Dis Loss : 0.0011286251246929169\n",
      "Steps : 129300, \t Total Gen Loss : 26.79189109802246, \t Total Dis Loss : 0.0011333582224324346\n",
      "Time for epoch 23 is 256.9950952529907 sec\n",
      "Steps : 129400, \t Total Gen Loss : 25.061824798583984, \t Total Dis Loss : 0.0011238798033446074\n",
      "Steps : 129500, \t Total Gen Loss : 28.907419204711914, \t Total Dis Loss : 0.0005997525877319276\n",
      "Steps : 129600, \t Total Gen Loss : 32.94069290161133, \t Total Dis Loss : 0.00015726627316325903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 129700, \t Total Gen Loss : 29.693979263305664, \t Total Dis Loss : 0.0003656541812233627\n",
      "Steps : 129800, \t Total Gen Loss : 25.336442947387695, \t Total Dis Loss : 0.0005365568795241416\n",
      "Steps : 129900, \t Total Gen Loss : 28.187623977661133, \t Total Dis Loss : 0.0003813967341557145\n",
      "Steps : 130000, \t Total Gen Loss : 30.093643188476562, \t Total Dis Loss : 0.00018855211965274066\n",
      "Steps : 130100, \t Total Gen Loss : 29.640155792236328, \t Total Dis Loss : 0.00042706960812211037\n",
      "Steps : 130200, \t Total Gen Loss : 28.830732345581055, \t Total Dis Loss : 0.0004681189311668277\n",
      "Steps : 130300, \t Total Gen Loss : 30.4169864654541, \t Total Dis Loss : 0.000423058052547276\n",
      "Steps : 130400, \t Total Gen Loss : 25.345422744750977, \t Total Dis Loss : 0.00037245842395350337\n",
      "Steps : 130500, \t Total Gen Loss : 27.110214233398438, \t Total Dis Loss : 0.0006010963697917759\n",
      "Steps : 130600, \t Total Gen Loss : 31.48415756225586, \t Total Dis Loss : 4.379290112410672e-05\n",
      "Steps : 130700, \t Total Gen Loss : 32.01853561401367, \t Total Dis Loss : 0.0001376130967400968\n",
      "Steps : 130800, \t Total Gen Loss : 27.676570892333984, \t Total Dis Loss : 0.00032694509718567133\n",
      "Steps : 130900, \t Total Gen Loss : 33.603431701660156, \t Total Dis Loss : 0.0002324757952010259\n",
      "Steps : 131000, \t Total Gen Loss : 28.96982765197754, \t Total Dis Loss : 0.0004190443141851574\n",
      "Steps : 131100, \t Total Gen Loss : 30.081727981567383, \t Total Dis Loss : 0.000523742288351059\n",
      "Steps : 131200, \t Total Gen Loss : 29.16973876953125, \t Total Dis Loss : 0.00042760674841701984\n",
      "Steps : 131300, \t Total Gen Loss : 27.26570701599121, \t Total Dis Loss : 0.00017063836276065558\n",
      "Steps : 131400, \t Total Gen Loss : 28.48269271850586, \t Total Dis Loss : 0.00047947769053280354\n",
      "Steps : 131500, \t Total Gen Loss : 26.832950592041016, \t Total Dis Loss : 0.0003395820676814765\n",
      "Steps : 131600, \t Total Gen Loss : 25.483510971069336, \t Total Dis Loss : 0.0005810224683955312\n",
      "Steps : 131700, \t Total Gen Loss : 28.8575382232666, \t Total Dis Loss : 0.00044615764636546373\n",
      "Steps : 131800, \t Total Gen Loss : 29.50303077697754, \t Total Dis Loss : 0.00025244487915188074\n",
      "Steps : 131900, \t Total Gen Loss : 31.405183792114258, \t Total Dis Loss : 0.00023242122551891953\n",
      "Steps : 132000, \t Total Gen Loss : 30.97753143310547, \t Total Dis Loss : 0.00011776277096942067\n",
      "Steps : 132100, \t Total Gen Loss : 29.622310638427734, \t Total Dis Loss : 0.001221523038111627\n",
      "Steps : 132200, \t Total Gen Loss : 30.38091278076172, \t Total Dis Loss : 0.0012291265884414315\n",
      "Steps : 132300, \t Total Gen Loss : 28.459758758544922, \t Total Dis Loss : 0.0006498456350527704\n",
      "Steps : 132400, \t Total Gen Loss : 29.933774948120117, \t Total Dis Loss : 0.00033261600765399635\n",
      "Steps : 132500, \t Total Gen Loss : 29.480361938476562, \t Total Dis Loss : 0.00035525645944289863\n",
      "Steps : 132600, \t Total Gen Loss : 28.048614501953125, \t Total Dis Loss : 0.0004086795961484313\n",
      "Steps : 132700, \t Total Gen Loss : 25.1749210357666, \t Total Dis Loss : 0.000686002429574728\n",
      "Steps : 132800, \t Total Gen Loss : 26.840953826904297, \t Total Dis Loss : 0.0010010189143940806\n",
      "Steps : 132900, \t Total Gen Loss : 31.400306701660156, \t Total Dis Loss : 2.407702777418308e-05\n",
      "Steps : 133000, \t Total Gen Loss : 36.670257568359375, \t Total Dis Loss : 0.0019050103146582842\n",
      "Steps : 133100, \t Total Gen Loss : 27.824918746948242, \t Total Dis Loss : 0.00043048473889939487\n",
      "Steps : 133200, \t Total Gen Loss : 26.584745407104492, \t Total Dis Loss : 0.0006365507724694908\n",
      "Steps : 133300, \t Total Gen Loss : 27.694862365722656, \t Total Dis Loss : 0.0003406694158911705\n",
      "Steps : 133400, \t Total Gen Loss : 31.812166213989258, \t Total Dis Loss : 0.00013820029562339187\n",
      "Steps : 133500, \t Total Gen Loss : 27.742259979248047, \t Total Dis Loss : 0.0002672867849469185\n",
      "Steps : 133600, \t Total Gen Loss : 31.841772079467773, \t Total Dis Loss : 0.00011431236634962261\n",
      "Steps : 133700, \t Total Gen Loss : 27.721538543701172, \t Total Dis Loss : 0.0005187123315408826\n",
      "Steps : 133800, \t Total Gen Loss : 31.540164947509766, \t Total Dis Loss : 0.0002543609298299998\n",
      "Steps : 133900, \t Total Gen Loss : 25.197673797607422, \t Total Dis Loss : 0.00048417277866974473\n",
      "Steps : 134000, \t Total Gen Loss : 29.69130516052246, \t Total Dis Loss : 0.00015876679390203208\n",
      "Steps : 134100, \t Total Gen Loss : 27.222246170043945, \t Total Dis Loss : 0.0001405430375598371\n",
      "Steps : 134200, \t Total Gen Loss : 31.995248794555664, \t Total Dis Loss : 0.000143898549140431\n",
      "Steps : 134300, \t Total Gen Loss : 31.870647430419922, \t Total Dis Loss : 0.00011249964882154018\n",
      "Steps : 134400, \t Total Gen Loss : 28.762596130371094, \t Total Dis Loss : 0.00016794457042124122\n",
      "Steps : 134500, \t Total Gen Loss : 31.01836585998535, \t Total Dis Loss : 3.083960837102495e-05\n",
      "Steps : 134600, \t Total Gen Loss : 31.636520385742188, \t Total Dis Loss : 0.00030022687860764563\n",
      "Steps : 134700, \t Total Gen Loss : 27.878137588500977, \t Total Dis Loss : 0.00021238296176306903\n",
      "Steps : 134800, \t Total Gen Loss : 32.29977035522461, \t Total Dis Loss : 0.00022656406508758664\n",
      "Steps : 134900, \t Total Gen Loss : 27.901857376098633, \t Total Dis Loss : 0.0009381044656038284\n",
      "Steps : 135000, \t Total Gen Loss : 28.644187927246094, \t Total Dis Loss : 0.0003061967436224222\n",
      "Time for epoch 24 is 255.22075033187866 sec\n",
      "Steps : 135100, \t Total Gen Loss : 27.209102630615234, \t Total Dis Loss : 0.0002721211058087647\n",
      "Steps : 135200, \t Total Gen Loss : 27.31913185119629, \t Total Dis Loss : 0.0003238005156163126\n",
      "Steps : 135300, \t Total Gen Loss : 28.54102325439453, \t Total Dis Loss : 0.00019533996237441897\n",
      "Steps : 135400, \t Total Gen Loss : 29.835477828979492, \t Total Dis Loss : 0.0014394287718459964\n",
      "Steps : 135500, \t Total Gen Loss : 28.405364990234375, \t Total Dis Loss : 0.00018709503638092428\n",
      "Steps : 135600, \t Total Gen Loss : 29.681747436523438, \t Total Dis Loss : 0.0002322499785805121\n",
      "Steps : 135700, \t Total Gen Loss : 26.239990234375, \t Total Dis Loss : 0.0006371265044435859\n",
      "Steps : 135800, \t Total Gen Loss : 26.974769592285156, \t Total Dis Loss : 0.0003842886071652174\n",
      "Steps : 135900, \t Total Gen Loss : 27.648202896118164, \t Total Dis Loss : 0.00047226971946656704\n",
      "Steps : 136000, \t Total Gen Loss : 31.426637649536133, \t Total Dis Loss : 0.0009224891546182334\n",
      "Steps : 136100, \t Total Gen Loss : 27.592052459716797, \t Total Dis Loss : 0.00010937821207335219\n",
      "Steps : 136200, \t Total Gen Loss : 31.168968200683594, \t Total Dis Loss : 6.662939267698675e-05\n",
      "Steps : 136300, \t Total Gen Loss : 28.722118377685547, \t Total Dis Loss : 0.00018944873590953648\n",
      "Steps : 136400, \t Total Gen Loss : 32.535438537597656, \t Total Dis Loss : 0.00014468775771092623\n",
      "Steps : 136500, \t Total Gen Loss : 28.73432159423828, \t Total Dis Loss : 0.0006098153535276651\n",
      "Steps : 136600, \t Total Gen Loss : 26.12892723083496, \t Total Dis Loss : 0.0004004043585155159\n",
      "Steps : 136700, \t Total Gen Loss : 27.58995246887207, \t Total Dis Loss : 0.000812811718787998\n",
      "Steps : 136800, \t Total Gen Loss : 27.636070251464844, \t Total Dis Loss : 0.002558379899710417\n",
      "Steps : 136900, \t Total Gen Loss : 25.74513053894043, \t Total Dis Loss : 0.0008925850270316005\n",
      "Steps : 137000, \t Total Gen Loss : 33.18668746948242, \t Total Dis Loss : 0.0001366821234114468\n",
      "Steps : 137100, \t Total Gen Loss : 27.028812408447266, \t Total Dis Loss : 0.00041059436625801027\n",
      "Steps : 137200, \t Total Gen Loss : 26.534732818603516, \t Total Dis Loss : 0.0002682004706002772\n",
      "Steps : 137300, \t Total Gen Loss : 30.21534538269043, \t Total Dis Loss : 0.00010959096834994853\n",
      "Steps : 137400, \t Total Gen Loss : 26.563573837280273, \t Total Dis Loss : 0.00017866211419459432\n",
      "Steps : 137500, \t Total Gen Loss : 31.87502098083496, \t Total Dis Loss : 0.0001284903264604509\n",
      "Steps : 137600, \t Total Gen Loss : 29.07441520690918, \t Total Dis Loss : 0.011990365572273731\n",
      "Steps : 137700, \t Total Gen Loss : 29.187185287475586, \t Total Dis Loss : 0.00017186536570079625\n",
      "Steps : 137800, \t Total Gen Loss : 26.2464599609375, \t Total Dis Loss : 0.0013040000339969993\n",
      "Steps : 137900, \t Total Gen Loss : 29.251300811767578, \t Total Dis Loss : 0.0005346960970200598\n",
      "Steps : 138000, \t Total Gen Loss : 28.385093688964844, \t Total Dis Loss : 0.00024351973843295127\n",
      "Steps : 138100, \t Total Gen Loss : 33.713871002197266, \t Total Dis Loss : 8.689913374837488e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 138200, \t Total Gen Loss : 31.205848693847656, \t Total Dis Loss : 0.0003236149495933205\n",
      "Steps : 138300, \t Total Gen Loss : 31.484128952026367, \t Total Dis Loss : 4.640112092602067e-05\n",
      "Steps : 138400, \t Total Gen Loss : 29.187366485595703, \t Total Dis Loss : 0.00033551949309185147\n",
      "Steps : 138500, \t Total Gen Loss : 28.3651065826416, \t Total Dis Loss : 0.0003903459873981774\n",
      "Steps : 138600, \t Total Gen Loss : 26.139341354370117, \t Total Dis Loss : 0.0005306632374413311\n",
      "Steps : 138700, \t Total Gen Loss : 26.431785583496094, \t Total Dis Loss : 0.0008910518954508007\n",
      "Steps : 138800, \t Total Gen Loss : 29.657073974609375, \t Total Dis Loss : 0.0004116565396543592\n",
      "Steps : 138900, \t Total Gen Loss : 31.523086547851562, \t Total Dis Loss : 0.00025550933787599206\n",
      "Steps : 139000, \t Total Gen Loss : 28.745620727539062, \t Total Dis Loss : 0.00016580353258177638\n",
      "Steps : 139100, \t Total Gen Loss : 26.93690299987793, \t Total Dis Loss : 0.00020759842300321907\n",
      "Steps : 139200, \t Total Gen Loss : 29.05915069580078, \t Total Dis Loss : 0.0005672440165653825\n",
      "Steps : 139300, \t Total Gen Loss : 29.477338790893555, \t Total Dis Loss : 0.00024995324201881886\n",
      "Steps : 139400, \t Total Gen Loss : 32.647850036621094, \t Total Dis Loss : 0.0035048900172114372\n",
      "Steps : 139500, \t Total Gen Loss : 28.851831436157227, \t Total Dis Loss : 0.0005323024233803153\n",
      "Steps : 139600, \t Total Gen Loss : 28.16227149963379, \t Total Dis Loss : 8.971030183602124e-05\n",
      "Steps : 139700, \t Total Gen Loss : 28.017425537109375, \t Total Dis Loss : 0.000330748240230605\n",
      "Steps : 139800, \t Total Gen Loss : 27.359811782836914, \t Total Dis Loss : 0.0002835373452398926\n",
      "Steps : 139900, \t Total Gen Loss : 27.403053283691406, \t Total Dis Loss : 0.0003726105787791312\n",
      "Steps : 140000, \t Total Gen Loss : 28.774311065673828, \t Total Dis Loss : 0.000853341945912689\n",
      "Steps : 140100, \t Total Gen Loss : 26.269495010375977, \t Total Dis Loss : 0.0009732673643156886\n",
      "Steps : 140200, \t Total Gen Loss : 30.182750701904297, \t Total Dis Loss : 0.002878103405237198\n",
      "Steps : 140300, \t Total Gen Loss : 28.230648040771484, \t Total Dis Loss : 0.00011200538574485108\n",
      "Steps : 140400, \t Total Gen Loss : 29.98955535888672, \t Total Dis Loss : 8.759006595937535e-05\n",
      "Steps : 140500, \t Total Gen Loss : 31.976472854614258, \t Total Dis Loss : 0.0001966861163964495\n",
      "Steps : 140600, \t Total Gen Loss : 29.030746459960938, \t Total Dis Loss : 0.0001916137698572129\n",
      "Time for epoch 25 is 254.0649118423462 sec\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 25\n",
    "steps = 0\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for images, labels in train_dataset:\n",
    "        steps += 1\n",
    "        gen_loss, disc_loss = train_step(images)\n",
    "        \n",
    "        if steps % 100 == 0:\n",
    "            print ('Steps : {}, \\t Total Gen Loss : {}, \\t Total Dis Loss : {}'.format(steps, gen_loss.numpy(), disc_loss.numpy()))\n",
    "        \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_path)\n",
    "        \n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 검증 결과의 시각화\n",
    "### 1. 정상-이상 데이터의 anomaly score 분포 시각화\n",
    "### 2. 적절한 threshold에 따른 이삼감지율 계산\n",
    "### 3. 감지 성공/실패사례 시각화 포함\n",
    "\n",
    "# Model Evaluation\n",
    "- 학습 도중 저장된 Checkpoint를 아래와 같이 활용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x7f2a6b213bd0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate(test_dataset, set_lambda=0.9):\n",
    "    an_scores = []\n",
    "    gt_labels = []\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(test_dataset):\n",
    "        generated_images = generator(x_batch_train, training=True)\n",
    "        _, feat_real = discriminator(x_batch_train, training=True)\n",
    "        _, feat_fake = discriminator(generated_images, training=True)\n",
    "\n",
    "        generated_images, feat_real, feat_fake = generated_images.numpy(), feat_real.numpy(), feat_fake.numpy()        \n",
    "\n",
    "        rec = abs(x_batch_train - generated_images)\n",
    "        lat = (feat_real - feat_fake) ** 2\n",
    "\n",
    "        rec = tf.reduce_sum(rec, [1,2,3])\n",
    "        lat = tf.reduce_sum(lat, [1,2,3])\n",
    "        \n",
    "        error = (set_lambda * tf.cast(rec, tf.float32)) + ((1 - set_lambda) * tf.cast(lat, tf.float32))\n",
    "        \n",
    "        an_scores.append(error)\n",
    "        gt_labels.append(y_batch_train)\n",
    "        \n",
    "    an_scores = np.concatenate(an_scores, axis=0).reshape([-1])\n",
    "    gt_labels = np.concatenate(gt_labels, axis=0).reshape([-1])\n",
    "    \n",
    "    an_scores = (an_scores - np.amin(an_scores)) / (np.amax(an_scores) - np.amin(an_scores))\n",
    "    \n",
    "    return an_scores, gt_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트 결과를 원래 라벨에 따라 anomaly 데이터와 normal 데이터로 나누어 따로 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000 15000\n"
     ]
    }
   ],
   "source": [
    "an_scores, gt_labels = _evaluate(test_dataset)\n",
    "\n",
    "print(len(an_scores), len(gt_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라벨에 따라 anomaly score의 분포가 다르게 나타나는지 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000,)\n",
      "(6000,)\n"
     ]
    }
   ],
   "source": [
    "normal = []\n",
    "anormaly = []\n",
    "for score, label in zip(an_scores, gt_labels):\n",
    "    if label == 0:\n",
    "        anormaly.append(score)\n",
    "    else:\n",
    "        normal.append(score)\n",
    "\n",
    "normal = np.array(normal)\n",
    "print(normal.shape)\n",
    "anormaly = np.array(anormaly)\n",
    "print(anormaly.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정상-이상 데이터의 anomaly score 분포 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(normal, bins=np.linspace(0.0, 1.0, num=100), color = 'b' , label = 'normal')\n",
    "plt.hist(anormaly, bins=np.linspace(0.0, 1.0, num=100), color = 'r' , label = 'anormaly')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트셋 이상감지 정확도(%) - 적절한 threshold에 따른 이상 감지율 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. mean(normal)= 0.373872 2. mean(anormaly)= 0.34054154\n",
      "3. std(normal)= 0.13718238 4. std(anormaly)= 0.1362585\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(normal, norm.pdf(normal, np.mean(normal), np.std(normal)), 'o', color = 'b' , label='normal')\n",
    "plt.plot(anormaly, norm.pdf(anormaly, np.mean(anormaly), np.std(anormaly)), 'o', color = 'r' , label='anormaly')\n",
    "plt.legend()\n",
    "\n",
    "print(\"1. mean(normal)=\", np.mean(normal), \"2. mean(anormaly)=\",np.mean(anormaly))\n",
    "print(\"3. std(normal)=\", np.std(normal), \"4. std(anormaly)=\",np.std(anormaly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이상감지 성공실패사례 제시\n",
    "\n",
    "- CIFAR-10에서 특정 클래스, 6번 라벨 개구리(Frog)를 이상 데이터로 활용함\n",
    "    1. 개구리(Frog) 6번 라벨링된 모든 이미지를 훈련 데이터에서 제거하고, 나머지 클래스로만 학습을 시킴\n",
    "    2. 테스트 데이터를 6번 라벨(이상 데이터를 포함한) 형태로 구성하여 이상감지 성능을 평가하는 방식임\n",
    "- 위 모델은 개구리는 학습되지 않은 데이터이므로, 테스트 데이터로 개구리가 나타나면 매우 특이한 상황이라고 가정하는 것임\n",
    "    1. 위 시각화 자료에서 anormaly 데이터(6번 라벨)와 normal 데이터(나머지 라벨)간의 분포가 다른 것을 확인할 수 있음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
